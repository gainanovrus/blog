var store = [{
        "title": "Monitoring system for Docker SLURM cluster",
        "excerpt":"Institute of Continuous Media Mechanics (ICMM UB RAS) has a high-performance cluster consist of about 50 nodes with &gt;400 CPUs and more than 1 TB RAM. It’s needed to make models of new materials and calculate its properties. And in the future, the Institute has plans to upgrade his infrastructure and virtualize his resources to get less dependencies of a hardware layer. The cluster uses Slurm workload manager to run programs on compute nodes. With SLURM many scientist can easy run his programs on the cluster in parallel and independent.   And I should work with these systems. My primary task is to make scripts for deploying virtual compute infrastructure with Docker. My second task is to design and configure a monitoring system to collect and view system statics in the real-time.   Has been developed a script that deploying a simple SLURM configuration with docker-compose. Instead of docker-compose could be used docker stack command with Docker Swarm to deploy containers to a compute nodes with included SLURM software. All information is described on GitHub.   I have used a stack InfluxDB+Grafana, I’ve known good from my previous work, to realize a monitoring system. I use HDF5 profiling plugin for collecting data about runned tasks. This plugin was based of work of the cfenoy user. I’ve modified it to work with the last SLURM version and add gathering some new fields. Also was configured a Grafana dashboard that helps view all statistics about task that have been ever run on the HPC cluster.    docker-slurmbase:   https://github.com/GRomR1/docker-slurmbase   influxdb-slurm-monitoring:   https://github.com/GRomR1/influxdb-slurm-monitoring                                                                                      ","categories": [],
        "tags": [],
        "url": "https://gainanovrus.github.io/blog/portfolio/influxdb-slurm-monitoring/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/influxdb-slurm-monitoring/influxdb-slurm-monitoring-teaser.png"},{
        "title": "Computer-aided system to control a moving of a robotic system in an environment with obstacles",
        "excerpt":"In my last course of the University, I’ve decided to help my department to create a model that will simulate the real system for scanning details in many angles. The main purpose of the work is to find an algorithm to move robots by path without collision to each other and the environment. I use [V-REP][v-rep] system to design models and to solve inverse and forward kinematics tasks of together motion two robots.  In the picture gallery is present models are consisting of two main robots is a manipulator (in the picture it is a “1”) and a frame robot (“2”).  Also has a table robot (“3”) that rotates a detail (“4”).   Maybe it is not a work for IT or programmer but I was exploring many science articles and approaches,  studying it to make a good thesis of a scientific work and presents it to a commission of the University.  And also I’ve learned the basics in Lua language.   I’ve uploaded the source code of the project into Github. Also you can view the video of a scanning process of a detail.                                                                                                                        ","categories": [],
        "tags": [],
        "url": "https://gainanovrus.github.io/blog/portfolio/robot-motion-system/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/robot-motion/robot-motion-main-min.png"},{
        "title": "Visualization of the JINR cloud resources utilization",
        "excerpt":"To visualize statistics on a distribution of resources of the JINR cloud the Grafana system was chosen.  It provides a user-friendly interface through a web browser displaying various kinds of statistical metrics in real-time,  gives flexible and functional ways to customize the layout of charts and graphs.   As data-storage to store gathering information from OpenNebula was chosen InfluxDB.  It is an open source database specifically to handle time series data with high availability and high performance requirements.  InfluxDB is meant to be used as a backend storage for many use cases involving large amounts of timestamped data,  including DevOps monitoring, application metrics and real-time analytics.  It has a simple, high performing write and query HTTP(S) APIs.                                                                                                                          Developed dashboards      ","categories": [],
        "tags": [],
        "url": "https://gainanovrus.github.io/blog/portfolio/visualisation-jinr-cloud/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/visualisation-jinr-cloud/visualisation-jinr-cloud-scheme-th-min.png"},{
        "title": "Add a free SSL certificate to NGINX server",
        "excerpt":"Let’s Encrypt provides an easy way to obtain and install free TLS/SSL certificates, thereby enabling encrypted HTTPS on web servers. It simplifies the process by providing a software client, Certbot, that attempts to automate most (if not all) of the required steps.   In this article I will show you how to use the certbot Let’s Encrypt client to obtain a free SSL certificate and use it with Nginx on CentOS 7. I will also show you how to automatically renew your SSL certificate.   Prerequisites  Before following this tutorial, you’ll need a few things.     CentOS 7 server.   DNS A Record that points your domain to the public IP address of your server.   Once you have all of the prerequisites out of the way, let’s move on to installing the Let’s Encrypt client software.   Instructions   Install and configure nginx   yum -y install nginx   Add server name to nginx config file. Next a minimal configures that you need to get certificates  stored in file /etc/nginx/nginx.conf:  http {     server {         listen       80 default_server;         server_name  example.com;          location ~ ^/(.well-known/acme-challenge/.*)$ { \t        root     /usr/share/nginx/html; \t    } \t    location / { \t       ... \t    }     } }  Install and configure Certbot   The official Let’s Encrypt client is called Certbot, and it is included in the EPEL repository. Install Certbot with yum:  yum -y install certbot certbot-nginx   Obtaining a Certificate   Certbot provides a variety of ways to obtain SSL certificates, through various plugins. This runs certbot with the --nginx plugin, using -d to specify the names we’d like the certificate to be valid for. If this is your first time running certbot, you will be prompted to enter an email address and agree to the terms of service. After doing so, certbot will communicate with the Let’s Encrypt server, then run a challenge to verify that you control the domain you’re requesting a certificate for.   I suggest you a simple command that will obtain a certifiacte without any questions and outputs:  certbot certonly --webroot           \\ -d example.com                       \\ --webroot-path /usr/share/nginx/html \\ --email your_email@example.com       \\ --post-hook \"systemctl reload nginx\" \\ --agree-tos                          \\ --quiet   But you can yet view logs placed on path /var/log/letsencrypt/letsencrypt.log   To show information about obtained certificates use command certbot certificates:  # certbot certificates Saving debug log to /var/log/letsencrypt/letsencrypt.log ------------------------------------------------------------------------------- Found the following certs:   Certificate Name: example.com     Domains: example.com     Expiry Date: 2018-05-03 04:20:43+00:00 (VALID: 89 days)     Certificate Path: /etc/letsencrypt/live/example.com/fullchain.pem     Private Key Path: /etc/letsencrypt/live/example.com/privkey.pem -------------------------------------------------------------------------------   The certificate is storing on the disk by path /etc/letsencrypt/live/example.com:  # ll /etc/letsencrypt/live/example.com total 4 lrwxrwxrwx. 1 root root  37 Feb  2 08:45 cert.pem -&gt; ../../archive/example.com/cert1.pem lrwxrwxrwx. 1 root root  38 Feb  2 08:45 chain.pem -&gt; ../../archive/example.com/chain1.pem lrwxrwxrwx. 1 root root  42 Feb  2 08:45 fullchain.pem -&gt; ../../archive/example.com/fullchain1.pem lrwxrwxrwx. 1 root root  40 Feb  2 08:45 privkey.pem -&gt; ../../archive/example.com/privkey1.pem -rw-r--r--. 1 root root 543 Feb  2 08:45 README   In file README consists next description:  This directory contains your keys and certificates.  `privkey.pem`  : the private key for your certificate. `fullchain.pem`: the certificate file used in most server software. `chain.pem`    : used for OCSP stapling in Nginx &gt;=1.3.7. `cert.pem`     : will break many server configurations, and should not be used                  without reading further documentation (see link below).   Additional information are available on the certbot site.   Revoke a Certificate   If you didn’t want use generated certificates any more just revoke it:  certbot revoke                                              \\ --cert-path /etc/letsencrypt/live/example.com/fullchain.pem \\ --delete-after-revoke                                       \\ --quiet  I use a --quiet parameter if you want see a result of certbot revoke just run it without this option:  # certbot revoke --cert-path /etc/letsencrypt/live/example.com/fullchain.pem Saving debug log to /var/log/letsencrypt/letsencrypt.log Starting new HTTPS connection (1): acme-v01.api.letsencrypt.org ------------------------------------------------------------------------------- Would you like to delete the cert(s) you just revoked? ------------------------------------------------------------------------------- (Y)es (recommended)/(N)o: y ------------------------------------------------------------------------------- Deleted all files relating to certificate ems.insyte.ru. ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Congratulations! You have successfully revoked the certificate that was located at /etc/letsencrypt/live/example.com/fullchain.pem -------------------------------------------------------------------------------   Renew a Certificate   Let’s Encrypt’s certificates are only valid for ninety days. This is to encourage users to automate their certificate renewal process. We’ll need to set up a regularly run command to check for expiring certificates and renew them automatically.   To run the renewal check daily, we will use cron, a standard system service for running periodic jobs.   We tell cron what to do by opening and editing a file called a crontab crontab -e:  15 3 * * * /usr/bin/certbot renew --post-hook \"systemctl reload nginx\" --quiet   The line runs the following command at 3:15 am, every day.   The renew command for Certbot will check all certificates installed on the system and update any that are set to expire in less than thirty days. After a renew process has completed a nginx server will been reload.   All installed certificates will be automatically renewed and reloaded when they have thirty days or less before they expire.   Read docs to get information about command line options.   Additional information     How To Secure Nginx with Let’s Encrypt on CentOS 7 - DigitalOcean   User Guide — Certbot documentation   Nginx on CentOS/RHEL 7 — Certbot   ","categories": ["linux"],
        "tags": ["ssl","nginx","centos"],
        "url": "https://gainanovrus.github.io/blog/linux/free-ssl-certbot-nginx-setup/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/free-ssl-certbot-nginx-setup-teaser.png"},{
        "title": "MySQL Master-Slave replication with a Docker",
        "excerpt":"In this article was described a simple configuration of MySQL Master-Slave replication with using Docker containers.   Prepare   Let’s begin. Firstly we need to write both basic configurations (in file 10-mysqld.cnf):  # The MySQL Server configuration file.  # http://dev.mysql.com/doc/mysql/en/server-system-variables.html  [mysqld] pid-file = /var/run/mysqld/mysqld.pid  socket = /var/run/mysqld/mysqld.sock  datadir = /var/lib/mysql symbolic-links = 0    After that create catalogs to store master and slave configures. I.e. master-config and slave-config in a root home directory. And copy the file 10-mysqld.cnf into the dirs.   Configure master   Next we writing master configurations that stored in master-config\\60-enable-replication.cnf (the number 60- is used to define a order to load config files, the name of file is not important):  [mysqld]  server-id = 1 # the number of server, that be unical  log-bin = mysql-bin # the name of binary log binlog_do_db = test_db # the name of replication database   Start master server   Run docker master container that based of official MySQL image and exposed port 33060:  docker run -d --rm --name mysql-master \\     -p 33060:3306 \\     -e MYSQL_ROOT_PASSWORD=root_secret -e MYSQL_DATABASE=test_db \\     -v /root/db_master:/var/lib/mysql \\     -v /root/master-config:/etc/mysql/mysql.conf.d \\     mysql   We can map the existing data that stored in /root/db_master  or create new database and  keep db files into this dir.   Now enter the container from command prompt and connect to created database:  docker exec -ti mysql-master /bin/bash root@c337863c7d3e:/# mysql -proot_secret mysql&gt;   Create replica user   Create new replication user replica with grants REPLICATION SLAVEin mysql console:  CREATE USER 'replica'@'%' IDENTIFIED BY 'replica_strong_password'; GRANT REPLICATION SLAVE ON *.* TO 'replica'@'%'; FLUSH PRIVILEGES;   Dump database   Dump selected database and copy it to the slave. Before we should block any user connection to modify data:  FLUSH TABLES WITH READ LOCK; SET GLOBAL read_only = ON;   Run mysqldump command to create dump database. I run it on my host computer that having IP address 192.168.0.161:  mysqldump -h 192.168.0.161 -P 33060 -u root -proot_secret test_db &gt; test_db.sql   Before unlock database don’t forget check last binary log position with command SHOW MASTER STATUS  mysql&gt; SHOW MASTER STATUS; +------------------+----------+--------------+------------------+ | File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000001 |      107 | test_db      |                  | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec)   After that you can unlock database:  SET GLOBAL read_only = OFF; UNLOCK TABLES;   Configure slave   Create file slave-config\\60-enable-replication.cnf and input next rows to it:  [mysqld] server-id          = 2           # Slave server ID (next after Master) relay_log          = mysql-relay  log_bin            = mysql-bin   binlog_do_db       = test_db      read_only          = 1           # The Slave will work only in read-only mode   Run slave server   Run container that will be slave server and link it to the mysql-master container:  docker run -d --rm --name mysql-slave \\     --link mysql-master:db \\     -p 33061:3306 \\     -e MYSQL_ROOT_PASSWORD=root_secret -e MYSQL_DATABASE=test_db \\     -v /root/db_slave:/var/lib/mysql \\     -v /root/slave-config:/etc/mysql/mysql.conf.d \\     mysql   Instead of linking containers you can use the host IP and exposed port to the master container (33060).   Deploy master dump   Deploy the created dump to the slave server:  mysql -h 192.168.0.161 -P 33061 -u root -proot_secret test_db &lt; test_db.sql   Go into the container of the slave server:  docker exec -ti mysql-slave /bin/bash root@53378642dd3a:/# mysql -proot_secret mysql&gt;   And run the command [CHANGE MASTER TO][change_to] to write a connection to the master in MySQL console with information that we has input early on the master:  CHANGE MASTER TO MASTER_HOST='db', MASTER_USER='replica', MASTER_PASSWORD='replica_strong_password', MASTER_LOG_FILE = 'mysql-bin.000001', MASTER_LOG_POS = 107;   And finally start replication:  START SLAVE;   To check the status of replication use next command:  mysql&gt; SHOW SLAVE STATUS\\G *************************** 1. row ***************************                Slave_IO_State: Waiting for master to send event                   Master_Host: db                   Master_User: replica                   Master_Port: 3306                 Connect_Retry: 60               Master_Log_File: mysql-bin.000002           Read_Master_Log_Pos: 4133                Relay_Log_File: mysql-relay.000003                 Relay_Log_Pos: 4346         Relay_Master_Log_File: mysql-bin.000002              Slave_IO_Running: Yes             Slave_SQL_Running: Yes               Replicate_Do_DB:           Replicate_Ignore_DB:            Replicate_Do_Table:        Replicate_Ignore_Table:       Replicate_Wild_Do_Table:   Replicate_Wild_Ignore_Table:                    Last_Errno: 0                    Last_Error:                  Skip_Counter: 0           Exec_Master_Log_Pos: 4133               Relay_Log_Space: 4715               Until_Condition: None                Until_Log_File:                 Until_Log_Pos: 0            Master_SSL_Allowed: No            Master_SSL_CA_File:            Master_SSL_CA_Path:               Master_SSL_Cert:             Master_SSL_Cipher:                Master_SSL_Key:         Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No                 Last_IO_Errno: 0                 Last_IO_Error:                Last_SQL_Errno: 0                Last_SQL_Error:   Replicate_Ignore_Server_Ids:              Master_Server_Id: 1                   Master_UUID: 23b67d70-fab5-11e7-ac09-0242ac11000f              Master_Info_File: /var/lib/mysql/master.info                     SQL_Delay: 0           SQL_Remaining_Delay: NULL       Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates            Master_Retry_Count: 86400                   Master_Bind:       Last_IO_Error_Timestamp:      Last_SQL_Error_Timestamp:                Master_SSL_Crl:            Master_SSL_Crlpath:            Retrieved_Gtid_Set:             Executed_Gtid_Set:                 Auto_Position: 0          Replicate_Rewrite_DB:                  Channel_Name:            Master_TLS_Version: 1 row in set (0.00 sec)  ","categories": ["linux"],
        "tags": ["docker","mysql","linux"],
        "url": "https://gainanovrus.github.io/blog/linux/mysql-replication/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/mysql-replication-teaser.jpg"},{
        "title": "Install and configure NTP on FreeBSD 11",
        "excerpt":"In this article will be described what use NTP to synchronize system time on FreeBSD. I use FreeBSD 11 that ran as VPS in Hetzner cloud.   Previously you should check what version is installed on your machine. Use for this command ntpd --version. Result:  ntpd 4.2.8p10-a (1)   If you have NTP server that version is less 4.2.7 you should update it because you might be attacked.   Using a portsnap to update your local ports:  portsnap fetch extract portsnap fetch update   After that instal zoneinfo package and copy selected local zone to change your timezone (my zone is Europe/Moscow:  cd /usr/ports/misc/zoneinfo &amp;&amp; make install clean cp /usr/share/zoneinfo/Europe/Moscow /etc/localtime   Editing your NTP configuration file /etc/ntp.conf:  restrict default kod nomodify notrap nopeer noquery restrict -6 default kod nomodify notrap nopeer noquery  restrict 127.0.0.1 restrict -6 ::1  server ntp1.hetzner.de iburst server ntp2.hetzner.de iburst server ntp3.hetzner.de iburst   Don’t forget setup your servers in these parameters:     server ntp#.hetzner.de iburst – use this server to synchronize time; iburst keyword allows to speed up first connection   And now only need to add a command that will allow running NTP on startup (rc.conf):  echo ntpd_enable=\\\"YES\\\" &gt;&gt; /etc/rc.conf echo ntpd_sync_on_start=\\\"YES\\\" &gt;&gt; /etc/rc.conf   Parameter ntpd_sync_on_start is setting YES to syncs the system’s clock\ton startup and to remove a restriction a lot time offset.   Finally lets start a local NTP server:  /etc/rc.d/ntpd start   Some moments ago your local system time will update and you can check a status on this:  # ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== +ntp1.hetzner.de 192.53.103.108 2 u 1 64 1 2.864 -0.889 0.153 +ntp2.hetzner.de 192.53.103.108 2 u 4 64 1 0.328 0.219 0.114 *ntp3.hetzner.de 192.53.103.108 2 u 2 64 1 0.306 0.832 0.092  # ntpdate -q localhost server 127.0.0.1, stratum 3, offset -0.000008, delay 0.02568 server ::1, stratum 3, offset 0.000005, delay 0.02571 ntpdate[52224]: adjust time server 127.0.0.1 offset -0.000008 sec   If you want to sync onetime use this command:  # ntpdate -v -b ntp1.hetzner.de ntpdate[11356]: ntpdate 4.2.8p10-a (1) ntpdate[11356]: step time server 213.239.239.164 offset 0.004194 sec   Additional information:  This links might useful to get any additional information:     Hetzner - Install the NTP daemon   ntpd – NTP daemon\tprogram    ntp.conf – Network Time Protocol (NTP) daemon configuration file format   Синхронизация часов через NTP   Настройка ntpdate/ntpd на FreeBSD 10   Атака с помощью вашего сервера времени: NTP amplification attack (CVE-2013-5211)   ","categories": ["freebsd"],
        "tags": ["ntp","freebsd"],
        "url": "https://gainanovrus.github.io/blog/freebsd/ntp-freebsd-setup/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/ntp-freebsd-setup.png"},{
        "title": "Upgrade kernel of CentOS 7 to latest version",
        "excerpt":"Previously I was writing about upgrading system now  I will show how to upgrade the CentOS7 kernel to the latest version. Using ELRepo repository we can get easy kernel updates.  ELRepo is focused on the packages related to hardware,  including filesystem drivers, graphic drivers, network drivers, sound card drivers, webcam, and others.   Instructions   Check the current kernel version.  # uname -sr Linux 4.11.3-1.el7.elrepo.x86_64   If we now go to , we will see that the latest kernel version is 4.15 at the time of this writing  (other versions are available from the same site).   To enable the ELRepo repository on CentOS 7, do  rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm    Next, install the latest mainline stable kernel  yum --enablerepo=elrepo-kernel install kernel-ml   I had this output of that command  # yum --enablerepo=elrepo-kernel install kernel-ml Loaded plugins: fastestmirror, langpacks elrepo-kernel                                                                                                                                                          | 2.9 kB  00:00:00 elrepo-kernel/primary_db                                                                                                                                               | 1.7 MB  00:00:01 Loading mirror speeds from cached hostfile  * base: mirror.reconn.ru  * elrepo: nl.mirror.babylon.network  * elrepo-kernel: nl.mirror.babylon.network  * epel: mirror.logol.ru  * extras: mirror.reconn.ru  * updates: mirror.reconn.ru Resolving Dependencies --&gt; Running transaction check ---&gt; Package kernel-ml.x86_64 0:4.15.8-1.el7.elrepo will be installed --&gt; Finished Dependency Resolution  Dependencies Resolved  ==============================================================================================================================================================================================  Package                                    Arch                                    Version                                              Repository                                      Size ============================================================================================================================================================================================== Installing:  kernel-ml                                  x86_64                                  4.15.8-1.el7.elrepo                                  elrepo-kernel                                   44 M  Transaction Summary ============================================================================================================================================================================================== Install  1 Package  Total download size: 44 M Installed size: 198 M Is this ok [y/d/N]: y Downloading packages: kernel-ml-4.15.8-1.el7.elrepo.x86_64.rpm                                                                                                                               |  44 MB  00:00:05 Running transaction check Running transaction test Transaction test succeeded Running transaction   Installing : kernel-ml-4.15.8-1.el7.elrepo.x86_64                                                                                                                                       1/1   Verifying  : kernel-ml-4.15.8-1.el7.elrepo.x86_64                                                                                                                                       1/1  Installed:   kernel-ml.x86_64 0:4.15.8-1.el7.elrepo  Complete!   Next, reboot your machine to apply the latest kernel  systemctl reboot   If you just reboot the system and run command to check the version of your kernel  # uname -sr Linux 4.11.3-1.el7.elrepo.x86_64   You will see your current kernel. WTF? But I updated it. Your system will be updated but to run the system with new kernel need configure Grub loader.   Let’s check the current configuration  # awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 0 : CentOS Linux (4.15.8-1.el7.elrepo.x86_64) 7 (Core) 1 : CentOS Linux (4.11.3-1.el7.elrepo.x86_64) 7 (Core) 2 : CentOS Linux (0-rescue-d39b765b9cf142baa7ed9ca6543375de) 7 (Core)   As you can see the new kernel has the second option in the loading process. We want to use kernel 4.15 as our default, so you can use the following command to make this happen.  grub2-set-default 0   When you want to revert back to the old kernel,  you can change the value of the grub2-set-default command to 1.   Finally, generate the grub2 config with gurb2-mkconfig command, and then reboot the server.  grub2-mkconfig -o /boot/grub2/grub.cfg systemctl reboot   After system will load check again the kernel version  # uname -sr Linux 4.15.8-1.el7.elrepo.x86_64   Excellent! We have the latest kernel.   Of course, don’t do this procedure on a system with users.   Additional information     How to Upgrade Kernel on CentOS 7   How to Install or Upgrade to Kernel 4.15 in CentOS 7   ","categories": ["linux"],
        "tags": ["centos"],
        "url": "https://gainanovrus.github.io/blog/linux/upgrade-centos-kernel/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/upgrade-centos-kernel.png"},{
        "title": "Upgrade CentOS 7.x to latest version",
        "excerpt":"Usually, you want to use the last version of the operating system and if it is true you will update your OS early or later. In this article, you can found a simple and small instruction to update Centos 7 from 7.x to the latest version (now it is 7.4) See CentOS 7.4 release note for more information about changes.   Instructions   Check the current version  # cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core)   Upgrading is the same as the update option with -obsoletes flag. Before upgrading purge all old cache and download a new one. And next, you can upgrade OS.  yum clean all yum makecache yum -y upgrade   Below is described info that I got (I left only main info)  # yum clean all ... Cleaning up everything Cleaning up list of fastest mirrors  # yum makecache ... Metadata Cache Created  # yum -y upgrade ... Complete!   After the upgrade process has completed seeing changes reboot a system  systemctl reboot   And repeat to check the current version of the system  # cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core)   Of course, don’t do this procedure on a system with users.   Additional information     Upgrading from CentOS 7.3 to 7.4   How to Update CentOS 7.0/7.1/7.2/7.3 to CentOS 7.4   ","categories": ["linux"],
        "tags": ["centos"],
        "url": "https://gainanovrus.github.io/blog/linux/upgrade-centos/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/upgrade-centos.jpg"},{
        "title": "Create daily database backups with Borg",
        "excerpt":"In this post, I want to describe a creating backup of PostgreSQL database and transferring it to a remote server. I will show you how to automate it to get regular daily backup and use a deduplication tool as Borgbackup for effective using storage. Also, the tool has methods for encryption your data and I will show how to use it.  Backup database to borg repository is the efficient and secure way to store your critically important data.   Prerequisites   Before following this tutorial, you should understand conditions that I have:     Has two Linux servers:            one is a backup remote storage       second is a database server           I will use Centos 7 on the remote server, and Ubuntu 16.04 on another server with installed PostgreSQL 9.6. They both have connections to one network, an IP of Centos is 192.168.0.1 and Ubuntu has 192.168.0.2. Backup server has a special backup user with name backup_user   Instructions   Install borg on both servers   Is a simple action just use apt or yum install:  apt install borgbackup   And checking your version after that borg --version.  Centos: borg 1.1.4 Ubuntu: borg 1.0.11   Ok, some differences can’t a problem with using it. If you want to use the last version follow this installing instruction. But didn’t use the version before 1.0.9 because that has a vulnerability.   Official docs have a good description of basic usage a borg commands and options,  see them for best understanding next commands.   Configure access to the backup server   Generate ssh key to remote access to the backup server if you don’t get it.  ssh-keygen -b 2048 -t rsa -q -N '' -f ~/.ssh/id_rsa   Create a config file if you don’t want to remember an IP of the remote server like me.  cat &lt;&lt; EOT &gt;&gt; ~/.ssh/config Host backup_server User backup_user HostName 192.168.0.1 Port 22 EOT   And run command to copy generated public key to the remote server.  ssh-copy-id backup_server   Test it.  # ssh backup_server Last login: Sun Apr 15 10:18:16 2018 [backup_user@backup_server ~]$   All is OK.   Create a backup script   Ok, let’s begin to automate the process of creating a backup of the database. I am using a pg_dumpall to create a backup of a full database. A result of this command a SQL-file that can create all existing databases and users in a new place. This result before coping will been stored on local path /root/db_dumps. Also I make backup of database configs.   #!/bin/bash BACKUP_DIR=/root/db_dumps BACKUP_DUMP_NAME=\"dump_$(date '+%Y-%m-%d_%H%M%S').sql\" BACKUP_CONFIG_NAME=\"config_$(date '+%Y-%m-%d_%H%M%S').tar.gz\" USERNAME=postgres BORG_CMD=\"borgmatic\" DB_CONF_PATH=\"/etc/postgresql/9.6/main\"  error_exit() {   echo \"$1\" 1&gt;&amp;2   exit 1 }  debug_message() {     echo \"$1\" 1&gt;&amp;2 }  debug_message \"[backup][START] Start creating backup\"  #remove previous backup [[ -d ${BACKUP_DIR} ]] &amp;&amp;  rm -rf ${BACKUP_DIR} [[ ! -d ${BACKUP_DIR} ]] &amp;&amp; mkdir ${BACKUP_DIR} &amp;&amp; chmod 700 ${BACKUP_DIR}  #backup database data cd ${BACKUP_DIR} if ! pg_dumpall -U \"$USERNAME\" --file=${BACKUP_DUMP_NAME}; then   error_exit \"[backup][ERROR] Failed to produce data backup\" else   debug_message \"[backup][SUCCESS] Data backup success created\" fi  #backup database configures cd ${DB_CONF_PATH} if ! tar -cvzf ${BACKUP_CONFIG_NAME} *.conf &gt; /dev/null 2&gt;&amp;1; then   error_exit \"[backup][ERROR] Cannot create archive with PG conf files\" else   [[ -f ${BACKUP_CONFIG_NAME} ]] &amp;&amp;  mv ${BACKUP_CONFIG_NAME} ${BACKUP_DIR}   debug_message \"[backup][SUCCESS] Config files success created\" fi  #copy to backup server if ! ${BORG_CMD} ; then   error_exit \"[backup][ERROR] Failed copy to backup server\" else   debug_message \"[backup][SUCCESS] Data backup success copied\" fi   #remove local backup files #none  debug_message \"[backup][COMPLETE] Backup success created\"   I saved this script in the database server by path /opt/postgres_backup.sh. And set execute rights to file (chmod 700 /opt/postgres_backup.sh).   BorgBackup command   In my script has been a variable BORG_CMD is a wrapper of Borgbackup - borgmatic. It initiates a backup, prunes any old backups according to a retention policy, and validates backups for consistency. The script supports specifying your settings in a declarative configuration file rather than having to put them all on the command-line and handles common errors.   To install borgmatic use pip3:  pip3 install --upgrade borgmatic   Borgmatic settings   The settings of borgmatic is stored on /etc/borgmatic/config.yaml. And my ones to our task are described below.  location:     source_directories:         - /root/db_dumps      repositories:         - backup_server:/backups/db  storage:     encryption_passcommand: \"cat /root/.borg-passphrase\"  retention:     keep_daily: 30     keep_weekly: 12     keep_monthly: 6  consistency:     checks:         - repository         - archives     check_last: 7   Create a passphrase on the database server   Next command creating a key file to use as a passphrase of an encrypted repository. Borg are using system environments to get some parameters. This one show borg how to get a passphrase to either new or existing repository.  head -c 1024 /dev/urandom | base64 &gt; /root/.borg-passphrase chmod 400 /root/.borg-passphrase export BORG_PASSCOMMAND=\"cat /root/.borg-passphrase\"   Create a repository on the backup server   Using a repokey modes to “passphrase-only” security. The key will be stored inside the repository (in its “config” file). In above mentioned attack scenario, the attacker will have the key (but not the passphrase).  # borg init --encryption=repokey backup_server:/backups/db  By default repositories initialized with this version will produce security errors if written to with an older version (up to and including Borg 1.0.8).  If you want to use these older versions, you can disable the check by running: borg upgrade --disable-tam db_test  See https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability for details about the security implications.   The output has some warnings about a using old version.   Create a deamon to automate running a backup script   Ubuntu has a nice mechanism to auto-running scripts or applications in the background. His name - SystemD. And if exists a necessity to run a script by a scheduler (i.e. every night) you should use timers that is part of systemd.   I write my own daemon  to run an above script. It has stored on /etc/systemd/system/backup.service.  [Unit] Description=Backup PostgreSQL database  [Service] Type=oneshot ExecStart=/bin/bash /opt/postgres_backup.sh   And I has a timer unit that runs a backup.service every day at midnight  [Unit] Description=Run backup script every day at midnight  [Timer] OnCalendar=daily  [Install] WantedBy=timers.target   After that you should reload daemon-service.  systemctl daemon-reload   Manual run script and view logs   To run our systemd unit (or daemon) just run next command on a terminal:  systemctl start backup   And anfter this command completed view logs by journalctl -u backup  Apr 14 11:07:48 ubuntu systemd[1]: Starting Backup PostgreSQL database... Apr 14 11:07:48 ubuntu bash[234]: [backup][START] Start creating backup Apr 14 11:08:48 ubuntu bash[234]: [backup][SUCCESS] Data backup success created Apr 14 11:08:48 ubuntu bash[234]: [backup][SUCCESS] Config files success created Apr 14 11:10:27 ubuntu bash[234]: [backup][SUCCESS] Data backup success copied Apr 14 11:10:27 ubuntu bash[234]: [backup][COMPLETE] Backup success created Apr 14 11:10:27 ubuntu systemd[1]: Started Backup PostgreSQL database.   Finish preparations and check the results   Don’t forget setup enable flag on the timer for working them after reboot system.  systemctl enable backup.timer systemctl start backup.timer   And check active system timers with systemctl list-timers.  NEXT                         LEFT     LAST                         PASSED       UNIT                         ACTIVATES Mon 2018-04-16 00:00:00 MSK  9h left  Sun 2018-04-15 00:00:52 MSK  14h ago      backup.timer                 backup.service ...  5 timers listed.   Ok, I just run a backup script two times on my database and have got two backups.  # borg list backup_server:/backups/db ubuntu-2018-04-15T00:05:46.344337 Sun, 2018-04-15 00:05:28 ubuntu-2018-04-15T00:08:46.344337 Sun, 2018-04-15 00:08:47   Let’s see the information about second one.  # borg info backup_server:/backups/db::ubuntu-2018-04-15T00:08:46.344337 Name: ubuntu-2018-04-15T00:08:46.344337 Fingerprint: 19314fb992f83d9e746eda534d22fc8375a8d20c0cf8828a4353463574asdw Hostname: ubuntu Username: root Time (start): Sun, 2018-04-15 00:08:47 Time (end):   Sun, 2018-04-15 00:08:47 Number of files: 2                         Original size      Compressed size    Deduplicated size This archive:                8.50 GB              8.50 GB                624 B All archives:               17.00 GB             17.00 GB              6.64 GB                         Unique chunks         Total chunks Chunk index:                    2548                 6750   One dump file has a size of 8G, but repository using deduplication stores two ones with less disk usage space. All archives has reduce about 6,6G. Next command will agree this words  backup_server &gt;# du -hd0 /backups/db 6,2G    db   And at the end, I want to show the command that extracts (“restore”) backup from an archive repository to a local path.  mkdir old_data cd old_data borg extract backup_server:/backups/db::ubuntu-2018-04-15T00:08:46.344337   In conclucion I want to say that the Borg is a simple, reliable and secure tool to store your backup data. The data deduplication technique used makes Borg suitable for daily backups since only changes are stored. The authenticated encryption technique makes it suitable for backups to not fully trusted targets. And it has a simple interface to run with many parameters with borgmatic.   Additional information:   This links might useful to get any additional information:     Borg by Thomas Waldmann   BorgBackup Offsite   borgmatic Offsite   Is Borg backup suitable for the production?   ","categories": ["linux"],
        "tags": ["postgres"],
        "url": "https://gainanovrus.github.io/blog/linux/database-backup-with-borg/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/500x300.jpg"},{
        "title": "Программа максимум или как объехать и увидеть Крым за 5 дней!",
        "excerpt":"Так вышло, что у меня было лишь 5 дней отпуска на изучение Крыма, и я понимал, что не смогу просто лежать на пляже всё это время. Тем более, что море еще недостаточно теплое (конец мая).   Необходимо было придумать себе цель путешествия. И я ее нашел. Что это за цель, я думаю, вы поймете к концу рассказа.   А ниже, в формате истории с некоторыми интересными фактами, она будет представлена.   Крымский полуостров   Крымский полуостров окружен морем с четырех сторон, и лишь Перекопский перешеек шириной в семь километров соединяет его с Большой землей.   Площадь Крыма около 27 000 км2. Он меньше Швейцарии, но больше Израиля или Кипра, а протяженность его побережья составляет 2500 км.   Природа Крыма разнообразна и уникальна. Здесь встречаются сразу три климатические зоны: умеренно-континентальный климат степей, горный пояс и субтропики южного побережья. Только на Крымском полуострове можно столкнуться с сочетанием плоских горных вершин с холодноватым и невероятно влажным воздухом, а всего через пару-тройку километров располагаются теплые черноморские берега с их галечными и песчаными пляжами.                                                                                                                        Западное побережье   Западное побережье – степное. Нет гор и лесов, только ветер и безводье. Растения в большинстве своем низкорослые, иссохшие, жесткие, колючие — акация, туя, скумпия, редко — тополь. В мае зацветают тюльпаны, затем маки. Степь в это время прекрасна.   Крайняя точка западного побережья - мыс Тарханкут. В средние века его называли “Мыс Бурь”. Расположен неподалеку от села Оленевка, насчитывающего около 1500 жителей. В этой местности море очень чистое, потому что поблизости нет рек, впадающих в него. Вода меняет оттенки от глубокого синего до лазурно-голубого. Наблюдать за сменой цвета можно бесконечно.   Переменчивость погоды, частые шторма и скальный рельеф береговой линии делали эти места весьма опасными. В 1814 году в обеспечении безопасного мореплавания приняли решение соорудить маяк. Высота Тарханкутского маяка составила 35 метров. Маяк оборудовали 15 лампами, дающими свет от сгорания масла, и массивным колоколом, который в непогоду звонил в набат. В наши дни маяк полностью переоборудован и вокруг него осуществляется охрана. Рядом с ним видно затонувший в 2010 году сухогруз.      И добраться до туда не так-то просто. До Оленевки ходят всего несколько рейсов из Евпатории и Черноморского с утра.   Доехав до Черноморского, я понял, что последний автобус ушёл 15 минут назад, следующий — только завтра.   Ехать на такси — не мой путь, и я решил взять велосипед напрокат, подумав, что 30 км не такая уж и большая дистанция. Я ж и больше проходил…   Совершенно проигнорировав особенности рельефа, сельскую дорогу и… штормовое предупреждение.   Благо всё обошлось, и за несколько часов я добрался до этого удивительного места.                                                                                                                                                                                                 Южное побережье   Высокие горы, теплое море, мягкий климат – вот что такое Юг Крымского полуострова.   Здесь можно найти живописные скалы, высокие горы и уютные бухты с песочными пляжами. Гряда Крымских гор защищает полуостров от воздушных масс, идущих с континента, а потому на южном побережье сохраняется субтропический климат с теплым летом, жарким солнцем, обилием зелени и теплой морской водой.   Самой южной точкой Крыма является мыс Сарыч. Большинство предполагают, что название Сарыч («золототканый») мыс получил благодаря цвету расположенных рядом холмов. Расстояние от мыса Сарыч до мыса Керемпе в Турции составляет около 260 км. Маяк на мысе построен по приказу императора Николая II в 1898 году. 12-ти метровая башня сделана из чугуна. Маяк дожил до наших дней в неизменном виде. Совсем рядом находятся территории дач высших правительственных чиновников СССР, где находился М.С. Горбачев во время попытки государственного переворота 1991 г.      Отбиваясь от череды таксистов и немного передохнув в Ялте, отправился покорять самую южную точку Крыма.   Добраться до туда можно на проходящем автобусе до Севастополя.   Билетов на ближайшие рейсы не оказалось, но местные водители автобусов без проблем подбирают людей, стоит лишь немного отойти за территорию вокзала.   Зачастую это бывает дешевле, чем в кассе.       Дорога до мыса проходит вдоль живописного побережья на значительной высоте над морем. Море и горы. Красота… Сам мыс посетить, к сожалению, не удалось.   Территория вокруг него была вся застроена различными постройками. К маяку также не попасть - режимный объект.   Зато застал живописный закат на фоне гор. А вообще, темнеет, на удивление, быстро и рано.                                                                                                                                                             Восточное побережье   Восток Крыма представляет собой скопление небольших бухт и полуостровов, например, Керченского полуострова. Он характерен плавным переходом от Крымских гор к степи. Климат на восточном берегу Крыма близок к средиземноморскому – мягкий и без резких температурных колебаний. Воздух здесь чистый и сухой, а, благодаря морским бризам, летний зной переносится достаточно легко. Пляжи на восточном побережье Крыма встречаются разные - песчано-гравийный с мелким ракушечником от Алушты до Судака, с серым песком в самом Судаке, гравийно-галечный в Зеленой бухте Нового Света и мелкопесчаные в Феодосии.   Мыс Фонарь является самой восточной точкой Крымского полуострова. У этого скалистого возвышения воды Керченского пролива переходят в воды Азовского моря. Название мыса — «Фонарь», переводится с греческого языка, как факел, светоч. Со времен греческой колонизации на мысе Фонарь был маяк, обеспечивающий морским судам безопасное прохождение Боспора Киммерийского. Сам мыс Фонарь представляет собой скалистое возвышение на границе Азовского моря и Керченского пролива. С его верхушки открываются потрясающие панорамы Азовского моря, Краснодарского края, косы Чушки и паромной переправы. Керченский пролив с этого места хорошо просматривается, а при хорошей погоде отлично видны постройки на той стороне. С 1820 года на самом высоком месте возвышается Еникальский маяк, указывающий путь морским судам. Но во время ожесточенных боев, в годы Великой Войны, маяк был разрушен. О тех событиях, и бесстрашных подвигах во имя жизни, времен войны, сегодня напоминают стелы и обелиски. Сейчас здесь оборудован современный навигационный комплекс со станцией GPS.      Приехав в Керчь, пересел на городской автобус из Керчи и доехал до популярной некогда паромной переправы.   Пообщавшись с работниками узнал, что из-за сильного ветра (порывами до 20 м/с) попасть на другой берег у меня не получится еще как минимум 40 часов и лучше это сделать по мосту.   Эх…       Солнце двигалось к закату, ветер дул в спину, и хотелось успеть вернуться до наступления темноты. Пройти предстояло около 10 км. Земля, давно не видавшая дождя, была покрыта чередой глубоких трещин, прикрытых пожелтевшей сухой травой. Завораживающие холмистые пейзажи вокруг мыса и почти полное отсутствие людей создают здесь особую таинственную атмосферу. Удивительное место с неповторимой энергетикой.                                                                                                                                                                                                 Отдельного упоминания стоит горный Крым.   Крымские горы   Уникальные явления природы — Крымские горы. Они являются одной из визитных карточек полуострова и излюбленным место туристов. Горы Крыма - это сложная структура горных хребтов, каждый элемент которых совершенно уникален и имеет собственное историческое значение. Горы Крыма нельзя назвать очень высокими, они ниже Кавказа, Алтая и Альп. Высота самой большой горы Роман-Кош всего лишь 1545 м. Самые пики Крымских гор принято называть яйла. Яйла — это цепь своеобразных столообразных плоских вершин, соединенных глубокими перевалами. Ландшафт их необычайно живописен и крайне разнообразен. Весной они представляют собой сплошной ковер из цветов.   Горные реки изобилуют многочисленными порогами, перекатами и водопадами (Учан-Су, Гэловкинский, Джур-Джур и др.). Горный Крым также называют Лесным — здесь сохранились крупные массивы реликтовых лесов, занимающие до 12% площади полуострова.   Одна из вершин Крымских гор, находящаяся в пределах большой Ялты – это Ставри-Кая. Ее высота около семисот метров над уровнем моря. В переводе с греческого языка «ставрос» это — крест, а с татарского «кая»- скала, так что название говорит само за себя – «крестовая скала». Вершину скалы венчает православный крест. Со скалы хорошо просматривается южный берег и Ялта. К ее вершине ведет сразу несколько удобных троп-терренкуров (терренкур - “лечение местностью”: tеrrа - земля, сurе - лечить), считающихся лечебными, потому что проходят по целебным сосновым лесам.   Боткинскую тропу проложили члены Ялтинского отделения Крымско-Кавказского горного клуба, почитавшие знаменитого профессора, врача С. П. Боткина. Тропа названа именем великого деятеля медицины, т.к. он одним из первых оценил и рассказал о целебном влиянии земель Южного берега Крыма на здоровье человека. Неповторимые ароматы хвои здешних лесов, чистый горный воздух оказывают мощное лечебное воздействие на организм. Человек здесь исцеляется не только телом, но и душой. А это залог скорого излечения от множества недугов. С. П. Боткин стал одним из первых проповедовать лечение силой земли.      Мой путь был от Поляны сказок (так называется автобусная остановка), по Боткинской тропе до горы Ставри-Кая,   переход на Штангеевскую тропу до водопада Учан-Су и обратно через лесной массив к Поляне сказок.   Удобная хорошо протоптанная тропа, проходящая через хвойный лес, почти не дает возможности сбиться с пути.   Воздух здесь необычайно чистый, легкий. Хочется дышать полной грудью.   Идти не трудно, и любой более-менее подготовленный человек способен преодолеть этот маршрут.   По дороге кое-где оборудованы скамейки, и чтобы не сбиться с маршрута имеется много меток на деревьях.   Также путь проходит через каскад водопадов реки Яузлар, в котором туристы набирают воду для питья.   Вода чистая и вкусная. В целом, маршрут у меня занял 6.5 часов.   Рекомендую всем к посещению! И как гласит одна мудрая надпись, помните, что в лесу мусор не растет!                                                                                                                                                                                                                                     Вместо заключения   Вроде бы зачем все это? Есть ведь много популярных достопримечательностей, среди которых Воронцовский дворец, Ласточкино гнездо, Генуэзские крепости, Херсонес, или просто отдых на пляже. Но кроме них Крым полон мест, менее известных, а от этого еще более привлекательных – уголков полуострова, окутанных своей тайной и невероятным очарованием.   И я уверен, что каждому по силам придумать свой уникальный маршрут путешествия в этом удивительном краю, и, надеюсь, что мой рассказ побудит вас открыться приключениям.                                               ","categories": ["travelling"],
        "tags": ["russia"],
        "url": "https://gainanovrus.github.io/blog/travelling/journey-to-crimea/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/journey-to-crimea/overlay_crimea.jpg"},{
        "title": "Tasks of Linux contest in IT-Planet",
        "excerpt":"IT-Planet is an international olympiad for students and young specialists in IT.  I participated in the contest of Linux Administration and I became one of the winners.  The tasks that presented below have been in the final of the contest.   «IT-Планета» — одно из самых масштабных состязаний в области информационных технологий, учрежденных в России.  Учредителями конкурсов являются ведущие российские и международные ИТ-компании: 1С, Huawei, Cisco, Oracle, ГНУ/Линуксцентр и СКБ Контур.  Олимпиада проводится по нескольким направлениям.   Я участвовал в конкурсе по номинации “Администрирование Linux”, прошел несколько отборочных этапов и стали одним из победителей в финальных соревнованиях. И здесь решил выложить сами задания финала, которые показались мне достаточно интересными. Если найду время - выложу и решения.      В нашем распоряжении была машина с OC Centos 7 минимальной конфигурации, подключенная физически к интернету. Необходимо было:     авторизоваться в системе, взломав пароль пользователя root   починить сломанные настройки сети   найти веб-сервер в локальной сети, на котором лежат задания   скачать текстовый файл с заданиями и начать их решать      Задания финала конкурса “Администрирование Linux”    Представиться в /root/motd      Не забывайте, что ваша рабочая станция после перезапуска должна автоматически  подключиться к сети.       Нужно создать раздел на диске размером 1000MB, примонтировать в /data/            Диск должен монтироваться при старте системы       Должны быть приняты меры на случай смены имени диска           Завести пользователей orta, atmos, hexen, plyaski, ai            orta, atmos должны иметь доступ read в /data       ai и plyaski rw доступ       Чтобы избежать переполнения раздела, нужно выставить квоту на запись для юзеров ai и plyaski, soft 7000kb, hard 8000kb           Настроить nfs-server            Каталог /var/nfs должен быть доступен для монтирования из локальной сети       root доступ должен быть отключен       пользовательская авторизация включена       firewall должен работать           Настроить кеширующий dns-сервер unbound на локальную сеть.            В качестве forward-zone использовать 8.8.8.8 / 8.8.4.4       запросы должны приниматься только из подсети 192.168.171.0/24           Юзеру hexen ограничить использование CPU до 10% а MEM до 128MB            проверка будет производиться приложением stress/stress-ng           Запустить в контейнере (docker/lxc) socks5-прокси сервер с авторизацией по логин:пароль (it:planet), порт 1080            Проверить работоспособность прокси при помощи curl/wget/aria2/etc                Установить и настроить prometheus для сбора метрик с рабочей станции сервис должен работать на стандартном порту (9090), отдавать базовую информацию о машине, быть доступным из локальной сети.       На сервере с задачами находится файл answers.txt Содержит он вопросы восстановления паролей у пользователей почтового сервиса. Для удобства обработки списка нужно написать скрипт, который            Забирает список с сервера       На лету преобразовывает его в json  (реализация принимается на bash, python, готовый скрипт положить в /root/answer_to_json.py,{sh})       json сохранить в /root/result.json       Бонусный балл можно заработать отфильтровав наиболее редкие вопросы восстановления (результат положить в /root/answers_filtered.txt)                На сервере с заданиями можно обнаружить странный архив botnet.tar.gz с бинарниками, происхождение и назначение которых вам неизвестны. После краткого расследования выяснилось, что злоумышленник проник в систему, потому что у одного из пользователей логин совпадал с паролем. Есть подозрение, что это часть какого-то ботнета. Задача собрать как можно больше информации о бинарном файле, о том что делает программа, при возможности деанонимизировать автора. Информацию о файле и свои мысли записать в /root/botnet.answer       Все манипуляции с файлами из архива с ботнетом производить в контейнере. Выполнение на хост-системе ведет к дисквалификации.       ","categories": ["linux"],
        "tags": ["itplanet","contest","centos","linux"],
        "url": "https://gainanovrus.github.io/blog/linux/tasks-of-linux-contest-in-itplanet/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/tasks-of-linux-contest-in-itplanet-teaser.jpg"},{
        "title": "GitLab CI/CD",
        "excerpt":"Today I want to start writing a cycle of articles about CI/CD process with GitLab. Many developers use GitLab to store code and work together on one project (write and review code, make issues, user management, etc.). But it has many other built-in tools includes automate test and deploy process.   With Gitlab you can easily build, test and deploy your code. In our company we are using Gitlab CI/CD for a long time. I will share my experience with you.   So these topics will be described in next articles:     GitLab CI/CD. Main concepts and terms   GitLab CI/CD. Install and configure Runner   GitLab CI/CD. Create simple pipeline   GitLab CI/CD. Tips and tricks    Official documentation     GitLab CI/CD Documentation   Getting started with GitLab CI/CD   Configuration of your jobs with .gitlab-ci.yml   ","categories": ["devops"],
        "tags": ["gitlab","devops"],
        "url": "https://gainanovrus.github.io/blog/devops/gitlab-ci-main/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/gitlab-ci-teaser.png"},{
        "title": "GitLab CI/CD. Main concepts and terms",
        "excerpt":"If you want to use Continuous Integration (CI) and Continuous Delivery (CD) services you need to know basic terms using in GitLab. I selected the foremost ones and set links to the description and additional information about them.   Configuration file  For using a CI/CD service you need to create a .gitlab-ci.yml file to the root directory of your repository. This file is used by GitLab Runner to manage your project’s jobs and stored in a YAML format.   Runner  GitLab Runner is a daemon in a host machine that is used to running your jobs and send the results back to GitLab. It permanently holds a connect with Gitlab. When a user runs job by pipeline Runner executes commands from .gitlab-ci.yml on the host.   Job  Jobs are set of commands that stored in section script in a config file. GitLab Runner runs jobs in a host machine. Each job is run independently of each other. The most popular jobs are a build_web_app1, build_web_app2, prepare, test, deploy, and etc.   Stage  A stage allows to group jobs, and jobs of the same stage are executed in parallel. Jobs of the next stage are run after the jobs from the previous stage complete successfully.   Pipeline  A pipeline is a group of jobs that get executed in stages (batches). All of the jobs in a stage are executed in parallel and if they all succeed the pipeline moves on to the next stage. If one of the jobs fails the next stage is not (usually) executed. You can access the pipelines page in your project’s Pipelines tab.   Artifact  An artifact is a list of files and directories which are attached to a job after it completes successfully. The uploaded artifacts will be kept in GitLab during expiry period. You can download the artifacts archive or browse its contents in Job Info page.   Environment  GitLab provides a full history of your deployments per every environment. Environments are like tags for your CI jobs, describing where code gets deployed. GitLab keeps track of your deployments, so you always know what is currently being deployed on your servers. For example, you can create and use typical environments - production and staging.   Dependency  When defined jobs exist in a dependency block of your .gitlab-ci.yml the Runner should be download all artifacts before start the current job. It can be used to divide build and deploy jobs for example. The deploy job often uses generated artifacts on previous stages.   Next post will describe how to install and configure a GitLab Runner for using it in your first pipeline.   ","categories": ["devops"],
        "tags": ["gitlab","devops"],
        "url": "https://gainanovrus.github.io/blog/devops/gitlab-ci-main-concepts/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/gitlab-ci/gitlab-ci-devops.png"},{
        "title": "GitLab CI/CD. Install and configure Runner",
        "excerpt":"In GitLab Runners run the jobs that you define in .gitlab-ci.yml. A Runner can be a virtual machine, a VPS, a bare-metal machine, a docker container or even a cluster of containers. GitLab and the Runners communicate through an API, so the only requirement is that the Runner’s machine has Internet access.   The official Runner supported by GitLab is written in Go and its documentation can be found at https://docs.gitlab.com/runner/.   In order to have a functional Runner you need to follow two steps:     Install it   Configure it   Next I’ll show you how to install and configure the latest GitLab Runner with shell executor in CentOS system. Additional steps you can find in official docs.   Install Runner      Add GitLab’s official repository (for RHEL/CentOS/Fedora):     curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh | sudo bash           Install the Runner:     yum install -y gitlab-runner           Configure Runner   Register Runner   Registering a Runner is the process that binds the Runner with a GitLab instance.   You can register a Runner in interactive mode with a command:  gitlab-runner register   And reply to the questions of the wizard.   But I would prefer to register it with inserting answers in parameters:  gitlab-runner register -n \\   --url \"https://YOUR_GITLAB_URL/\" \\   --registration-token \"XXXXXXXXXXXXXXXXXXXXXXX\" \\   --description \"nginx\" \\   --tag-list \"nginx-shell\" \\   --executor \"shell\" \\   --limit 1   Don’t forget place your toker. It stored in Admin Area ➔ Overview ➔ Runners:   https://YOUR_GITLAB_URL/admin/runners      And set the correct description and tags. I prefer to use a hostname as a description. I named it nginx because I run it on Nginx server. And set up tags. I suggest use host + executor as a tag.   Start Runner  gitlab-runner start   Check a status of a service:  # gitlab-runner status gitlab-runner: Service is running!   Show configured runners on the host:  # gitlab-runner list Listing configured runners                          ConfigFile=/etc/gitlab-runner/config.toml host1                                                 Executor=shell Token=XXXXXXXXXXXXXXXXXXXXXXX URL=https://YOUR_GITLAB_URL/   After that I usually change a type of a Runner from a shared into a specific.   Converting Runner type   Set up the current Runner as a specific one to deny to use a Runner of all projects. Just choose our new Runner in Admin Area and determine some project or projects with it.   Once the Runner has been set up, you should see it on the Runners page of your project, following Settings ➔ CI/CD:      After that you can configure your CI process in a .gitlab-ci.yml file with using tags to select specific Runners. For example:  example_job:   tags:     - nginx-shell   script:     - execute_something_script   Next post will help you to write a basic CI/CD configuration.   ","categories": ["devops"],
        "tags": ["gitlab","devops","centos"],
        "url": "https://gainanovrus.github.io/blog/devops/gitlab-ci-runner-install/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/gitlab-ci/gitlab-ci-runner-centos.png"},{
        "title": "GitLab CI/CD. Create simple pipeline",
        "excerpt":"Today I want to show you how to build and run a simple web application with pipelines. I wrote an example Java application to demonstrate a possibility of CI. The application code is placed here.   For using it in GitLab just clone and add as new project.   \t           \t           About project   This is a typical and simple web application that shows a web page with “Hello world” when a user gets http://localhost:8081/ from browser. Firstly you should compile a project with Maven. In the result of it you have got a file hello.war. For running the app copy this war file into a webapps folder and run Tomcat WebServer.   Runner host preparing   It supposes you have configured a shell Runner and have installed Docker.   After that for allowing to manage docker containers by Runner you should add a gitlab-runner user into docker group.  usermod -a -G docker gitlab-runner   In addition I setup gitlab-runner as sudo user to run protected linux commands without user passwords. Just add a next row into a file with a command sudo visudo -f /etc/sudoers.d/gitlab-runner  gitlab-runner ALL=(ALL) NOPASSWD: ALL   In the end, enable a specific Runner to this project if you didn’t do that yet.    \t           Writing pipeline   In this project I want to use two stages to build and deploy an application.    \t           The stage deploy has two jobs to deploy the app into a stand and production servers. Runners on these servers have tags prod-shell and stand-shell. The deploy to the production server requires a manual action. To build an app are used a docker maven container. The app is deploying with tomcat container.   Thus the next code will configure the defined pipeline. It requires to be inputted in .gitlab-ci.yml:  stages:   - build   - deploy  build_app:   stage: build   dependencies: []   tags:   - stand-shell   script:   - docker run -i --rm --name hello-maven -v ${PWD}:/hello -w /hello maven       mvn clean install   - cp target/hello.war hello.war   - docker run -i --rm --name hello-maven -v ${PWD}:/hello -w /hello maven       mvn clean   artifacts:     paths:     - hello.war     expire_in: 1 week  deploy:stand:   stage: deploy   dependencies:   - build_app   tags:   - stand-shell   script:   - docker run -d --rm --name hello-tomcat-${CI_COMMIT_SHA:0:8} -P       -v ${PWD}/hello.war:/usr/local/tomcat/webapps/hello.war          tomcat:9.0-jre8-alpine   - docker ps -f \"name=hello-tomcat-${CI_COMMIT_SHA:0:8}\" --format '{{.Ports}}'  deploy:prod:   stage: deploy   when: manual   dependencies:   - build_app   tags:   - prod-shell   script:   - docker run -d --rm --name hello-tomcat-${CI_COMMIT_SHA:0:8} -P       -v ${PWD}/hello.war:/usr/local/tomcat/webapps/hello.war          tomcat:9.0-jre8-alpine   - docker ps -f \"name=hello-tomcat-${CI_COMMIT_SHA:0:8}\" --format '{{.Ports}}'   In the result we have an automation process that builds and deploy the web app to stand host for test purposes after every commit into a repo.    \t           The result pipeline is:    \t           The app can be accessed by IP and PORT.  The PORT are dynamically generated and are showed in the deploy:stand job result:    \t           In my job output port is 32768. And the web app can be accessed by:   \t           With this configuration you have tried to use CI/CD process with GitLab. Don’t forget this simple pipeline doesn’t provide jobs to stop stands. You should are stopping unused docker containers yourself.   Next you can change it to your complex configuration for own purposes.   In the future I will explain how to use Ansible for CI/CD in the script block. It useful if you have a complex pipeline and you want write readable scripts.   ","categories": ["devops"],
        "tags": ["gitlab","devops","java"],
        "url": "https://gainanovrus.github.io/blog/devops/gitlab-ci-create-first-pipeline/",
        "teaser":"https://gainanovrus.github.io/blog/assets/images/gitlab-ci/gitlab-ci-simple-pipeline.png"}]
